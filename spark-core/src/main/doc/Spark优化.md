
## Spark调优

1数据倾斜

    编写运行Spark代码时候，常常会由于数据等问题出现数据倾斜的问题。

    根据个人经验解决方法：  
    1. 增加RDD的并行度，对读取的数据增大分区个数；  
    2. 对读取的数据自定义分区，让聚集的key分散开；
    3. 通过加大参数配置，增大资源配置(短期见效，不建议长久)；
    4. 把RDD分为正常和倾斜2部分，单独对倾斜处理；
    5. 检查上游是否有脏数据，对源数据过滤出脏数据(首选)；
    6. ...


算子优化  
  
    尽量少使用groupByKey



