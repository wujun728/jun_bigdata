# Hadoopè°ƒä¼˜

NameNode å†…å­˜ç”Ÿäº§é…ç½®
1ï¼‰NameNode å†…å­˜è®¡ç®—
æ¯ä¸ªæ–‡ä»¶å—å¤§æ¦‚å ç”¨ 150byteï¼Œä¸€å°æœåŠ¡å™¨ 128G å†…å­˜ä¸ºä¾‹ï¼Œèƒ½å­˜å‚¨å¤šå°‘æ–‡ä»¶å—å‘¢ï¼Ÿ

```xml
128 * 1024 * 1024 * 1024 / 150Byte â‰ˆ 9.1 äº¿
  G 	MB	   KB 	Byte
```

2ï¼‰Hadoop2.x ç³»åˆ—ï¼Œé…ç½® NameNode å†…å­˜
NameNode å†…å­˜é»˜è®¤ 2000mï¼Œå¦‚æœæœåŠ¡å™¨å†…å­˜ 4Gï¼ŒNameNode å†…å­˜å¯ä»¥é…ç½® 3gã€‚åœ¨ hadoop-env.sh æ–‡ä»¶ä¸­é…ç½®å¦‚ä¸‹ã€‚

```shell
HADOOP_NAMENODE_OPTS=-Xmx3072m
```

3ï¼‰Hadoop3.x ç³»åˆ—ï¼Œé…ç½® NameNode å†…å­˜

ï¼ˆ1ï¼‰hadoop-env.sh ä¸­æè¿° Hadoop çš„å†…å­˜æ˜¯åŠ¨æ€åˆ†é…çš„

```shell
# The maximum amount of heap to use (Java -Xmx). If no unit
# is provided, it will be converted to MB. Daemons will
# prefer any Xmx setting in their respective _OPT variable.
# There is no default; the JVM will autoscale based upon machine
# memory size.
# export HADOOP_HEAPSIZE_MAX=
# The minimum amount of heap to use (Java -Xms). If no unit
# is provided, it will be converted to MB. Daemons will
# prefer any Xms setting in their respective _OPT variable.
# There is no default; the JVM will autoscale based upon machine
# memory size.
# export HADOOP_HEAPSIZE_MIN=
HADOOP_NAMENODE_OPTS=-Xmx102400m
```

ï¼ˆ2ï¼‰æŸ¥çœ‹ NameNode å ç”¨å†…å­˜:   jps

```sh
3088 NodeManager
2611 NameNode
3271 JobHistory
3579 Jps
```

```sh
jmap -heap 2611
Heap Configuration: MaxHeapSize = 1031798784 (984.0MB)
```

ï¼ˆ3ï¼‰æŸ¥çœ‹ DataNode å ç”¨å†…å­˜

```shell
jmap -heap 2744
Heap Configuration: MaxHeapSize = 1031798784 (984.0MB)
```

å‚è€ƒï¼š

* NameNodeæœ€å°å€¼1Gï¼Œæ¯å¢åŠ 1000000ä¸ªblockå¢åŠ 1Gå†…å­˜

* DataNodeæœ€å°å€¼4Gï¼Œblockæ•°ï¼Œæˆ–è€…å‰¯æœ¬æ•°å¢åŠ ï¼Œéƒ½åº”è¯¥è°ƒå¤§DataNodeçš„å€¼
* ä¸€ä¸ªDataNodeä¸Šçš„å‰¯æœ¬æ•°æ€»ä½äº4000000ï¼Œè°ƒä¸º4Gï¼Œè¶…è¿‡4000000ï¼Œæ¯å¢åŠ 1000000ï¼Œå¢åŠ 1Gã€‚



**å…·ä½“ä¿®æ”¹ï¼šhadoop-env.sh**

```shell
export HDFS_NAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS - Xmx1024m"
export HDFS_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS -Xmx1024m"
```

## **NameNode å¿ƒè·³å¹¶å‘é…ç½®**

![image-20211102120133697](Images/image-20211102120133697.png)

**1ï¼‰hdfs-site.xml**

The number of Namenode RPC server threads that listen to requests from clients. If dfs.namenode.servicerpc-address is not configured then Namenode RPC server threads listen to requests from all nodes.
NameNode æœ‰ä¸€ä¸ªå·¥ä½œçº¿ç¨‹æ± ï¼Œç”¨æ¥å¤„ç†ä¸åŒ DataNode çš„å¹¶å‘å¿ƒè·³ä»¥åŠå®¢æˆ·ç«¯å¹¶å‘çš„å…ƒæ•°æ®æ“ä½œã€‚
å¯¹äºå¤§é›†ç¾¤æˆ–è€…æœ‰å¤§é‡å®¢æˆ·ç«¯çš„é›†ç¾¤æ¥è¯´ï¼Œé€šå¸¸éœ€è¦å¢å¤§è¯¥å‚æ•°ã€‚é»˜è®¤å€¼æ˜¯ 10ã€‚

```xml
<property>
 <name>dfs.namenode.handler.count</name>
 <value>21</value>
</property>
```

**ç»éªŒï¼š**

dfs.namenode.handler.count=20 Ã— ğ‘™ğ‘œğ‘”ğ‘’ ğ¶ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿ ğ‘†ğ‘–ğ‘§ğ‘’ï¼Œæ¯”å¦‚é›†ç¾¤è§„æ¨¡ï¼ˆDataNode å° æ•°ï¼‰ä¸º 3 å°æ—¶ï¼Œæ­¤å‚æ•°è®¾ç½®ä¸º 21ã€‚å¯é€šè¿‡ç®€å•çš„ python ä»£ç è®¡ç®—è¯¥å€¼ã€‚

```python
python
Python 2.7.5 (default, Apr 11 2018, 07:36:10)
[GCC 4.8.5 20150623 (Red Hat 4.8.5-28)] on linux2
Type "help", "copyright", "credits" or "license" for more
information.
>>> import math
>>> print int(20*math.log(3))
21
>>> quit()
```

## **å¼€å¯å›æ”¶ç«™é…ç½®**

å¼€å¯å›æ”¶ç«™åŠŸèƒ½ï¼Œå¯ä»¥å°†åˆ é™¤çš„æ–‡ä»¶åœ¨ä¸è¶…æ—¶çš„æƒ…å†µä¸‹ï¼Œæ¢å¤åŸæ•°æ®ï¼Œèµ·åˆ°é˜²æ­¢è¯¯åˆ é™¤ã€ å¤‡ä»½ç­‰ä½œç”¨ã€‚

**1ï¼‰å›æ”¶ç«™å·¥ä½œæœºåˆ¶**

![image-20211102120332502](Images/image-20211102120332502.png)

2ï¼‰å¼€å¯å›æ”¶ç«™åŠŸèƒ½å‚æ•°è¯´æ˜
ï¼ˆ1ï¼‰é»˜è®¤å€¼ fs.trash.interval = 0ï¼Œ0 è¡¨ç¤ºç¦ç”¨å›æ”¶ç«™ï¼›å…¶ä»–å€¼è¡¨ç¤ºè®¾ç½®æ–‡ä»¶çš„å­˜æ´»æ—¶é—´ã€‚
ï¼ˆ2ï¼‰é»˜è®¤å€¼ fs.trash.checkpoint.interval = 0ï¼Œæ£€æŸ¥å›æ”¶ç«™çš„é—´éš”æ—¶é—´ã€‚å¦‚æœè¯¥å€¼ä¸º 0ï¼Œåˆ™è¯¥å€¼è®¾ç½®å’Œ 		 

â€‹	     fs.trash.interval çš„å‚æ•°å€¼ç›¸ç­‰ã€‚
ï¼ˆ3ï¼‰è¦æ±‚ fs.trash.checkpoint.interval <= fs.trash.intervalã€‚



**å¯ç”¨å›æ”¶ç«™**

ä¿®æ”¹ core-site.xmlï¼Œé…ç½®åƒåœ¾å›æ”¶æ—¶é—´ä¸º 1 åˆ†é’Ÿã€‚

```xml
<property>
 <name>fs.trash.interval</name>
 <value>1</value>
</property>
```

**æŸ¥çœ‹å›æ”¶ç«™**
**å›æ”¶ç«™ç›®å½•åœ¨ HDFS é›†ç¾¤ä¸­çš„è·¯å¾„**ï¼š/user/dsjprs/.Trash/â€¦.



5ï¼‰æ³¨æ„ï¼šé€šè¿‡ç½‘é¡µä¸Šç›´æ¥åˆ é™¤çš„æ–‡ä»¶ä¹Ÿä¸ä¼šèµ°å›æ”¶ç«™ã€‚



6ï¼‰é€šè¿‡ç¨‹åºåˆ é™¤çš„æ–‡ä»¶ä¸ä¼šç»è¿‡å›æ”¶ç«™ï¼Œéœ€è¦è°ƒç”¨ moveToTrash()æ‰è¿›å…¥å›æ”¶ç«™

```java
Trash trash = New Trash(conf); trash.moveToTrash(path);
```

7ï¼‰åªæœ‰åœ¨å‘½ä»¤è¡Œåˆ©ç”¨ hadoop fs -rm å‘½ä»¤åˆ é™¤çš„æ–‡ä»¶æ‰ä¼šèµ°å›æ”¶ç«™

```sh
hadoop fs -rm -r /user/atguigu/input
2021-07-14 16:13:42,643 INFO fs.TrashPolicyDefault: Moved:
'hdfs://ndoe01:9820/user/dsjprs/input' to trash at:
hdfs://hadoop102:9820/user/dsjprs/.Trash/Current/user/dsjprs
/input
```

8ï¼‰æ¢å¤å›æ”¶ç«™æ•°æ®

```sh
hadoop fs -mv /user/dsjprs/.Trash/Current/user/dsjprs/input /user/dsjprs/input
```



## **HDFSâ€”å¤šç›®å½•**

### **NameNode å¤šç›®å½•é…ç½®**
1ï¼‰NameNode çš„æœ¬åœ°ç›®å½•å¯ä»¥é…ç½®æˆå¤šä¸ªï¼Œä¸”æ¯ä¸ªç›®å½•å­˜æ”¾å†…å®¹ç›¸åŒï¼Œå¢åŠ äº†å¯é æ€§

![image-20211102120638618](Images/image-20211102120638618.png)



2ï¼‰å…·ä½“é…ç½®å¦‚ä¸‹
ï¼ˆ1ï¼‰åœ¨ hdfs-site.xml æ–‡ä»¶ä¸­æ·»åŠ å¦‚ä¸‹å†…å®¹

```xml
<property>
 <name>dfs.namenode.name.dir</name>
 <value>file://${hadoop.tmp.dir}/dfs/name1,file://${hadoop.tmp.dir}/dfs/name2</value>
</property>
```

### **DataNode å¤šç›®å½•é…ç½®**

1ï¼‰DataNode å¯ä»¥é…ç½®æˆå¤šä¸ªç›®å½•ï¼Œæ¯ä¸ªç›®å½•å­˜å‚¨çš„æ•°æ®ä¸ä¸€æ ·ï¼ˆæ•°æ®ä¸æ˜¯å‰¯æœ¬ï¼‰

![image-20211102120809733](Images/image-20211102120809733.png)

2ï¼‰å…·ä½“é…ç½®å¦‚ä¸‹

åœ¨ hdfs-site.xml æ–‡ä»¶ä¸­æ·»åŠ å¦‚ä¸‹å†…å®¹

```xml
<property>
 <name>dfs.datanode.data.dir</name>
 <value>file://${hadoop.tmp.dir}/dfs/data1,file://${hadoop.tmp.dir}/dfs/data2</value>
</property>
```

## **é›†ç¾¤æ•°æ®å‡è¡¡ä¹‹ç£ç›˜é—´æ•°æ®å‡è¡¡**

ç¡¬ç›˜ç©ºé—´ä¸è¶³ï¼Œå¾€å¾€éœ€è¦å¢åŠ ä¸€å—ç¡¬ç›˜ã€‚åˆšåŠ è½½çš„ç¡¬ç›˜æ²¡æœ‰æ•°æ®æ—¶ï¼Œå¯ ä»¥æ‰§è¡Œç£ç›˜æ•°æ®å‡è¡¡å‘½ä»¤ã€‚

![image-20211102120903148](Images/image-20211102120903148.png)



ï¼ˆ1ï¼‰ç”Ÿæˆå‡è¡¡è®¡åˆ’ï¼ˆæˆ‘ä»¬åªæœ‰ä¸€å—ç£ç›˜ï¼Œä¸ä¼šç”Ÿæˆè®¡åˆ’ï¼‰

```sh
hdfs diskbalancer -plan hadoop103
```

ï¼ˆ2ï¼‰æ‰§è¡Œå‡è¡¡è®¡åˆ’

```sh
hdfs diskbalancer -execute hadoop103.plan.json
```


ï¼ˆ3ï¼‰æŸ¥çœ‹å½“å‰å‡è¡¡ä»»åŠ¡çš„æ‰§è¡Œæƒ…å†µ

```sh
hdfs diskbalancer -query hadoop103
```

ï¼ˆ4ï¼‰å–æ¶ˆå‡è¡¡ä»»åŠ¡

```sh
hdfs diskbalancer -cancel hadoop103.plan.json
```



## **HDFSâ€”é›†ç¾¤æ‰©å®¹åŠç¼©å®¹**

ç™½åå•ï¼šè¡¨ç¤ºåœ¨ç™½åå•çš„ä¸»æœº IP åœ°å€å¯ä»¥ï¼Œç”¨æ¥å­˜å‚¨æ•°æ®ã€‚é…ç½®ç™½åå•ï¼Œå¯ä»¥å°½é‡é˜²æ­¢é»‘å®¢æ¶æ„è®¿é—®æ”»å‡»ã€‚

![image-20211102121014267](Images/image-20211102121014267.png)

**é…ç½®ç™½åå•æ­¥éª¤å¦‚ä¸‹ï¼š**

1ï¼‰åœ¨ NameNode èŠ‚ç‚¹çš„/opt/module/hadoop-3.1.3/etc/hadoop ç›®å½•ä¸‹åˆ†åˆ«åˆ›å»º whitelist å’Œ blacklist æ–‡ä»¶

ï¼ˆ1ï¼‰åˆ›å»ºç™½åå• 

```shell
vim whitelist
# åœ¨ whitelist ä¸­æ·»åŠ å¦‚ä¸‹ä¸»æœºåç§°ï¼Œå‡å¦‚é›†ç¾¤æ­£å¸¸å·¥ä½œçš„èŠ‚ç‚¹ä¸º 102 103 
hadoop102 
hadoop103
```

ï¼ˆ2ï¼‰åˆ›å»ºé»‘åå•

```sh
touch blacklist
# ä¿æŒç©ºçš„å°±å¯ä»¥
```

2ï¼‰åœ¨ hdfs-site.xml é…ç½®æ–‡ä»¶ä¸­å¢åŠ  dfs.hosts é…ç½®å‚æ•°

```xml
<!-- ç™½åå• -->
<property>
 <name>dfs.hosts</name>
 <value>/opt/module/hadoop-3.1.3/etc/hadoop/whitelist</value>
</property>
<!-- é»‘åå• -->
<property>
 <name>dfs.hosts.exclude</name>
 <value>/opt/module/hadoop-3.1.3/etc/hadoop/blacklist</value>
</property
```

**ç¬¬ä¸€æ¬¡æ·»åŠ ç™½åå•å¿…é¡»é‡å¯é›†ç¾¤ï¼Œä¸æ˜¯ç¬¬ä¸€æ¬¡ï¼Œåªéœ€è¦åˆ·æ–° NameNode èŠ‚ç‚¹å³å¯**

```sh
hdfs dfsadmin -refreshNodes 
```




## **æœå½¹æ–°æœåŠ¡å™¨**

1ï¼‰éœ€æ±‚ 
éšç€ä¸šåŠ¡çš„å¢é•¿ï¼Œæ•°æ®é‡è¶Šæ¥è¶Šå¤§ï¼ŒåŸæœ‰çš„æ•°æ®èŠ‚ç‚¹çš„å®¹é‡å·²ç»ä¸èƒ½æ»¡è¶³å­˜å‚¨æ•°æ® çš„éœ€æ±‚ï¼Œéœ€è¦åœ¨åŸæœ‰é›†ç¾¤åŸºç¡€ä¸ŠåŠ¨æ€æ·»åŠ æ–°çš„æ•°æ®èŠ‚ç‚¹ã€‚

2ï¼‰ç¯å¢ƒå‡†å¤‡
ï¼ˆ1ï¼‰åœ¨ hadoop100 ä¸»æœºä¸Šå†å…‹éš†ä¸€å° hadoop105 ä¸»æœº 
ï¼ˆ2ï¼‰ä¿®æ”¹ IP åœ°å€å’Œä¸»æœºåç§°

```sh
vim /etc/sysconfig/network-scripts/ifcfgens33 
vim /etc/hostname
```

ï¼ˆ3ï¼‰æ‹·è´ hadoop102 çš„/opt/module ç›®å½•å’Œ/etc/profile.d/my_env.sh åˆ° hadoop105

```sh
scp -r module/* dsjprs@hadoop105:/opt/module/ 

scp /etc/profile.d/my_env.sh root@hadoop105:/etc/profile.d/my_env.sh
```

```sh
source /etc/profile
```

ï¼ˆ4ï¼‰åˆ é™¤ hadoop105 ä¸Š Hadoop çš„å†å²æ•°æ®ï¼Œdata å’Œ log æ•°æ®

```sh
rm -rf data/ logs/
```

ï¼ˆ5ï¼‰é…ç½® hadoop102 å’Œ hadoop103 åˆ° hadoop105 çš„ ssh æ— å¯†ç™»å½•

```sh
ssh-copy-id hadoop105 
ssh-copy-id hadoop105 
```

æœå½¹æ–°èŠ‚ç‚¹å…·ä½“æ­¥éª¤
ï¼ˆ1ï¼‰ç›´æ¥å¯åŠ¨ DataNodeï¼Œå³å¯å…³è”åˆ°é›†ç¾¤

```sh
hdfs --daemon start datanode
yarn --daemon start nodemanager
```



**åœ¨ç™½åå•ä¸­å¢åŠ æ–°æœå½¹çš„æœåŠ¡å™¨** 

ï¼ˆ1ï¼‰åœ¨ç™½åå• whitelist ä¸­å¢åŠ  hadoop104ã€hadoop105ï¼Œå¹¶é‡å¯é›†ç¾¤

```sh
vim whitelist
# ä¿®æ”¹ä¸ºå¦‚ä¸‹å†…å®¹
hadoop102
hadoop103
hadoop104
hadoop105
```

```sh
# åˆ·æ–° NameNode
hdfs dfsadmin -refreshNodes 

Refresh nodes successful 
```

## **é»‘åå•é…ç½®**

1ï¼‰ç¼–è¾‘/opt/module/hadoop-3.1.3/etc/hadoop ç›®å½•ä¸‹çš„ blacklist æ–‡ä»¶

```sh
vim blacklist
# æ·»åŠ å¦‚ä¸‹ä¸»æœºåç§°ï¼ˆè¦é€€å½¹çš„èŠ‚ç‚¹ï¼‰
hadoop105
```

æ³¨æ„ï¼šå¦‚æœç™½åå•ä¸­æ²¡æœ‰é…ç½®ï¼Œéœ€è¦åœ¨ hdfs-site.xml é…ç½®æ–‡ä»¶ä¸­å¢åŠ  dfs.hosts é…ç½®å‚æ•°

```xml
<!-- é»‘åå• -->
<property>
 <name>dfs.hosts.exclude</name>
 <value>/opt/module/hadoop-3.1.3/etc/hadoop/blacklist</value>
</property>
```

2ï¼‰åˆ†å‘é…ç½®æ–‡ä»¶ blacklistï¼Œhdfs-site.xml

```sh
xsync hdfs-site.xml blacklist
```

3ï¼‰ç¬¬ä¸€æ¬¡æ·»åŠ é»‘åå•å¿…é¡»é‡å¯é›†ç¾¤ï¼Œä¸æ˜¯ç¬¬ä¸€æ¬¡ï¼Œåªéœ€è¦åˆ·æ–° NameNode èŠ‚ç‚¹å³å¯

```sh
hdfs dfsadmin -refreshNodes 

Refresh nodes successful
```

## **æœåŠ¡å™¨é—´æ•°æ®å‡è¡¡**

1ï¼‰ä¼ä¸šç»éªŒï¼š 
åœ¨ä¼ä¸šå¼€å‘ä¸­ï¼Œå¦‚æœç»å¸¸åœ¨ hadoop102 å’Œ hadoop104 ä¸Šæäº¤ä»»åŠ¡ï¼Œä¸”å‰¯æœ¬æ•°ä¸º 2ï¼Œç”±äºæ•°æ®æœ¬åœ°æ€§åŸåˆ™ï¼Œå°±ä¼šå¯¼è‡´ hadoop102 å’Œ hadoop104 æ•°æ®è¿‡å¤šï¼Œhadoop103 å­˜å‚¨çš„æ•°æ®é‡å°ã€‚



å¦ä¸€ç§æƒ…å†µï¼Œå°±æ˜¯æ–°æœå½¹çš„æœåŠ¡å™¨æ•°æ®é‡æ¯”è¾ƒå°‘ï¼Œéœ€è¦æ‰§è¡Œé›†ç¾¤å‡è¡¡å‘½ä»¤ã€‚

![image-20211102121738693](Images/image-20211102121738693.png)



2ï¼‰å¼€å¯æ•°æ®å‡è¡¡å‘½ä»¤ï¼š

```sh
sbin/start-balancer.sh - threshold 10 
```

å¯¹äºå‚æ•° 10ï¼Œä»£è¡¨çš„æ˜¯é›†ç¾¤ä¸­å„ä¸ªèŠ‚ç‚¹çš„ç£ç›˜ç©ºé—´åˆ©ç”¨ç‡ç›¸å·®ä¸è¶…è¿‡ 10%ï¼Œå¯æ ¹æ®å® é™…æƒ…å†µè¿›è¡Œè°ƒæ•´ã€‚



3ï¼‰åœæ­¢æ•°æ®å‡è¡¡å‘½ä»¤ï¼š

```sh
sbin/stop-balancer.sh
```

æ³¨æ„ï¼šç”±äº HDFS éœ€è¦å¯åŠ¨å•ç‹¬çš„ Rebalance Server æ¥æ‰§è¡Œ Rebalance æ“ä½œï¼Œæ‰€ä»¥å°½é‡ ä¸è¦åœ¨ NameNode ä¸Šæ‰§è¡Œ start-balancer.shï¼Œè€Œæ˜¯æ‰¾ä¸€å°æ¯”è¾ƒç©ºé—²çš„æœºå™¨ã€‚ 



## **HDFSâ€”å­˜å‚¨ä¼˜åŒ–**

### **çº åˆ ç åŸç†**

HDFS é»˜è®¤æƒ…å†µä¸‹ï¼Œä¸€ä¸ªæ–‡ä»¶æœ‰ 3 ä¸ªå‰¯æœ¬ï¼Œè¿™æ ·æé«˜äº†æ•°æ®çš„å¯é æ€§ï¼Œä½†ä¹Ÿå¸¦æ¥äº† 2 å€ çš„å†—ä½™å¼€é”€ã€‚Hadoop3.x å¼•å…¥äº†çº åˆ ç ï¼Œé‡‡ç”¨è®¡ç®—çš„æ–¹å¼ï¼Œå¯ä»¥èŠ‚çœçº¦ 50ï¼…å·¦å³çš„å­˜å‚¨ç©ºé—´ã€‚

![image-20211102122109905](Images/image-20211102122109905.png)



1ï¼‰çº åˆ ç æ“ä½œç›¸å…³çš„å‘½ä»¤

```shell
hdfs ec
Usage: bin/hdfs ec [COMMAND]
 [-listPolicies]
 [-addPolicies -policyFile <file>]
 [-getPolicy -path <path>]
 [-removePolicy -policy <policy>]
 [-setPolicy -path <path> [-policy <policy>] [-replicate]]
 [-unsetPolicy -path <path>]
 [-listCodecs]
 [-enablePolicy -policy <policy>]
 [-disablePolicy -policy <policy>]
 [-help <command-name>].
```

2ï¼‰æŸ¥çœ‹å½“å‰æ”¯æŒçš„çº åˆ ç ç­–ç•¥

```shell
hdfs ec -listPolicies
Erasure Coding Policies:
ErasureCodingPolicy=[Name=RS-10-4-1024k, Schema=[ECSchema=[Codec=rs,
numDataUnits=10, numParityUnits=4]], CellSize=1048576, Id=5],
State=DISABLED
ErasureCodingPolicy=[Name=RS-3-2-1024k, Schema=[ECSchema=[Codec=rs,
numDataUnits=3, numParityUnits=2]], CellSize=1048576, Id=2],
State=DISABLED
ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs,
numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1],
State=ENABLED
ErasureCodingPolicy=[Name=RS-LEGACY-6-3-1024k,
Schema=[ECSchema=[Codec=rs-legacy, numDataUnits=6, numParityUnits=3]],
CellSize=1048576, Id=3], State=DISABLED
ErasureCodingPolicy=[Name=XOR-2-1-1024k, Schema=[ECSchema=[Codec=xor,
numDataUnits=2, numParityUnits=1]], CellSize=1048576, Id=4],
State=DISABLED
```

3ï¼‰çº åˆ ç ç­–ç•¥è§£é‡Š:
RS-3-2-1024kï¼šä½¿ç”¨ RS ç¼–ç ï¼Œæ¯ 3 ä¸ªæ•°æ®å•å…ƒï¼Œç”Ÿæˆ 2 ä¸ªæ ¡éªŒå•å…ƒï¼Œå…± 5 ä¸ªå•å…ƒï¼Œä¹Ÿ å°±æ˜¯è¯´ï¼šè¿™ 5 ä¸ªå•å…ƒä¸­ï¼Œåªè¦æœ‰ä»»æ„çš„ 3 ä¸ªå•å…ƒå­˜åœ¨ï¼ˆä¸ç®¡æ˜¯æ•°æ®å•å…ƒè¿˜æ˜¯æ ¡éªŒå•å…ƒï¼Œåªè¦æ€»æ•°=3ï¼‰ï¼Œå°±å¯ä»¥å¾—åˆ°åŸå§‹æ•°æ®ã€‚æ¯ä¸ªå•å…ƒçš„å¤§å°æ˜¯ 1024k=1024*1024=1048576ã€‚

![image-20211102122155235](Images/image-20211102122155235.png)



```xml
RS-10-4-1024kï¼šä½¿ç”¨ RS ç¼–ç ï¼Œæ¯ 10 ä¸ªæ•°æ®å•å…ƒï¼ˆcellï¼‰ï¼Œç”Ÿæˆ 4 ä¸ªæ ¡éªŒå•å…ƒï¼Œå…± 14ä¸ªå•å…ƒï¼Œä¹Ÿå°±æ˜¯è¯´ï¼šè¿™14 ä¸ªå•å…ƒä¸­ï¼Œåªè¦æœ‰ä»»æ„çš„ 10 ä¸ªå•å…ƒå­˜åœ¨ï¼ˆä¸ç®¡æ˜¯æ•°æ®å•å…ƒè¿˜æ˜¯æ ¡éªŒå•å…ƒï¼Œåªè¦æ€»æ•°=10ï¼‰ï¼Œå°±å¯ä»¥å¾—åˆ°åŸå§‹æ•°æ®ã€‚æ¯ä¸ªå•å…ƒçš„å¤§å°æ˜¯ 1024k=1024*1024=1048576ã€‚

RS-6-3-1024kï¼šä½¿ç”¨ RS ç¼–ç ï¼Œæ¯ 6 ä¸ªæ•°æ®å•å…ƒï¼Œç”Ÿæˆ 3 ä¸ªæ ¡éªŒå•å…ƒï¼Œå…± 9 ä¸ªå•å…ƒï¼Œä¹Ÿå°±æ˜¯è¯´ï¼šè¿™ 9 ä¸ªå•å…ƒä¸­ï¼Œåªè¦æœ‰ä»»æ„çš„ 6 ä¸ªå•å…ƒå­˜åœ¨ï¼ˆä¸ç®¡æ˜¯æ•°æ®å•å…ƒè¿˜æ˜¯æ ¡éªŒå•å…ƒï¼Œåªè¦æ€»æ•°=6ï¼‰ï¼Œå°±å¯ä»¥å¾—åˆ°åŸå§‹æ•°æ®ã€‚æ¯ä¸ªå•å…ƒçš„å¤§å°æ˜¯ 1024k=1024*1024=1048576ã€‚

RS-LEGACY-6-3-1024kï¼šç­–ç•¥å’Œä¸Šé¢çš„ RS-6-3-1024k ä¸€æ ·ï¼Œåªæ˜¯ç¼–ç çš„ç®—æ³•ç”¨çš„æ˜¯ rslegacyã€‚

XOR-2-1-1024kï¼šä½¿ç”¨ XOR ç¼–ç ï¼ˆé€Ÿåº¦æ¯” RS ç¼–ç å¿«ï¼‰ï¼Œæ¯ 2 ä¸ªæ•°æ®å•å…ƒï¼Œç”Ÿæˆ 1 ä¸ªæ ¡éªŒå•å…ƒï¼Œå…± 3 ä¸ªå•å…ƒï¼Œä¹Ÿå°±æ˜¯è¯´ï¼šè¿™ 3 ä¸ªå•å…ƒä¸­ï¼Œåªè¦æœ‰ä»»æ„çš„ 2 ä¸ªå•å…ƒå­˜åœ¨ï¼ˆä¸ç®¡æ˜¯æ•°æ®å•å…ƒè¿˜æ˜¯æ ¡éªŒå•å…ƒï¼Œåªè¦æ€»æ•°= 2ï¼‰ï¼Œå°±å¯ä»¥å¾—åˆ°åŸå§‹æ•°æ®ã€‚æ¯ä¸ªå•å…ƒçš„å¤§å°æ˜¯1024k=1024*1024=1048576ã€‚
```

### **å¼‚æ„å­˜å‚¨ï¼ˆå†·çƒ­æ•°æ®åˆ†ç¦»ï¼‰** 
å¼‚æ„å­˜å‚¨ä¸»è¦è§£å†³ï¼Œä¸åŒçš„æ•°æ®ï¼Œå­˜å‚¨åœ¨ä¸åŒç±»å‹çš„ç¡¬ç›˜ä¸­ï¼Œè¾¾åˆ°æœ€ä½³æ€§èƒ½çš„é—®é¢˜ã€‚ 

![image-20211102122311441](Images/image-20211102122311441.png)



å­˜å‚¨ç±»å‹å’Œå­˜å‚¨ç­–ç•¥

1ï¼‰å…³äºå­˜å‚¨ç±»å‹

RAM_DISKï¼šï¼ˆå†…å­˜é•œåƒæ–‡ä»¶ç³»ç»Ÿï¼‰

SSDï¼šï¼ˆSSDå›ºæ€ç¡¬ç›˜ï¼‰

DISKï¼šï¼ˆæ™®é€šç£ç›˜ï¼Œåœ¨HDFSä¸­ï¼Œå¦‚æœæ²¡æœ‰ä¸»åŠ¨å£°æ˜æ•°æ®ç›®å½•å­˜å‚¨ç±»å‹é»˜è®¤éƒ½æ˜¯DISKï¼‰

ARCHIVEï¼šï¼ˆæ²¡æœ‰ç‰¹æŒ‡å“ªç§å­˜å‚¨ä»‹è´¨ï¼Œä¸»è¦çš„æŒ‡çš„æ˜¯è®¡ç®—èƒ½åŠ›æ¯”è¾ƒå¼±è€Œå­˜å‚¨å¯†åº¦æ¯”è¾ƒé«˜çš„å­˜å‚¨ä»‹è´¨ï¼Œç”¨æ¥è§£å†³æ•°æ®é‡çš„å®¹é‡æ‰©å¢çš„é—®é¢˜ï¼Œä¸€èˆ¬ç”¨äºå½’æ¡£ï¼‰

2ï¼‰å…³äºå­˜å‚¨ç­–ç•¥
è¯´æ˜ï¼šä»Lazy_Persiståˆ°Coldï¼Œåˆ†åˆ«ä»£è¡¨äº†è®¾å¤‡çš„è®¿é—®é€Ÿåº¦ä»å¿«åˆ°æ…¢

![image-20211102122338023](Images/image-20211102122338023.png)



### **å¼‚æ„å­˜å‚¨ Shell æ“ä½œ**

ï¼ˆ1ï¼‰æŸ¥çœ‹å½“å‰æœ‰å“ªäº›å­˜å‚¨ç­–ç•¥å¯ä»¥ç”¨

```sh
hdfs storagepolicies -listPolicies
```

ï¼ˆ2ï¼‰ä¸ºæŒ‡å®šè·¯å¾„ï¼ˆæ•°æ®å­˜å‚¨ç›®å½•ï¼‰è®¾ç½®æŒ‡å®šçš„å­˜å‚¨ç­–ç•¥

```sh
hdfs storagepolicies -setStoragePolicy -path xxx -policy xxx
```

ï¼ˆ3ï¼‰è·å–æŒ‡å®šè·¯å¾„ï¼ˆæ•°æ®å­˜å‚¨ç›®å½•æˆ–æ–‡ä»¶ï¼‰çš„å­˜å‚¨ç­–ç•¥

```sh
hdfs storagepolicies -getStoragePolicy -path xxx
```

ï¼ˆ4ï¼‰å–æ¶ˆå­˜å‚¨ç­–ç•¥ï¼›æ‰§è¡Œæ”¹å‘½ä»¤ä¹‹åè¯¥ç›®å½•æˆ–è€…æ–‡ä»¶ï¼Œä»¥å…¶ä¸Šçº§çš„ç›®å½•ä¸ºå‡†ï¼Œå¦‚æœæ˜¯æ ¹ ç›®å½•ï¼Œé‚£ä¹ˆå°±æ˜¯ HOT

```sh
hdfs storagepolicies -unsetStoragePolicy -path xxx
```

ï¼ˆ5ï¼‰æŸ¥çœ‹æ–‡ä»¶å—çš„åˆ†å¸ƒ

```sh
bin/hdfs fsck xxx -files -blocks -locations
```

ï¼ˆ6ï¼‰æŸ¥çœ‹é›†ç¾¤èŠ‚ç‚¹

```sh
hadoop dfsadmin -report
```



## **HDFSâ€”æ•…éšœæ’é™¤**

### **NameNode æ•…éšœå¤„ç†**

![image-20211102122527910](Images/image-20211102122527910.png)

1ï¼‰éœ€æ±‚ï¼š

NameNode è¿›ç¨‹æŒ‚äº†å¹¶ä¸”å­˜å‚¨çš„æ•°æ®ä¹Ÿä¸¢å¤±äº†ï¼Œå¦‚ä½•æ¢å¤ NameNode

2ï¼‰æ•…éšœæ¨¡æ‹Ÿ
ï¼ˆ1ï¼‰kill -9 NameNode è¿›ç¨‹

```sh
kill -9 19886
```

ï¼ˆ2ï¼‰åˆ é™¤ NameNode å­˜å‚¨çš„æ•°æ®ï¼ˆ/opt/module/hadoop-3.1.3/data/tmp/dfs/nameï¼‰

```sh
rm -rf /opt/module/hadoop3.1.3/data/dfs/name/* 
```

3ï¼‰é—®é¢˜è§£å†³
ï¼ˆ1ï¼‰æ‹·è´ SecondaryNameNode ä¸­æ•°æ®åˆ°åŸ NameNode å­˜å‚¨æ•°æ®ç›®å½•

```sh
scp -r dsjprs@hadoop104:/opt/module/hadoop3.1.3/data/dfs/namesecondary/* ./name/
```

ï¼ˆ2ï¼‰é‡æ–°å¯åŠ¨ NameNode

```sh
hdfs --daemon start namenode
```

ï¼ˆ3ï¼‰å‘é›†ç¾¤ä¸Šä¼ ä¸€ä¸ªæ–‡ä»¶ 



é›†ç¾¤å®‰å…¨æ¨¡å¼&ç£ç›˜ä¿®å¤;

1ï¼‰å®‰å…¨æ¨¡å¼ï¼šæ–‡ä»¶ç³»ç»Ÿåªæ¥å—è¯»æ•°æ®è¯·æ±‚ï¼Œè€Œä¸æ¥å—åˆ é™¤ã€ä¿®æ”¹ç­‰å˜æ›´è¯·æ±‚

2ï¼‰è¿›å…¥å®‰å…¨æ¨¡å¼åœºæ™¯
â¢ NameNode åœ¨åŠ è½½é•œåƒæ–‡ä»¶å’Œç¼–è¾‘æ—¥å¿—æœŸé—´å¤„äºå®‰å…¨æ¨¡å¼ï¼›

â¢ NameNode å†æ¥æ”¶ DataNode æ³¨å†Œæ—¶ï¼Œå¤„äºå®‰å…¨æ¨¡å¼

![image-20211102122619040](Images/image-20211102122619040.png)



3ï¼‰é€€å‡ºå®‰å…¨æ¨¡å¼æ¡ä»¶

```sh
dfs.namenode.safemode.min.datanodes:æœ€å°å¯ç”¨ datanode æ•°é‡ï¼Œé»˜è®¤ 0

dfs.namenode.safemode.threshold-pct:å‰¯æœ¬æ•°è¾¾åˆ°æœ€å°è¦æ±‚çš„ block å ç³»ç»Ÿæ€» block æ•°çš„ ç™¾åˆ†æ¯”ï¼Œé»˜è®¤ 0.999fã€‚ï¼ˆåªå…è®¸ä¸¢ä¸€ä¸ªå—ï¼‰

dfs.namenode.safemode.extension:ç¨³å®šæ—¶é—´ï¼Œé»˜è®¤å€¼ 30000 æ¯«ç§’ï¼Œå³ 30 ç§’
```

4ï¼‰åŸºæœ¬è¯­æ³•

é›†ç¾¤å¤„äºå®‰å…¨æ¨¡å¼ï¼Œä¸èƒ½æ‰§è¡Œé‡è¦æ“ä½œï¼ˆå†™æ“ä½œï¼‰ã€‚é›†ç¾¤å¯åŠ¨å®Œæˆåï¼Œè‡ªåŠ¨é€€å‡ºå®‰å…¨æ¨¡ å¼ã€‚

ï¼ˆ1ï¼‰bin/hdfs dfsadmin -safemode get ï¼ˆåŠŸèƒ½æè¿°ï¼šæŸ¥çœ‹å®‰å…¨æ¨¡å¼çŠ¶æ€ï¼‰
ï¼ˆ2ï¼‰bin/hdfs dfsadmin -safemode enter ï¼ˆåŠŸèƒ½æè¿°ï¼šè¿›å…¥å®‰å…¨æ¨¡å¼çŠ¶æ€ï¼‰
ï¼ˆ3ï¼‰bin/hdfs dfsadmin -safemode leaveï¼ˆåŠŸèƒ½æè¿°ï¼šç¦»å¼€å®‰å…¨æ¨¡å¼çŠ¶æ€ï¼‰
ï¼ˆ4ï¼‰bin/hdfs dfsadmin -safemode wait ï¼ˆåŠŸèƒ½æè¿°ï¼šç­‰å¾…å®‰å…¨æ¨¡å¼çŠ¶æ€ï¼‰





## **æ…¢ç£ç›˜ç›‘æ§**

â€œæ…¢ç£ç›˜â€æŒ‡çš„æ—¶å†™å…¥æ•°æ®éå¸¸æ…¢çš„ä¸€ç±»ç£ç›˜ã€‚å…¶å®æ…¢æ€§ç£ç›˜å¹¶ä¸å°‘è§ï¼Œå½“æœºå™¨è¿è¡Œæ—¶ é—´é•¿äº†ï¼Œä¸Šé¢è·‘çš„ä»»åŠ¡å¤šäº†ï¼Œç£ç›˜çš„è¯»å†™æ€§èƒ½è‡ªç„¶ä¼šé€€åŒ–ï¼Œä¸¥é‡æ—¶å°±ä¼šå‡ºç°å†™å…¥æ•°æ®å»¶æ—¶çš„é—®é¢˜ã€‚



### **å¦‚ä½•å‘ç°æ…¢ç£ç›˜ï¼Ÿ**

æ­£å¸¸åœ¨ HDFS ä¸Šåˆ›å»ºä¸€ä¸ªç›®å½•ï¼Œåªéœ€è¦ä¸åˆ° 1s çš„æ—¶é—´ã€‚å¦‚æœä½ å‘ç°åˆ›å»ºç›®å½•è¶…è¿‡ 1 åˆ† é’ŸåŠä»¥ä¸Šï¼Œè€Œä¸”è¿™ä¸ªç°è±¡å¹¶ä¸æ˜¯æ¯æ¬¡éƒ½æœ‰ã€‚åªæ˜¯å¶å°”æ…¢äº†ä¸€ä¸‹ï¼Œå°±å¾ˆæœ‰å¯èƒ½å­˜åœ¨æ…¢ç£ç›˜ã€‚ å¯ä»¥é‡‡ç”¨å¦‚ä¸‹æ–¹æ³•æ‰¾å‡ºæ˜¯å“ªå—ç£ç›˜æ…¢ï¼š



1ï¼‰é€šè¿‡å¿ƒè·³æœªè”ç³»æ—¶é—´ã€‚

ä¸€èˆ¬å‡ºç°æ…¢ç£ç›˜ç°è±¡ï¼Œä¼šå½±å“åˆ° DataNode ä¸ NameNode ä¹‹é—´çš„å¿ƒè·³ã€‚æ­£å¸¸æƒ…å†µå¿ƒè·³æ—¶ é—´é—´éš”æ˜¯ 3sã€‚è¶…è¿‡ 3s è¯´æ˜æœ‰å¼‚å¸¸ã€‚





2ï¼‰fio å‘½ä»¤ï¼Œæµ‹è¯•ç£ç›˜çš„è¯»å†™æ€§èƒ½

ï¼ˆ1ï¼‰é¡ºåºè¯»æµ‹è¯•

```sh
sudo yum install -y fio

sudo fio - filename=/home/atguigu/test.log -direct=1 -iodepth 1 -thread - rw=read -ioengine=psync -bs=16k -size=2G -numjobs=10 - runtime=60 -group_reporting -name=test_r

Run status group 0 (all jobs):

READ: bw=360MiB/s (378MB/s), 360MiB/s-360MiB/s (378MB/s-378MB/s),io=20.0GiB (21.5GB), run=56885-56885msec
```

ç»“æœæ˜¾ç¤ºï¼Œç£ç›˜çš„æ€»ä½“é¡ºåºè¯»é€Ÿåº¦ä¸º 360MiB/sã€‚


ï¼ˆ2ï¼‰é¡ºåºå†™æµ‹è¯•

```sh
sudo fio - filename=/home/atguigu/test.log -direct=1 -iodepth 1 -thread - rw=write -ioengine=psync -bs=16k -size=2G -numjobs=10 - runtime=60 -group_reporting -name=test_w

Run status group 0 (all jobs):

WRITE: bw=341MiB/s (357MB/s), 341MiB/s-341MiB/s (357MB/s357MB/s), io=19.0GiB (21.4GB), run=60001-60001msec
```

ç»“æœæ˜¾ç¤ºï¼Œç£ç›˜çš„æ€»ä½“é¡ºåºå†™é€Ÿåº¦ä¸º 341MiB/sã€‚ 

ï¼ˆ3ï¼‰éšæœºå†™æµ‹è¯•

```sh
sudo fio - filename=/home/atguigu/test.log -direct=1 -iodepth 1 -thread -rw=randwrite -ioengine=psync -bs=16k -size=2G -numjobs=10 - runtime=60 -group_reporting -name=test_randw
Run status group 0 (all jobs):

WRITE: bw=309MiB/s (324MB/s), 309MiB/s-309MiB/s (324MB/s-324MB/s),
io=18.1GiB (19.4GB), run=60001-60001msec
```

ï¼ˆ4ï¼‰æ··åˆéšæœºè¯»å†™ï¼š

```sh
sudo fio - filename=/home/atguigu/test.log -direct=1 -iodepth 1 -thread - rw=randrw -rwmixread=70 -ioengine=psync -bs=16k -size=2G - numjobs=10 -runtime=60 -group_reporting -name=test_r_w - ioscheduler=noop

Run status group 0 (all jobs):
READ: bw=220MiB/s (231MB/s), 220MiB/s-220MiB/s (231MB/s231MB/s), io=12.9GiB (13.9GB), run=60001-60001msec 

WRITE: bw=94.6MiB/s (99.2MB/s), 94.6MiB/s-94.6MiB/s
(99.2MB/s-99.2MB/s), io=5674MiB (5950MB), run=60001-60001msec
```

ç»“æœæ˜¾ç¤ºï¼Œç£ç›˜çš„æ€»ä½“æ··åˆéšæœºè¯»å†™ï¼Œè¯»é€Ÿåº¦ä¸º 220MiB/sï¼Œå†™é€Ÿåº¦ 94.6MiB/sã€‚





## **å°æ–‡ä»¶å½’æ¡£**

### **1ï¼‰HDFS å­˜å‚¨å°æ–‡ä»¶å¼Šç«¯**

![image-20211102122900258](Images/image-20211102122900258.png)



æ¯ä¸ªæ–‡ä»¶å‡æŒ‰å—å­˜å‚¨ï¼Œæ¯ä¸ªå—çš„å…ƒæ•°æ®å­˜å‚¨åœ¨ NameNode çš„å†…å­˜ä¸­ï¼Œå› æ­¤ HDFS å­˜å‚¨ å°æ–‡ä»¶ä¼šéå¸¸ä½æ•ˆã€‚å› ä¸ºå¤§é‡çš„å°æ–‡ä»¶ä¼šè€—å°½ NameNode ä¸­çš„å¤§éƒ¨åˆ†å†…å­˜ã€‚ä½†æ³¨æ„ï¼Œå­˜å‚¨å° æ–‡ä»¶æ‰€éœ€è¦çš„ç£ç›˜å®¹é‡å’Œæ•°æ®å—çš„å¤§å°æ— å…³ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ª 1MB çš„æ–‡ä»¶è®¾ç½®ä¸º 128MB çš„å— å­˜å‚¨ï¼Œå®é™…ä½¿ç”¨çš„æ˜¯ 1MB çš„ç£ç›˜ç©ºé—´ï¼Œè€Œä¸æ˜¯ 128MBã€‚



### **2ï¼‰è§£å†³å­˜å‚¨å°æ–‡ä»¶åŠæ³•ä¹‹ä¸€**
HDFS å­˜æ¡£æ–‡ä»¶æˆ– HAR æ–‡ä»¶ï¼Œæ˜¯ä¸€ä¸ªæ›´é«˜æ•ˆçš„æ–‡ä»¶å­˜æ¡£å·¥å…·ï¼Œå®ƒå°†æ–‡ä»¶å­˜å…¥ HDFS å—ï¼Œ åœ¨å‡å°‘ NameNode å†…å­˜ä½¿ç”¨çš„åŒæ—¶ï¼Œå…è®¸å¯¹æ–‡ä»¶è¿›è¡Œé€æ˜çš„è®¿é—®ã€‚å…·ä½“è¯´æ¥ï¼ŒHDFS å­˜æ¡£æ–‡ ä»¶å¯¹å†…è¿˜æ˜¯ä¸€ä¸ªä¸€ä¸ªç‹¬ç«‹æ–‡ä»¶ï¼Œå¯¹ NameNode è€Œè¨€å´æ˜¯ä¸€ä¸ªæ•´ä½“ï¼Œå‡å°‘äº† NameNode çš„å†…å­˜ã€‚

![image-20211102122937464](Images/image-20211102122937464.png)

ï¼ˆ1ï¼‰éœ€è¦å¯åŠ¨ YARN è¿›ç¨‹

```sh
start-yarn.sh
```

ï¼ˆ2ï¼‰å½’æ¡£æ–‡ä»¶
æŠŠ/input ç›®å½•é‡Œé¢çš„æ‰€æœ‰æ–‡ä»¶å½’æ¡£æˆä¸€ä¸ªå« input.har çš„å½’æ¡£æ–‡ä»¶ï¼Œå¹¶æŠŠå½’æ¡£åæ–‡ä»¶å­˜å‚¨ åˆ°/output è·¯å¾„ä¸‹ã€‚

```sh
hadoop archive -archiveName input.har -p /input /output 
```

ï¼ˆ3ï¼‰æŸ¥çœ‹å½’æ¡£

```sh
hadoop fs -ls
/output/input.har

hadoop fs -ls 
har:///output/input.har 
```

ï¼ˆ4ï¼‰è§£å½’æ¡£æ–‡ä»¶

```sh
hadoop fs -cp har:///output/input.har/* / 
```



## **HDFSâ€”é›†ç¾¤è¿ç§»**

Apache å’Œ Apache é›†ç¾¤é—´æ•°æ®æ‹·è´

1ï¼‰scp å®ç°ä¸¤ä¸ªè¿œç¨‹ä¸»æœºä¹‹é—´çš„æ–‡ä»¶å¤åˆ¶

```sh
scp -r hello.txt root@hadoop103:/user/dsjprs/hello.txt # æ¨ push

scp -r root@hadoop103:/user/dsjprs/hello.txt hello.txt # æ‹‰ pull

scp -r root@hadoop103:/user/dsjprs/hello.txt root@hadoop104:/user/dsjprs # æ˜¯é€šè¿‡æœ¬åœ°ä¸»æœºä¸­è½¬å®ç°ä¸¤ä¸ªè¿œç¨‹ä¸»æœºçš„æ–‡ä»¶å¤åˆ¶ï¼›å¦‚æœåœ¨ä¸¤ä¸ªè¿œç¨‹ä¸»æœºä¹‹é—´ ssh æ²¡æœ‰é…ç½®çš„æƒ…å†µä¸‹ å¯ä»¥ä½¿ç”¨è¯¥æ–¹å¼ã€‚
```

2ï¼‰é‡‡ç”¨ distcp å‘½ä»¤å®ç°ä¸¤ä¸ª Hadoop é›†ç¾¤ä¹‹é—´çš„é€’å½’æ•°æ®å¤åˆ¶

```sh
bin/hadoop distcp

hdfs://hadoop102:8020/user/dsjprs/hello.txt 
hdfs://hadoop105:8020/user/dsjprs/hello.txt
```





## **MapReduce ç”Ÿäº§è°ƒä¼˜**

**MapReduce è·‘çš„æ…¢çš„åŸå› **

![image-20211102123430905](Images/image-20211102123430905.png)

**MapReduce ç¨‹åºæ•ˆç‡çš„ç“¶é¢ˆåœ¨äºä¸¤ç‚¹ï¼š**

1ï¼‰è®¡ç®—æœºæ€§èƒ½

2ï¼‰I/O æ“ä½œä¼˜åŒ–
ï¼ˆ1ï¼‰æ•°æ®å€¾æ–œ 
ï¼ˆ2ï¼‰Map è¿è¡Œæ—¶é—´å¤ªé•¿ï¼Œå¯¼è‡´ Reduce ç­‰å¾…è¿‡ä¹… 
ï¼ˆ3ï¼‰å°æ–‡ä»¶è¿‡å¤š



MapReduce å¸¸ç”¨è°ƒä¼˜å‚æ•°

### **MapReduceä¼˜åŒ–ï¼ˆä¸Šï¼‰**
1ï¼‰è‡ªå®šä¹‰åˆ†åŒºï¼Œå‡å°‘æ•°æ®å€¾æ–œ; å®šä¹‰ç±»ï¼Œç»§æ‰¿Partitioneræ¥å£ï¼Œé‡å†™getPartitionæ–¹æ³•

2ï¼‰å‡å°‘æº¢å†™çš„æ¬¡æ•°

```sh
mapreduce.task.io.sort.mb Shuffleçš„ç¯å½¢ç¼“å†²åŒºå¤§å°ï¼Œé»˜è®¤100mï¼Œ
å¯ä»¥æé«˜åˆ°200m mapreduce.map.sort.spill.percent ç¯å½¢ç¼“å†²åŒºæº¢å‡ºçš„é˜ˆå€¼ï¼Œé»˜è®¤80% ï¼Œå¯ä»¥æé«˜çš„90%
```

3ï¼‰å¢åŠ æ¯æ¬¡Mergeåˆå¹¶æ¬¡æ•°

```sh
mapreduce.task.io.sort.factoré»˜è®¤10ï¼Œå¯ä»¥æé«˜åˆ°20
```

4ï¼‰åœ¨ä¸å½±å“ä¸šåŠ¡ç»“æœçš„å‰ææ¡ä»¶ä¸‹å¯ä»¥æå‰é‡‡ç”¨Combiner

```java
job.setCombinerClass(xxxReducer.class);
```

5ï¼‰ä¸ºäº†å‡å°‘ç£ç›˜IOï¼Œå¯ä»¥é‡‡ç”¨Snappyæˆ–è€…LZOå‹ç¼©

```xml
conf.setBoolean("mapreduce.map.output.compress", true); 
conf.setClass("mapreduce.map.output.compress.codec", 
SnappyCodec.class,CompressionCodec.class);
```

6ï¼‰mapreduce.map.memory.mb é»˜è®¤MapTaskå†…å­˜ä¸Šé™1024MBå¯ä»¥æ ¹æ®128mæ•°æ®å¯¹åº”1Gå†…å­˜åŸåˆ™æé«˜è¯¥å†…å­˜.

7ï¼‰mapreduce.map.java.optsï¼šæ§åˆ¶MapTaskå †å†…å­˜å¤§å°ã€‚ï¼ˆå¦‚æœå†…å­˜ä¸å¤Ÿï¼Œ æŠ¥java.lang.OutOfMemoryErrorï¼‰

8ï¼‰mapreduce.map.cpu.vcores é»˜è®¤MapTaskçš„CPUæ ¸æ•°1ã€‚è®¡ç®—å¯†é›†å‹ä»» åŠ¡å¯ä»¥å¢åŠ CPUæ ¸æ•°

9ï¼‰å¼‚å¸¸é‡è¯• 
mapreduce.map.maxattemptsæ¯ä¸ªMap Taskæœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œä¸€æ—¦é‡è¯• æ¬¡æ•°è¶…è¿‡è¯¥å€¼ï¼Œåˆ™è®¤ä¸ºMap Taskè¿è¡Œå¤±è´¥ï¼Œé»˜è®¤å€¼ï¼š4ã€‚æ ¹æ®æœºå™¨ æ€§èƒ½é€‚å½“æé«˜



### **MapReduceä¼˜åŒ–ï¼ˆä¸‹ï¼‰**

![image-20211102123648984](Images/image-20211102123648984.png)

```xml
1ï¼‰mapreduce.reduce.shuffle.parallelcopiesæ¯ä¸ªReduceå»Map ä¸­æ‹‰å–æ•°æ®çš„å¹¶è¡Œæ•°ï¼Œé»˜è®¤å€¼æ˜¯5ã€‚å¯ä»¥æé«˜åˆ°10ã€‚ 

2ï¼‰mapreduce.reduce.shuffle.input.buffer.percent Bufferå¤§å°å Reduceå¯ç”¨å†…å­˜çš„æ¯”ä¾‹ï¼Œé»˜è®¤å€¼0.7ã€‚å¯ä»¥æé«˜åˆ°0.8

3ï¼‰mapreduce.reduce.shuffle.merge.percent Bufferä¸­çš„æ•°æ®è¾¾åˆ°å¤šå°‘æ¯”ä¾‹ å¼€å§‹å†™å…¥ç£ç›˜ï¼Œé»˜è®¤å€¼0.66ã€‚å¯ä»¥æé«˜åˆ°0.75

4ï¼‰mapreduce.reduce.memory.mb é»˜è®¤ReduceTaskå†…å­˜ä¸Šé™1024MBï¼Œ æ ¹æ®128mæ•°æ®å¯¹åº”1Gå†…å­˜åŸåˆ™ï¼Œé€‚å½“æé«˜å†…å­˜åˆ°4-6G

5ï¼‰mapreduce.reduce.java.optsï¼šæ§åˆ¶ReduceTaskå †å†…å­˜å¤§å°ã€‚ï¼ˆå¦‚æœå†… å­˜ä¸å¤Ÿï¼ŒæŠ¥ï¼šjava.lang.OutOfMemoryErrorï¼‰

6ï¼‰mapreduce.reduce.cpu.vcoresé»˜è®¤ReduceTaskçš„CPUæ ¸æ•°1ä¸ªã€‚å¯ ä»¥æé«˜åˆ°2-4ä¸ª

7ï¼‰mapreduce.reduce.maxattemptsæ¯ä¸ªReduce Taskæœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œ ä¸€æ—¦é‡è¯•æ¬¡æ•°è¶…è¿‡è¯¥å€¼ï¼Œåˆ™è®¤ä¸ºMap Taskè¿è¡Œå¤±è´¥ï¼Œé»˜è®¤å€¼ï¼š4ã€‚

8ï¼‰mapreduce.job.reduce.slowstart.completedmapså½“MapTaskå®Œæˆçš„æ¯” ä¾‹è¾¾åˆ°è¯¥å€¼åæ‰ä¼šä¸ºReduceTaskç”³è¯·èµ„æºã€‚é»˜è®¤æ˜¯0.05ã€‚

9ï¼‰mapreduce.task.timeoutå¦‚æœä¸€ä¸ªTaskåœ¨ä¸€å®šæ—¶é—´å†…æ²¡æœ‰ä»»ä½•è¿›å…¥ï¼Œ å³ä¸ä¼šè¯»å–æ–°çš„æ•°æ®ï¼Œä¹Ÿæ²¡æœ‰è¾“å‡ºæ•°æ®ï¼Œåˆ™è®¤ä¸ºè¯¥Taskå¤„äºBlockçŠ¶æ€ï¼Œ å¯èƒ½æ˜¯å¡ä½äº†ï¼Œä¹Ÿè®¸æ°¸è¿œä¼šå¡ä½ï¼Œä¸ºäº†é˜²æ­¢å› ä¸ºç”¨æˆ·ç¨‹åºæ°¸è¿œBlockä½ ä¸é€€å‡ºï¼Œåˆ™å¼ºåˆ¶è®¾ç½®äº†ä¸€ä¸ªè¯¥è¶…æ—¶æ—¶é—´ï¼ˆå•ä½æ¯«ç§’ï¼‰ï¼Œé»˜è®¤æ˜¯600000 ï¼ˆ10åˆ†é’Ÿï¼‰ã€‚å¦‚æœä½ çš„ç¨‹åºå¯¹æ¯æ¡è¾“å…¥æ•°æ®çš„å¤„ç†æ—¶é—´è¿‡é•¿ï¼Œå»ºè®®å°† è¯¥å‚æ•°è°ƒå¤§ã€‚

10ï¼‰å¦‚æœå¯ä»¥ä¸ç”¨Reduceï¼Œå°½å¯èƒ½ä¸ç”¨
```

### **MapReduce æ•°æ®å€¾æ–œé—®é¢˜**

1ï¼‰æ•°æ®å€¾æ–œç°è±¡

æ•°æ®é¢‘ç‡å€¾æ–œâ€”â€”æŸä¸€ä¸ªåŒºåŸŸçš„æ•°æ®é‡è¦è¿œè¿œå¤§äºå…¶ä»–åŒºåŸŸã€‚

æ•°æ®å¤§å°å€¾æ–œâ€”â€”éƒ¨åˆ†è®°å½•çš„å¤§å°è¿œè¿œå¤§äºå¹³å‡å€¼ã€‚



2ï¼‰å‡å°‘æ•°æ®å€¾æ–œçš„æ–¹æ³•
ï¼ˆ1ï¼‰é¦–å…ˆæ£€æŸ¥æ˜¯å¦ç©ºå€¼è¿‡å¤šé€ æˆçš„æ•°æ®å€¾æ–œ

â€‹		ç”Ÿäº§ç¯å¢ƒï¼Œå¯ä»¥ç›´æ¥è¿‡æ»¤æ‰ç©ºå€¼ï¼›å¦‚æœæƒ³ä¿ç•™ç©ºå€¼ï¼Œå°±è‡ªå®šä¹‰åˆ†åŒºï¼Œå°†ç©ºå€¼åŠ éšæœºæ•°æ‰“ æ•£ã€‚

â€‹		æœ€åå†äºŒæ¬¡èšåˆã€‚

ï¼ˆ2ï¼‰èƒ½åœ¨ map é˜¶æ®µæå‰å¤„ç†ï¼Œæœ€å¥½å…ˆåœ¨ Map é˜¶æ®µå¤„ç†ã€‚å¦‚ï¼šCombinerã€MapJoin



3ï¼‰è®¾ç½®å¤šä¸ª reduce ä¸ªæ•°

Hadoop-Yarn ç”Ÿäº§ç»éªŒ

1ï¼‰è°ƒä¼˜å‚æ•°åˆ—è¡¨

```xml
ï¼ˆ1ï¼‰Resourcemanager ç›¸å…³
yarn.resourcemanager.scheduler.client.thread-count ResourceManager å¤„ç†è°ƒåº¦ å™¨è¯·æ±‚çš„çº¿ç¨‹æ•°é‡ 
yarn.resourcemanager.scheduler.class é…ç½®è°ƒåº¦å™¨

ï¼ˆ2ï¼‰Nodemanager ç›¸å…³
yarn.nodemanager.resource.memory-mb NodeManager ä½¿ç”¨å†…å­˜æ•°
yarn.nodemanager.resource.system-reserved-memory-mb NodeManager ä¸ºç³»ç»Ÿä¿ç•™å¤šå°‘å†…å­˜ï¼Œå’Œä¸Šä¸€ä¸ªå‚æ•°äºŒè€…å–ä¸€å³å¯

yarn.nodemanager.resource.cpu-vcores NodeManager ä½¿ç”¨ CPU æ ¸æ•°
yarn.nodemanager.resource.count-logical-processors-as-cores æ˜¯å¦å°†è™šæ‹Ÿæ ¸æ•°å½“ä½œ CPU æ ¸æ•°
yarn.nodemanager.resource.pcores-vcores-multiplier è™šæ‹Ÿæ ¸æ•°å’Œç‰©ç†æ ¸æ•°ä¹˜æ•°ï¼Œä¾‹å¦‚ï¼š4 æ ¸ 8 çº¿ç¨‹ï¼Œè¯¥å‚æ•°å°±åº”è®¾ä¸º 2

yarn.nodemanager.resource.detect-hardware-capabilities æ˜¯å¦è®© yarn è‡ªå·±æ£€æµ‹ç¡¬ä»¶è¿›è¡Œé…ç½®
yarn.nodemanager.pmem-check-enabled æ˜¯å¦å¼€å¯ç‰©ç†å†…å­˜æ£€æŸ¥é™åˆ¶ container
yarn.nodemanager.vmem-check-enabled æ˜¯å¦å¼€å¯è™šæ‹Ÿå†…å­˜æ£€æŸ¥é™åˆ¶ container
yarn.nodemanager.vmem-pmem-ratio è™šæ‹Ÿå†…å­˜ç‰©ç†å†…å­˜æ¯”ä¾‹

ï¼ˆ3ï¼‰Container å®¹å™¨ç›¸å…³
yarn.scheduler.minimum-allocation-mb å®¹å™¨æœ€å°å†…å­˜
yarn.scheduler.maximum-allocation-mb å®¹å™¨æœ€å¤§å†…å­˜
yarn.scheduler.minimum-allocation-vcores å®¹å™¨æœ€å°æ ¸æ•°
yarn.scheduler.maximum-allocation-vcores å®¹å™¨æœ€å¤§æ ¸æ•°
```



## **Hadoop å°æ–‡ä»¶è§£å†³æ–¹æ¡ˆ**

1ï¼‰åœ¨æ•°æ®é‡‡é›†çš„æ—¶å€™ï¼Œå°±å°†å°æ–‡ä»¶æˆ–å°æ‰¹æ•°æ®åˆæˆå¤§æ–‡ä»¶å†ä¸Šä¼  HDFSï¼ˆæ•°æ®æºå¤´ï¼‰



2ï¼‰Hadoop Archiveï¼ˆå­˜å‚¨æ–¹å‘ï¼‰

æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„å°†å°æ–‡ä»¶æ”¾å…¥ HDFS å—ä¸­çš„æ–‡ä»¶å­˜æ¡£å·¥å…·ï¼Œèƒ½å¤Ÿå°†å¤šä¸ªå°æ–‡ä»¶æ‰“åŒ…æˆä¸€ä¸ª HAR æ–‡ä»¶ï¼Œä»è€Œè¾¾åˆ°å‡å°‘ NameNode çš„å†…å­˜ä½¿ç”¨



3ï¼‰CombineTextInputFormatï¼ˆè®¡ç®—æ–¹å‘ï¼‰

CombineTextInputFormat ç”¨äºå°†å¤šä¸ªå°æ–‡ä»¶åœ¨åˆ‡ç‰‡è¿‡ç¨‹ä¸­ç”Ÿæˆä¸€ä¸ªå•ç‹¬çš„åˆ‡ç‰‡æˆ–è€…å°‘é‡çš„åˆ‡ç‰‡ã€‚



4ï¼‰å¼€å¯ uber æ¨¡å¼ï¼Œå®ç° JVM é‡ç”¨ï¼ˆè®¡ç®—æ–¹å‘ï¼‰é»˜è®¤æƒ…å†µä¸‹ï¼Œæ¯ä¸ª Task ä»»åŠ¡éƒ½éœ€è¦å¯åŠ¨ä¸€ä¸ª JVM æ¥è¿è¡Œï¼Œå¦‚æœ Task ä»»åŠ¡è®¡ç®—çš„æ•°æ®é‡å¾ˆå°ï¼Œæˆ‘ä»¬å¯ä»¥è®©åŒä¸€ä¸ª Job çš„å¤šä¸ª Task è¿è¡Œåœ¨ä¸€ä¸ª JVM ä¸­ï¼Œä¸å¿…ä¸ºæ¯ä¸ª Task éƒ½å¼€å¯
ä¸€ä¸ª JVMã€‚



ï¼ˆ1ï¼‰æœªå¼€å¯ uber æ¨¡å¼ï¼Œåœ¨/input è·¯å¾„ä¸Šä¸Šä¼ å¤šä¸ªå°æ–‡ä»¶å¹¶æ‰§è¡Œ wordcount ç¨‹åº

```sh
hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output2 
```

ï¼ˆ2ï¼‰è§‚å¯Ÿæ§åˆ¶å°

```sh
2021-02-14 16:13:50,607 INFO mapreduce.Job: Job job_1613281510851_0002 running in uber mode : false
```


ï¼ˆ3ï¼‰å¼€å¯ uber æ¨¡å¼ï¼Œåœ¨ mapred-site.xml ä¸­æ·»åŠ å¦‚ä¸‹é…ç½®


```xml
<!-- å¼€å¯ uber æ¨¡å¼ï¼Œé»˜è®¤å…³é—­ -->
<property>
 <name>mapreduce.job.ubertask.enable</name>
 <value>true</value>
</property>
<!-- uber æ¨¡å¼ä¸­æœ€å¤§çš„ mapTask æ•°é‡ï¼Œå¯å‘ä¸‹ä¿®æ”¹ -->
<property>
 <name>mapreduce.job.ubertask.maxmaps</name>
 <value>9</value>
</property>
<!-- uber æ¨¡å¼ä¸­æœ€å¤§çš„ reduce æ•°é‡ï¼Œå¯å‘ä¸‹ä¿®æ”¹ -->
<property>
 <name>mapreduce.job.ubertask.maxreduces</name>
 <value>1</value>
</property>
<!-- uber æ¨¡å¼ä¸­æœ€å¤§çš„è¾“å…¥æ•°æ®é‡ï¼Œé»˜è®¤ä½¿ç”¨ dfs.blocksize çš„å€¼ï¼Œå¯å‘ä¸‹ä¿®æ”¹ -->
<property>
 <name>mapreduce.job.ubertask.maxbytes</name>
 <value></value>
</property>
```

## **æµ‹è¯• MapReduce è®¡ç®—æ€§èƒ½**

ä½¿ç”¨ Sort ç¨‹åºè¯„æµ‹ MapReduce

æ³¨ï¼šä¸€ä¸ªè™šæ‹Ÿæœºä¸è¶…è¿‡ 150G ç£ç›˜å°½é‡ä¸è¦æ‰§è¡Œè¿™æ®µä»£ç 

ï¼ˆ1ï¼‰ä½¿ç”¨ RandomWriter æ¥äº§ç”Ÿéšæœºæ•°ï¼Œæ¯ä¸ªèŠ‚ç‚¹è¿è¡Œ 10 ä¸ª Map ä»»åŠ¡ï¼Œæ¯ä¸ª Map äº§ ç”Ÿå¤§çº¦ 1G å¤§å°çš„äºŒè¿›åˆ¶éšæœºæ•°

```sh
hadoop jar /opt/module/hadoop3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples3.1.3.jar randomwriter random-data
```

ï¼ˆ2ï¼‰æ‰§è¡Œ Sort ç¨‹åº

```sh
hadoop jar /opt/module/hadoop3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples3.1.3.jar sort random-data sorted-data
```

ï¼ˆ3ï¼‰éªŒè¯æ•°æ®æ˜¯å¦çœŸæ­£æ’å¥½åºäº†

```sh
hadoop jar /opt/module/hadoop3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-clientjobclient-3.1.3-tests.jar testmapredsort -sortInput random-data -sortOutput sorted-data
```



## **HDFS å‚æ•°è°ƒä¼˜**

ï¼ˆ1ï¼‰ä¿®æ”¹ï¼šhadoop-env.sh

```sh
export HDFS_NAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS -Xmx1024m"

export HDFS_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS-Xmx1024m"
```

ï¼ˆ2ï¼‰ä¿®æ”¹ hdfs-site.xml 

```xml
<!-- NameNode æœ‰ä¸€ä¸ªå·¥ä½œçº¿ç¨‹æ± ï¼Œé»˜è®¤å€¼æ˜¯ 10 -->
<property>
 <name>dfs.namenode.handler.count</name>
 <value>21</value>
</property>
```

ï¼ˆ3ï¼‰ä¿®æ”¹ core-site.xml

```xml
<!-- é…ç½®åƒåœ¾å›æ”¶æ—¶é—´ä¸º 60 åˆ†é’Ÿ -->
<property>
 <name>fs.trash.interval</name>
<value>60</value>
</property>
```



## **MapReduce å‚æ•°è°ƒä¼˜**

ï¼ˆ1ï¼‰ä¿®æ”¹ mapred-site.xm

```xml
<!-- ç¯å½¢ç¼“å†²åŒºå¤§å°ï¼Œé»˜è®¤ 100m -->
<property>
 <name>mapreduce.task.io.sort.mb</name>
 <value>100</value>
</property>
<!-- ç¯å½¢ç¼“å†²åŒºæº¢å†™é˜ˆå€¼ï¼Œé»˜è®¤ 0.8 -->
<property>
 <name>mapreduce.map.sort.spill.percent</name>
 <value>0.80</value>
</property>
<!-- merge åˆå¹¶æ¬¡æ•°ï¼Œé»˜è®¤ 10 ä¸ª -->
<property>
 <name>mapreduce.task.io.sort.factor</name>
 <value>10</value>
</property>
<!-- maptask å†…å­˜ï¼Œé»˜è®¤ 1gï¼› maptask å †å†…å­˜å¤§å°é»˜è®¤å’Œè¯¥å€¼å¤§å°ä¸€è‡´mapreduce.map.java.opts -->
<property>
 <name>mapreduce.map.memory.mb</name>
 <value>-1</value>
 </description>
</property>
<!-- matask çš„ CPU æ ¸æ•°ï¼Œé»˜è®¤ 1 ä¸ª -->
<property>
 <name>mapreduce.map.cpu.vcores</name>
 <value>1</value>
</property>
<!-- matask å¼‚å¸¸é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤ 4 æ¬¡ -->
<property>
 <name>mapreduce.map.maxattempts</name>
 <value>4</value>
</property>
<!-- æ¯ä¸ª Reduce å» Map ä¸­æ‹‰å–æ•°æ®çš„å¹¶è¡Œæ•°ã€‚é»˜è®¤å€¼æ˜¯ 5 -->
<property>
 <name>mapreduce.reduce.shuffle.parallelcopies</name>
<value>5</value>
</property>
<!-- Buffer å¤§å°å  Reduce å¯ç”¨å†…å­˜çš„æ¯”ä¾‹ï¼Œé»˜è®¤å€¼ 0.7 -->
<property>
 <name>mapreduce.reduce.shuffle.input.buffer.percent</name>
 <value>0.70</value>
</property>
<!-- Buffer ä¸­çš„æ•°æ®è¾¾åˆ°å¤šå°‘æ¯”ä¾‹å¼€å§‹å†™å…¥ç£ç›˜ï¼Œé»˜è®¤å€¼ 0.66ã€‚ -->
<property>
 <name>mapreduce.reduce.shuffle.merge.percent</name>
 <value>0.66</value>
</property>
<!-- reducetask å†…å­˜ï¼Œé»˜è®¤ 1gï¼›reducetask å †å†…å­˜å¤§å°é»˜è®¤å’Œè¯¥å€¼å¤§å°ä¸€è‡´mapreduce.reduce.java.opts -->
<property>
 <name>mapreduce.reduce.memory.mb</name>
 <value>-1</value>
</property>
<!-- reducetask çš„ CPU æ ¸æ•°ï¼Œé»˜è®¤ 1 ä¸ª -->
<property>
 <name>mapreduce.reduce.cpu.vcores</name>
 <value>2</value>
</property>
<!-- reducetask å¤±è´¥é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤ 4 æ¬¡ -->
<property>
 <name>mapreduce.reduce.maxattempts</name>
 <value>4</value>
</property>
<!-- å½“ MapTask å®Œæˆçš„æ¯”ä¾‹è¾¾åˆ°è¯¥å€¼åæ‰ä¼šä¸º ReduceTask ç”³è¯·èµ„æºã€‚é»˜è®¤æ˜¯ 0.05-->
<property>
 <name>mapreduce.job.reduce.slowstart.completedmaps</name>
 <value>0.05</value>
</property>
<!-- å¦‚æœç¨‹åºåœ¨è§„å®šçš„é»˜è®¤ 10 åˆ†é’Ÿå†…æ²¡æœ‰è¯»åˆ°æ•°æ®ï¼Œå°†å¼ºåˆ¶è¶…æ—¶é€€å‡º -->
<property>
 <name>mapreduce.task.timeout</name>
 <value>600000</value>
</property>
```

## **Yarn å‚æ•°è°ƒä¼˜**

ï¼ˆ1ï¼‰ä¿®æ”¹ yarn-site.xml é…ç½®å‚æ•°å¦‚ä¸‹ï¼š

```xml
<!-- é€‰æ‹©è°ƒåº¦å™¨ï¼Œé»˜è®¤å®¹é‡ -->
<property>
<description>The class to use as the resource scheduler.</description>
<name>yarn.resourcemanager.scheduler.class</name>
<value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capaci
ty.CapacityScheduler</value>
</property>
<!-- ResourceManager å¤„ç†è°ƒåº¦å™¨è¯·æ±‚çš„çº¿ç¨‹æ•°é‡,é»˜è®¤ 50ï¼›å¦‚æœæäº¤çš„ä»»åŠ¡æ•°å¤§äº 50ï¼Œå¯ä»¥å¢åŠ è¯¥å€¼ï¼Œä½†æ˜¯ä¸èƒ½è¶…è¿‡ 3 å° * 4 çº¿ç¨‹ = 12 çº¿ç¨‹ï¼ˆå»é™¤å…¶ä»–åº”ç”¨ç¨‹åºå®é™…ä¸èƒ½è¶…è¿‡ 8ï¼‰ -->
<property>
<description>Number of threads to handle schedulerinterface.</description>
<name>yarn.resourcemanager.scheduler.client.thread-count</name>
<value>8</value>
</property>
<!-- æ˜¯å¦è®© yarn è‡ªåŠ¨æ£€æµ‹ç¡¬ä»¶è¿›è¡Œé…ç½®ï¼Œé»˜è®¤æ˜¯ falseï¼Œå¦‚æœè¯¥èŠ‚ç‚¹æœ‰å¾ˆå¤šå…¶ä»–åº”ç”¨ç¨‹åºï¼Œå»ºè®®æ‰‹åŠ¨é…ç½®ã€‚å¦‚æœè¯¥èŠ‚ç‚¹æ²¡æœ‰å…¶ä»–åº”ç”¨ç¨‹åºï¼Œå¯ä»¥é‡‡ç”¨è‡ªåŠ¨ -->
<property>
<name>yarn.nodemanager.resource.detect-hardware-capabilities</name>
<value>false</value>
</property>
<!-- æ˜¯å¦å°†è™šæ‹Ÿæ ¸æ•°å½“ä½œ CPU æ ¸æ•°ï¼Œé»˜è®¤æ˜¯ falseï¼Œé‡‡ç”¨ç‰©ç† CPU æ ¸æ•° -->
<property>
<name>yarn.nodemanager.resource.count-logical-processors-ascores</name>
<value>false</value>
</property>
<!-- è™šæ‹Ÿæ ¸æ•°å’Œç‰©ç†æ ¸æ•°ä¹˜æ•°ï¼Œé»˜è®¤æ˜¯ 1.0 -->
<property>
<name>yarn.nodemanager.resource.pcores-vcores-multiplier</name>
<value>1.0</value>
</property>
<!-- NodeManager ä½¿ç”¨å†…å­˜æ•°ï¼Œé»˜è®¤ 8Gï¼Œä¿®æ”¹ä¸º 4G å†…å­˜ -->
<property>
<name>yarn.nodemanager.resource.memory-mb</name>
<value>4096</value>
</property>
<!-- nodemanager çš„ CPU æ ¸æ•°ï¼Œä¸æŒ‰ç…§ç¡¬ä»¶ç¯å¢ƒè‡ªåŠ¨è®¾å®šæ—¶é»˜è®¤æ˜¯ 8 ä¸ªï¼Œä¿®æ”¹ä¸º 4 ä¸ª -->
<property>
<name>yarn.nodemanager.resource.cpu-vcores</name>
<value>4</value>
</property>
<!-- å®¹å™¨æœ€å°å†…å­˜ï¼Œé»˜è®¤ 1G -->
<property>
<name>yarn.scheduler.minimum-allocation-mb</name>
<value>1024</value>
</property>
<!-- å®¹å™¨æœ€å¤§å†…å­˜ï¼Œé»˜è®¤ 8Gï¼Œä¿®æ”¹ä¸º 2G -->
<property>
<name>yarn.scheduler.maximum-allocation-mb</name>
<value>2048</value>
</property>
<!-- å®¹å™¨æœ€å° CPU æ ¸æ•°ï¼Œé»˜è®¤ 1 ä¸ª -->
<property>
</description>
<name>yarn.scheduler.minimum-allocation-vcores</name>
<value>1</value>
</property>
<!-- å®¹å™¨æœ€å¤§ CPU æ ¸æ•°ï¼Œé»˜è®¤ 4 ä¸ªï¼Œä¿®æ”¹ä¸º 2 ä¸ª -->
<property>
<name>yarn.scheduler.maximum-allocation-vcores</name>
<value>2</value>
</property>
<!-- è™šæ‹Ÿå†…å­˜æ£€æŸ¥ï¼Œé»˜è®¤æ‰“å¼€ï¼Œä¿®æ”¹ä¸ºå…³é—­ -->
<property>
<description>Whether virtual memory limits will be enforced for
containers.</description>
<name>yarn.nodemanager.vmem-check-enabled</name>
<value>false</value>
</property>
<!-- è™šæ‹Ÿå†…å­˜å’Œç‰©ç†å†…å­˜è®¾ç½®æ¯”ä¾‹,é»˜è®¤ 2.1 -->
<property>
<name>yarn.nodemanager.vmem-pmem-ratio</name>
<value>2.1</value>
</property>
```

