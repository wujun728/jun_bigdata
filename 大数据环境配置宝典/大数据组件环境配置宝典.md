ğŸ¤”Linux(Cenots7)ç¯å¢ƒé…ç½®

å®‰è£…å¥½VmWareè™šæ‹Ÿæœºåï¼Œæ“ä½œæ­¥éª¤

VmWareæ¿€æ´»ç ï¼š

```scala
ZF3R0-FHED2-M80TY-8QYGC-NPKYF
YF390-0HF8P-M81RQ-2DXQE-M2UT6
ZF71R-DMX85-08DQY-8YMNC-PPHV8
```

1.ä¿®æ”¹ä¸»æœºå

```shell
vi /etc/hostname
node01
```

2.æ·»åŠ ä¸»æœºä¹‹é—´çš„æ˜ å°„

```shell
vi /etc/hosts
192.168.244.121 www.node01.com node01
192.168.244.122 www.node02.com node02
192.168.244.123 www.node03.com node03
```

3.å…³é—­selinux

```shell
vi /etc/selinux/config
SELINUX=enforcing
æ”¹ä¸º
SELINUX=disabled
```

4/å…³é—­é˜²ç«å¢™

```shell
systemctl status firewalld
systemctl stop firewalld
systemctl disable firewalld
```

5.ä¿®æ”¹rootæƒé™

```shell
vi /etc/sudoers
## Allows people in group wheel to run all commands
%wheel ALL=(ALL) ALL
dsjprs ALL=(ALL) ALL

## Allow root to run any commands anywhere
root ALL=(ALL) ALL
dsjprs ALL=(ALL) ALL
```

6.é…ç½®yum

```shell
CentOS-Base.repo

#The mirror system uses the connecting IP address of the client and thed

update status of each mirror to pick mirrors that are updated to and

geographically close to the client.  You should use this for CentOS updates

unless you are manually picking other mirrors.

#If the mirrorlist= does not work for you, as a fall back you can try the

remarked out baseurl= line instead.

#
#

[base]
name=CentOS-$releasever - Base
baseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/os/$basearch/
#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=os
enabled=1
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-7

#released updates
[updates]
name=CentOS-$releasever - Updates
baseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/updates/$basearch/
#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=updates
enabled=1
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-7



#additional packages that may be useful
[extras]
name=CentOS-$releasever - Extras
baseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/extras/$basearch/
#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=extras
enabled=1
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-7



#additional packages that extend functionality of existing packages
[centosplus]
name=CentOS-$releasever - Plus
baseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/centosplus/$basearch/
#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=centosplus
gpgcheck=1
enabled=0
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-7
```

7.å»ºç«‹ç¼“å­˜

```shell
yum clean all
yum makecache
```

8.æ›´æ–°

```shell
yum update
```

9.é…ç½®ç½‘ç»œNATè¿æ¥

```shell
IPADDR=
NETMASK=
GATEWAY=
DNS1=
HWADDR=
#=========================================
EVICE="eth0"
BOOTPROTO="static"
NM_CONTROLLED="yes"
ONBOOT="yes"
TYPE="Ethernet"
IPADDR=192.168.188.130
NETMASK=255.255.255.0
GATEWAY=192.168.188.2
DNS1=202.106.0.20
HWADDR=00:0C:29:84:0C:21ã€€
```

10.ä¿®æ”¹ä¸»æœºåé…ç½®æ–‡ä»¶

```shell
vi /etc/sysconfig/network
NETWORKING=yes
HOSUNAME=dsjprs
```

11.ä¿®æ”¹DNS

```shell
vi /etc/resolv.conf
nameserver 192.168.244.2
```

12.é‡å¯ç½‘ç»œæœåŠ¡

```shell
service network restart
```

13.å¦‚æœå®‰è£…çš„ç²¾ç®€ç‰ˆé•œåƒï¼Œåˆ™å®‰è£…

å®‰è£…å‘½ä»¤ï¼š

```shell
yum install -y epel-release
```

å®‰è£… net-toolï¼šå·¥å…·åŒ…é›†åˆï¼ŒåŒ…å«ifconfigç­‰å‘½ä»¤

å®‰è£…å‘½ä»¤ï¼š

```shell
yum install -y net-tools
```

å®‰è£…vimï¼šç¼–è¾‘å™¨

å®‰è£…å‘½ä»¤ï¼š

```shell
yum install -y vim
```

é›†ç¾¤æ—¶é—´åŒæ­¥ï¼ˆ3å°æœºå™¨ï¼‰ 

```shell
yum -y install ntpdate 
ntpdate ntp4.aliyun.com
```

### Ubuntu ç¯å¢ƒé…ç½®

```shell
ubuntu é™æ€ç½‘ç»œé…ç½®
# Let NetworkManager manage all devices on this system
network:
  version: 2
  renderer: NetworkManager
  ethernets:
    ens33:
         dhcp4: no
		 # è¿™é‡Œå¡«å†™é™æ€ ipï¼Œç½‘æ®µå’Œå‰é¢çœ‹åˆ°çš„ç½‘å…³ ip ä¸€è‡´å³å¯
         addresses: [192.168.255.7/24]
		 # è¿™é‡Œå¡«å†™çš„ ip å°±æ˜¯å‰é¢ NAT è®¾ç½®é‡Œçš„ç½‘å…³ ip
         gateway4: 192.168.255.2
         nameservers:
             addresses: [8.8.8.8, 8.8.4.4]
```

é˜¿é‡Œäº‘é•œåƒ

```shell
########   Ubuntu18.04 é˜¿é‡Œäº‘é•œåƒæº   ###############
deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse
deb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse
deb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse
deb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse
deb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse
########   é˜¿é‡Œäº‘é•œåƒæº END  ###############
```

### ç¼–ç¨‹è¯­è¨€ç¯å¢ƒé…ç½®

1.windowsä¸Šé…ç½®javaç¯å¢ƒ

```shell
JAVA_HOME= C:\Program Files (x86)\Java\jdk1.8.0_91
// è¦æ ¹æ®è‡ªå·±çš„å®é™…è·¯å¾„é…ç½®
CLASSPATH= .;%JAVA_HOME%\lib\dt.jar;%JAVA_HOME%\lib\tools.jar;   
//è®°å¾—å‰é¢æœ‰ä¸ª"."
PATH= %JAVA_HOME%\bin;%JAVA_HOME%\jre\bin;
```

2.linuxä¸Šé…ç½®javaç¯å¢ƒ

```shell
export JAVA_HOME=/usr/share/jdk1.6.0_14
export PATH=$JAVA_HOME/bin:$PATH
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
```

3.windowsä¸Šé…ç½®scalaç¯å¢ƒ

```shell
%SCALA_HOME%=c:\Progra~1\Scala
%PATH%=%PATH%;%SCALA_HOME%\bin
```

4.linuxä¸Šé…ç½®scalaç¯å¢ƒ

```shell
export SCALA_HOME=/usr/share/jdk1.6.0_14
export PATH=$SCALA_HOME/bin:$PATH
```

5.windowsä¸Šé…ç½®pythonç¯å¢ƒ

```shell
path = c:\python
path = %path%
```

6.linuxä¸Šé…ç½®pythonç¯å¢ƒ

```shell
export PATH="$PATH:/usr/local/bin/python"
```

7.windowsä¸Šé…ç½®goç¯å¢ƒ

```shell
GOROOT= D:\WindowsSoftware\Golang
GOPATH = D:\WindowsSoftware\GoPATH

GOPATH = %GOPATH%bin
GIROOT = %GOROOT%bin
```

GOPATH åˆ›å»º bin pkg srcæ–‡ä»¶ç›®å½•

binï¼šä¸»è¦å­˜æ”¾å¯æ‰§è¡Œæ–‡ä»¶ã€‚
pkgï¼šå­˜æ”¾ç¼–è¯‘å¥½çš„åº“æ–‡ä»¶, ä¸»è¦æ˜¯*.aæ–‡ä»¶ã€‚
srcï¼šä¸‹ä¸»è¦å­˜æ”¾goçš„æºæ–‡ä»¶ã€‚
æ­¤å¤–è¿˜éœ€è¦æ³¨æ„çš„æ˜¯ä¸è¦è®²GOROOTè®¾ç½®åŸGoè¯­è¨€çš„è·¯å¾„ï¼Œé¿å…å‡ºç°ä¸å¿…è¦çš„å†²çªã€‚

8.linuxä¸Šé…ç½®goç¯å¢ƒ

```shell
export GOROOT=/usr/local/go
export PATH=$PATH:$GOROOT/bin
```

### Linuxéƒ¨ç½²ç»„ä»¶

#### å®‰è£…Hadoopé›†ç¾¤

#### åœ¨Linuxç³»ç»Ÿä¸Š

ï¼ˆ1ï¼‰ä¿®æ”¹ä¸»æœºåç§°

```shell
[root@localhost ~]# vim /etc/hostname
node01
```

ï¼ˆ2ï¼‰é…ç½®Linuxå…‹éš†æœºä¸»æœºåç§°æ˜ å°„hostsæ–‡ä»¶ï¼Œæ‰“å¼€/etc/hosts

```shell
[root@node01~]# vim /etc/hosts
# æ·»åŠ å¦‚ä¸‹å†…å®¹
192.168.244.121 www.node01.com node01
192.168.244.122 www.node02.com node02
192.168.244.123 www.node03.com node03
```

#### åœ¨windowsç³»ç»Ÿä¸Š

ï¼ˆ1ï¼‰å¦‚æœæ“ä½œç³»ç»Ÿæ˜¯window7ï¼Œå¯ä»¥ç›´æ¥ä¿®æ”¹ 

â€‹	â‘ è¿›å…¥C:\Windows\System32\drivers\etcè·¯å¾„

â€‹	â‘¡æ‰“å¼€hostsæ–‡ä»¶å¹¶æ·»åŠ å¦‚ä¸‹å†…å®¹ï¼Œç„¶åä¿å­˜

```shell
192.168.244.121 www.node01.com node01
192.168.244.122 www.node02.com node02
192.168.244.123 www.node03.com node03
```

ï¼ˆ2ï¼‰å¦‚æœæ“ä½œç³»ç»Ÿæ˜¯window10ï¼Œå…ˆæ‹·è´å‡ºæ¥ï¼Œä¿®æ”¹ä¿å­˜ä»¥åï¼Œå†è¦†ç›–å³å¯

â€‹	â‘ è¿›å…¥C:\Windows\System32\drivers\etcè·¯å¾„

â€‹	â‘¡æ‹·è´hostsæ–‡ä»¶åˆ°æ¡Œé¢

â€‹	â‘¢æ‰“å¼€æ¡Œé¢hostsæ–‡ä»¶å¹¶æ·»åŠ å¦‚ä¸‹å†…å®¹

```shell
192.168.244.121 www.node01.com node01
192.168.244.122 www.node02.com node02
192.168.244.123 www.node03.com node03
```

â‘£å°†æ¡Œé¢hostsæ–‡ä»¶è¦†ç›–C:\Windows\System32\drivers\etcè·¯å¾„hostsæ–‡ä»¶

#### 1)SSHæ— å¯†ç™»å½•é…ç½®

ï¼ˆ1ï¼‰node01ä¸Šç”Ÿæˆå…¬é’¥å’Œç§é’¥ï¼š

```shell
[dsjprs@node01 .ssh]$ ssh-keygen -t rsa
```

ç„¶åæ•²ï¼ˆä¸‰ä¸ªå›è½¦ï¼‰ï¼Œå°±ä¼šç”Ÿæˆä¸¤ä¸ªæ–‡ä»¶id_rsaï¼ˆç§é’¥ï¼‰ã€id_rsa.pubï¼ˆå…¬é’¥ï¼‰

ï¼ˆ2ï¼‰å°†node01å…¬é’¥æ‹·è´åˆ°è¦å…å¯†ç™»å½•çš„ç›®æ ‡æœºå™¨ä¸Š

```shell
[dsjprs@node01 .ssh]$ ssh-copy-id node01
[dsjprs@node01 .ssh]$ ssh-copy-id node02
[dsjprs@node01 .ssh]$ ssh-copy-id node03
```

ï¼ˆ3ï¼‰node02ä¸Šç”Ÿæˆå…¬é’¥å’Œç§é’¥ï¼š

```shell
[dsjprs@node02 .ssh]$ ssh-keygen -t rsa
```

ç„¶åæ•²ï¼ˆä¸‰ä¸ªå›è½¦ï¼‰ï¼Œå°±ä¼šç”Ÿæˆä¸¤ä¸ªæ–‡ä»¶id_rsaï¼ˆç§é’¥ï¼‰ã€id_rsa.pubï¼ˆå…¬é’¥ï¼‰

ï¼ˆ4ï¼‰å°†node02å…¬é’¥æ‹·è´åˆ°è¦å…å¯†ç™»å½•çš„ç›®æ ‡æœºå™¨ä¸Š

```shell
[dsjprs@node02 .ssh]$ ssh-copy-id node01
[dsjprs@node02 .ssh]$ ssh-copy-id node02
[dsjprs@node02 .ssh]$ ssh-copy-id node03
```

#### 2) æå‰åˆ›å»ºæ–‡ä»¶å¤¹module sofware

ï¼ˆ1ï¼‰åœ¨/optç›®å½•ä¸‹åˆ›å»ºmoduleã€softwareæ–‡ä»¶å¤¹

```shell
[root@node01 ~]# mkdir /opt/module
[root@node01 ~]# mkdir /opt/software
```

ï¼ˆ2ï¼‰ä¿®æ”¹moduleã€softwareæ–‡ä»¶å¤¹çš„æ‰€æœ‰è€…å’Œæ‰€å±ç»„å‡ä¸ºdsjprsç”¨æˆ·

```shell
[root@node01 ~]# chown dsjprs:dsjprs /opt/module 
[root@node01 ~]# chown dsjprs:dsjprs /opt/software
```



#### 3) xsyncé›†ç¾¤åˆ†å‘è„šæœ¬

â‘ åœ¨å®¶ç›®å½•/home/dsjprsä¸‹åˆ›å»ºbinæ–‡ä»¶å¤¹

```shell
[dsjprs@node01 ~]$ mkdir bin
```

â‘¡åœ¨/home/dsjprs/binç›®å½•ä¸‹åˆ›å»ºxsyncæ–‡ä»¶ï¼Œä»¥ä¾¿å…¨å±€è°ƒç”¨

```shell
[dsjprs@node01 ~]$cd /home/dsjprs/bin
[dsjprs@node01 ~]$vim xsync.sh
```

åœ¨xsync.shæ–‡ä»¶ä¸­ç¼–å†™å¦‚ä¸‹ä»£ç 

```shell
#!/bin/bash
#1. åˆ¤æ–­å‚æ•°ä¸ªæ•°
if [ $# -lt 1 ]
then
  echo Not Enough Arguement!
  exit;
fi
#2. éå†é›†ç¾¤æ‰€æœ‰æœºå™¨
for host in node01 node02 node03
do
  echo ====================  $host  ====================
  #3. éå†æ‰€æœ‰ç›®å½•ï¼ŒæŒ¨ä¸ªå‘é€
  for file in $@
  do
    #4 åˆ¤æ–­æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if [ -e $file ]
    then
      #5. è·å–çˆ¶ç›®å½•
      pdir=$(cd -P $(dirname $file); pwd)
      #6. è·å–å½“å‰æ–‡ä»¶çš„åç§°
      fname=$(basename $file)
      ssh $host "mkdir -p $pdir"
      rsync -av $pdir/$fname $host:$pdir
    else
      echo $file does not exists!
    fi
  done
done
```

â‘¢ä¿®æ”¹è„šæœ¬xsyncå…·æœ‰æ‰§è¡Œæƒé™

```shell
[dsjprs@node01 bin]$ chmod +x xsync
```

â‘£æµ‹è¯•è„šæœ¬

```shell
[dsjprs@node01 bin]$ xsync xsync
```

**åœ¨hadoop102çš„/home/dsjprsç›®å½•ä¸‹åˆ›å»ºbinç›®å½•ï¼Œè¿™æ ·è„šæœ¬å¯ä»¥åœ¨æœåŠ¡å™¨çš„ä»»ä½•ç›®å½•æ‰§è¡Œã€‚**

```shell
[dsjprs@node01 ~]$ echo $PATH
/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/dsjprs/.local/bin:/home/dsjprs/bin
```


#### 4)JDKå‡†å¤‡

1.å¸è½½ç°æœ‰JDKï¼ˆ3å°èŠ‚ç‚¹ï¼‰

```shell
[dsjprs@node01 opt]# sudo rpm -qa | grep -i java | xargs -n1 sudo rpm -e --nodeps

[dsjprs@node02 opt]# sudo rpm -qa | grep -i java | xargs -n1 sudo rpm -e --nodeps

[dsjprs@node03 opt]# sudo rpm -qa | grep -i java | xargs -n1 sudo rpm -e --nodeps
```

ï¼ˆ1ï¼‰rpm -qaï¼šè¡¨ç¤ºæŸ¥è¯¢æ‰€æœ‰å·²ç»å®‰è£…çš„è½¯ä»¶åŒ…

ï¼ˆ2ï¼‰grep -iï¼šè¡¨ç¤ºè¿‡æ»¤æ—¶ä¸åŒºåˆ†å¤§å°å†™

ï¼ˆ3ï¼‰xargs -n1ï¼šè¡¨ç¤ºä¸€æ¬¡è·å–ä¸Šæ¬¡æ‰§è¡Œç»“æœçš„ä¸€ä¸ªå€¼

ï¼ˆ4ï¼‰rpm -e --nodepsï¼šè¡¨ç¤ºå¸è½½è½¯ä»¶



2.è§£å‹JDKåˆ°/opt/moduleç›®å½•ä¸‹

```shell
[dsjprs@node01 software]# tar -zxvf jdk-8u212-linux-x64.tar.gz -C /opt/module/
```

3.é…ç½®JDKç¯å¢ƒå˜é‡

ï¼ˆ1ï¼‰æ–°å»º/etc/profile.d/my_env.shæ–‡ä»¶

```shell
[dsjprs@node01 module]# sudo vim /etc/profile.d/my_env.sh
```

æ·»åŠ å¦‚ä¸‹å†…å®¹ï¼Œç„¶åä¿å­˜ï¼ˆ:wqï¼‰é€€å‡º

```shell
export JAVA_HOME=/usr/share/jdk1.6.0_14
export PATH=$JAVA_HOME/bin:$PATH
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
```

ï¼ˆ2ï¼‰è®©ç¯å¢ƒå˜é‡ç”Ÿæ•ˆ

```shell
[dsjprs@node01 software]$ source /etc/profile.d/my_env.sh
```

4.æµ‹è¯•JDKæ˜¯å¦å®‰è£…æˆåŠŸ

```shell
[dsjprs@node01 module]# java -version
```

5.åˆ†å‘JDK

```shell
[dsjprs@node01 module]$ xsync /opt/module/jdk1.8.0_212/
```

6.åˆ†å‘ç¯å¢ƒå˜é‡é…ç½®æ–‡ä»¶

```shell
[dsjprs@node01 module]$ sudo /home/dsjprs/bin/xsync /etc/profile.d/my_env.sh
```

7.åˆ†åˆ«åœ¨node02, node03ä¸Šæ‰§è¡Œsource

```shell
[dsjprs@node02 module]$ source /etc/profile.d/my_env.sh
[dsjprs@node03 module]$ source /etc/profile.d/my_env.sh
```



### å®‰è£…Hadoop

#### 1).ä¸Šä¼ ï¼Œè§£å‹å®‰è£…åŒ…(hadoop102)

```shell
hadoop-3.1.4-bin-snappy-CentOS7.tar.gz 
tar -zxvf hadoop-3.1.4-bin-snappy-CentOS7.tar.gz -C /opt/module
```

#### 2).ç¼–è¾‘Hadoopé…ç½®æ–‡ä»¶

##### 1.hadoop.sh

```shell
cd /opt/module/hadoop-3.1.4/etc/hadoop/ 
vim hadoop-env.sh
#é…ç½®JAVA_HOME export JAVA_HOME=/export/server/jdk1.8.0_65 
#è®¾ç½®ç”¨æˆ·ä»¥æ‰§è¡Œå¯¹åº”è§’è‰²shellå‘½ä»¤ 
export HDFS_NAMENODE_USER=dsjprs 
export HDFS_DATANODE_USER=dsjprs
export HDFS_SECONDARYNAMENODE_USER=dsjprs 
export YARN_RESOURCEMANAGER_USER=dsjprs
export YARN_NODEMANAGER_USER=dsjprs
# Hadoopè°ƒä¼˜
export HDFS_NAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS - Xmx1024m"
export HDFS_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS -Xmx1024m"
```

##### 2.core-site.xml

```shell
vim core-site.xml
```

```xml
<configuration> 
    <!-- é»˜è®¤æ–‡ä»¶ç³»ç»Ÿçš„åç§°ã€‚é€šè¿‡URIä¸­schemaåŒºåˆ†ä¸åŒæ–‡ä»¶ç³»ç»Ÿã€‚ --> 
    <!-- file:///æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ hdfs:// hadoopåˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ gfs://ã€‚--> 
    <!-- hdfsæ–‡ä»¶ç³»ç»Ÿè®¿é—®åœ°å€ï¼šhttp://nn_host:8020ã€‚--> 
<property> 
	<name>fs.defaultFS</name> 		                    	
    <value>hdfs://node01:8020</value> 
</property> 
<!-- hadoopæœ¬åœ°æ•°æ®å­˜å‚¨ç›®å½• formatæ—¶è‡ªåŠ¨ç”Ÿæˆ --> 
<property> 
    <name>hadoop.tmp.dir</name> 
    <value>/export/data/hadoop-3.1.4</value> 
</property> 
<!-- åœ¨Web UIè®¿é—®HDFSä½¿ç”¨çš„ç”¨æˆ·åã€‚--> 
<property> 
    <name>hadoop.http.staticuser.user</name> 
    <value>dsjprs</value> 
</property>
<!-- é…ç½®åƒåœ¾å›æ”¶æ—¶é—´ä¸º 1 åˆ†é’Ÿã€‚ -->
<property>
 	<name>fs.trash.interval</name>
 	<value>1</value>
</property>
<property>  
  	<name>io.file.buffer.size</name>  
 	<value>131072</value>  
</property>
<property>  
  	<name>hadoop.proxyuser.spark.hosts</name>  
  	<value>*</value>  
</property>  
<property>  
  	<name>hadoop.proxyuser.spark.groups</name>  
  	<value>*</value>  
</property>
</configuration>
```

##### 3.hdfs-site.xml

```shell
vim hdfs-site.xml
```

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
<!-- nn web ç«¯è®¿é—®åœ°å€-->
<property>
	<name>dfs.namenode.http-address</name>
	<value>node01:9870</value>
</property>
<!-- 2nn web ç«¯è®¿é—®åœ°å€-->
<property>
 	<name>dfs.namenode.secondary.http-address</name>
 	<value>node03:9868</value>
</property>
<property>
 	<name>dfs.namenode.name.dir</name>
	<value>file://${hadoop.tmp.dir}/dfs/name1,file://${hadoop.tmp.dir}/dfs/name2</value>
</property>
<property>
 	<name>dfs.datanode.data.dir</name>
	<value>file://${hadoop.tmp.dir}/dfs/data1,file://${hadoop.tmp.dir}/dfs/data2</value>
</property>
<!--SecondaryNameNode æ¯éš”ä¸€å°æ—¶æ‰§è¡Œä¸€æ¬¡-->
<property>
 	<name>dfs.namenode.checkpoint.period</name>
 	<value>3600s</value>
</property>
<!-- ä¸€åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡æ“ä½œæ¬¡æ•°ï¼Œå½“æ“ä½œæ¬¡æ•°è¾¾åˆ° 1 ç™¾ä¸‡æ—¶ï¼ŒSecondaryNameNode æ‰§è¡Œä¸€æ¬¡-->
<property>
 	<name>dfs.namenode.checkpoint.txns</name>
 	<value>1000000</value>
	<description>æ“ä½œåŠ¨ä½œæ¬¡æ•°</description>
</property>
<property>
 	<name>dfs.namenode.checkpoint.check.period</name>
 	<value>60s</value>
	<description> 1 åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡æ“ä½œæ¬¡æ•°</description>
</property>
<!-- DN å‘ NN æ±‡æŠ¥å½“å‰è§£è¯»ä¿¡æ¯çš„æ—¶é—´é—´éš”ï¼Œé»˜è®¤ 6 å°æ—¶ -->
<property>
	<name>dfs.blockreport.intervalMsec</name>
	<value>21600000</value>
	<description>Determines block reporting interval in milliseconds.</description>
</property>
<!-- DN æ‰«æè‡ªå·±èŠ‚ç‚¹å—ä¿¡æ¯åˆ—è¡¨çš„æ—¶é—´ï¼Œé»˜è®¤ 6 å°æ—¶ -->
<property>
	<name>dfs.datanode.directoryscan.interval</name>
	<value>21600s</value>   
</property>
<!--  heartbeat.recheck.interval çš„å•ä½ä¸ºæ¯«ç§’ï¼Œ dfs.heartbeat.interval çš„å•ä½ä¸ºç§’ã€‚ -->
<property>
 	<name>dfs.namenode.heartbeat.recheck-interval</name>
 	<value>300000</value>
</property>
<property>
 	<name>dfs.heartbeat.interval</name>
 	<value>3</value>
</property>
<!-- NameNode æœ‰ä¸€ä¸ªå·¥ä½œçº¿ç¨‹æ± ï¼Œç”¨æ¥å¤„ç†ä¸åŒ DataNode çš„å¹¶å‘å¿ƒè·³ä»¥åŠå®¢æˆ·ç«¯å¹¶å‘çš„å…ƒæ•°æ®æ“ä½œã€‚ -->
<property>
 	<name>dfs.namenode.handler.count</name>
 	<value>21</value>
</property>
<!-- ç™½åå• -->
<property>
    <name>dfs.hosts</name>
 	<value>/opt/module/hadoop-3.1.3/etc/hadoop/whitelist</value>
</property>
<!-- é»‘åå• -->
<property>
 	<name>dfs.hosts.exclude</name>
 	<value>/opt/module/hadoop-3.1.3/etc/hadoop/blacklist</value>
</property
<property>  
 	<name>dfs.webhdfs.enabled</name>  
  	<value>true</value>  
</property>  
</configuration>
```

##### 4.mapred-site.xml

```shell
vim mapred-site.xml
```

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
<!-- æŒ‡å®š MapReduce ç¨‹åºè¿è¡Œåœ¨ Yarn ä¸Š -->
<property>
 	<name>mapreduce.framework.name</name>
 	<value>yarn</value>
</property>
<!-- MR App Masterç¯å¢ƒå˜é‡ã€‚--> 
<property> 
    <name>yarn.app.mapreduce.am.env</name> 
    <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value> 
</property> 
<!-- MR MapTaskç¯å¢ƒå˜é‡ã€‚--> 
<property> 
    <name>mapreduce.map.env</name> 
    <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value> 
</property> 
<!-- MR ReduceTaskç¯å¢ƒå˜é‡ã€‚--> 
<property> 
    <name>mapreduce.reduce.env</name> 
    <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value> 
</property>
<!-- å†å²æœåŠ¡å™¨ç«¯åœ°å€ -->
<property>
 	<name>mapreduce.jobhistory.address</name>
 	<value>node01:10020</value>
</property>
<!-- å†å²æœåŠ¡å™¨ web ç«¯åœ°å€ -->
<property>
 	<name>mapreduce.jobhistory.webapp.address</name>
 	<value>node01:19888</value>
</property>
<!-- ç¯å½¢ç¼“å†²åŒºå¤§å°ï¼Œé»˜è®¤ 100m -->
<property>
 	<name>mapreduce.task.io.sort.mb</name>
 	<value>100</value>
</property>
<!-- ç¯å½¢ç¼“å†²åŒºæº¢å†™é˜ˆå€¼ï¼Œé»˜è®¤ 0.8 -->
<property>
 	<name>mapreduce.map.sort.spill.percent</name>
 	<value>0.80</value>
</property>
<!-- merge åˆå¹¶æ¬¡æ•°ï¼Œé»˜è®¤ 10 ä¸ª -->
<property>
 	<name>mapreduce.task.io.sort.factor</name>
 	<value>10</value>
</property>
<!-- maptask å†…å­˜ï¼Œé»˜è®¤ 1gï¼› maptask å †å†…å­˜å¤§å°é»˜è®¤å’Œè¯¥å€¼å¤§å°ä¸€è‡´
mapreduce.map.java.opts -->
<property>
 	<name>mapreduce.map.memory.mb</name>
 	<value>-1</value>
 	<description>The amount of memory to request from the scheduler for each map task. If 	  this is not specified or is non-positive, it is inferred from mapreduce.map.java.opts 	and mapreduce.job.heap.memory-mb.ratio. If java-opts are also not specified, we set 	it to 1024.
 	</description>
</property>
<!-- matask çš„ CPU æ ¸æ•°ï¼Œé»˜è®¤ 1 ä¸ª -->
<property>
 	<name>mapreduce.map.cpu.vcores</name>
 	<value>1</value>
</property>
<!-- matask å¼‚å¸¸é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤ 4 æ¬¡ -->
<property>
 	<name>mapreduce.map.maxattempts</name>
 	<value>4</value>
</property>
<!-- æ¯ä¸ª Reduce å» Map ä¸­æ‹‰å–æ•°æ®çš„å¹¶è¡Œæ•°ã€‚é»˜è®¤å€¼æ˜¯ 5 -->
<property>
 	<name>mapreduce.reduce.shuffle.parallelcopies</name>
	<value>5</value>
</property>
<!-- Buffer å¤§å°å  Reduce å¯ç”¨å†…å­˜çš„æ¯”ä¾‹ï¼Œé»˜è®¤å€¼ 0.7 -->
<property>
 	<name>mapreduce.reduce.shuffle.input.buffer.percent</name>
 	<value>0.70</value>
</property>
<!-- Buffer ä¸­çš„æ•°æ®è¾¾åˆ°å¤šå°‘æ¯”ä¾‹å¼€å§‹å†™å…¥ç£ç›˜ï¼Œé»˜è®¤å€¼ 0.66ã€‚ -->
<property>
 	<name>mapreduce.reduce.shuffle.merge.percent</name>
 	<value>0.66</value>
</property>
<!-- reducetask å†…å­˜ï¼Œé»˜è®¤ 1gï¼›reducetask å †å†…å­˜å¤§å°é»˜è®¤å’Œè¯¥å€¼å¤§å°ä¸€è‡´
mapreduce.reduce.java.opts -->
<property>
 	<name>mapreduce.reduce.memory.mb</name>
 	<value>-1</value>
 	<description>The amount of memory to request from the scheduler for each reduce task. 	  If this is not specified or is non-positive, it is inferred from
    mapreduce.reduce.java.opts and mapreduce.job.heap.memory-mb.ratio. If java-opts 		are also not specified, we set it to 1024.
 	</description>
</property>
<!-- reducetask çš„ CPU æ ¸æ•°ï¼Œé»˜è®¤ 1 ä¸ª -->
<property>
 	<name>mapreduce.reduce.cpu.vcores</name>
 	<value>2</value>
</property>
<!-- reducetask å¤±è´¥é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤ 4 æ¬¡ -->
<property>
 	<name>mapreduce.reduce.maxattempts</name>
 	<value>4</value>
</property>
<!-- å½“ MapTask å®Œæˆçš„æ¯”ä¾‹è¾¾åˆ°è¯¥å€¼åæ‰ä¼šä¸º ReduceTask ç”³è¯·èµ„æºã€‚é»˜è®¤æ˜¯ 0.05
-->
<property>
 	<name>mapreduce.job.reduce.slowstart.completedmaps</name>
 	<value>0.05</value>
</property>
<!-- å¦‚æœç¨‹åºåœ¨è§„å®šçš„é»˜è®¤ 10 åˆ†é’Ÿå†…æ²¡æœ‰è¯»åˆ°æ•°æ®ï¼Œå°†å¼ºåˆ¶è¶…æ—¶é€€å‡º -->
<property>
 	<name>mapreduce.task.timeout</name>
 	<value>600000</value>
</property>
</configuration>
```

##### 5.yarn-site.xml

```shell
vim yarn-site.xml
```

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
 <!-- æŒ‡å®š MR èµ° shuffle -->
<property>
 	<name>yarn.nodemanager.aux-services</name>
	<value>mapreduce_shuffle</value>
</property>
<!-- æŒ‡å®š ResourceManager çš„åœ°å€-->
<property>
 	<name>yarn.resourcemanager.address</name>
	<value>node03:8032</value>
</property>
<property>
     <name>yarn.resourcemanager.scheduler.address</name>
     <value>node03:8030</value>
</property>
<property>
     <name>yarn.resourcemanager.resource-tracker.address</name>
     <value>node03:8031</value>
</property>
<property>
      <name>yarn.resourcemanager.admin.address</name>
      <value>mode03:8033</value>
</property>
<property>
       <name>yarn.resourcemanager.webapp.address</name>
       <value>node03:8088</value>
 </property>
<!-- ç¯å¢ƒå˜é‡çš„ç»§æ‰¿ -->
<property>
 	<name>yarn.nodemanager.env-whitelist</name>
	<value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CO
	NF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
</property>
<!-- å¼€å¯æ—¥å¿—èšé›†åŠŸèƒ½ -->
<property>
 	<name>yarn.log-aggregation-enable</name>
	<value>true</value>
</property>
<!-- è®¾ç½®æ—¥å¿—èšé›†æœåŠ¡å™¨åœ°å€ -->
<property>
 	<name>yarn.log.server.url</name>
 	<value>http://node01:19888/jobhistory/logs</value>
</property>
<!-- è®¾ç½®æ—¥å¿—ä¿ç•™æ—¶é—´ä¸º 7 å¤© -->
<property>
 	<name>yarn.log-aggregation.retain-seconds</name>
 	<value>604800</value>
</property>
    <!-- æ¯ä¸ªå®¹å™¨è¯·æ±‚çš„æœ€å°å†…å­˜èµ„æºï¼ˆä»¥MBä¸ºå•ä½ï¼‰ã€‚--> 
<property> 
    <name>yarn.scheduler.minimum-allocation-mb</name> 
    <value>512</value> 
</property> 
<!-- æ¯ä¸ªå®¹å™¨è¯·æ±‚çš„æœ€å¤§å†…å­˜èµ„æºï¼ˆä»¥MBä¸ºå•ä½ï¼‰ã€‚--> 
<property> 
    <name>yarn.scheduler.maximum-allocation-mb</name> 
    <value>2048</value> 
</property> 
<!-- å®¹å™¨è™šæ‹Ÿå†…å­˜ä¸ç‰©ç†å†…å­˜ä¹‹é—´çš„æ¯”ç‡ã€‚--> 
<property> 
    <name>yarn.nodemanager.vmem-pmem-ratio</name> 
    <value>4</value> 
</property>
<!-- é€‰æ‹©è°ƒåº¦å™¨ï¼Œé»˜è®¤å®¹é‡ -->
<property>
<description>The class to use as the resource scheduler.</description>
	<name>yarn.resourcemanager.scheduler.class</name>
	<value>org.apache.hadoop.yarn.server.resourcemanager.
    scheduler.capacity.CapacityScheduler</value>
</property>
<!-- ResourceManager å¤„ç†è°ƒåº¦å™¨è¯·æ±‚çš„çº¿ç¨‹æ•°é‡,é»˜è®¤ 50ï¼›å¦‚æœæäº¤çš„ä»»åŠ¡æ•°å¤§äº 50ï¼Œå¯ä»¥å¢åŠ è¯¥å€¼ï¼Œä½†æ˜¯ä¸èƒ½è¶…è¿‡ 3 å° * 4 çº¿ç¨‹ = 12 çº¿ç¨‹ï¼ˆå»é™¤å…¶ä»–åº”ç”¨ç¨‹åºå®é™…ä¸èƒ½è¶…è¿‡ 8ï¼‰ -->
<property>
	<description>Number of threads to handle schedulerinterface.</description>
	<name>yarn.resourcemanager.scheduler.client.thread-count</name>
	<value>8</value>
</property>
<!-- æ˜¯å¦è®© yarn è‡ªåŠ¨æ£€æµ‹ç¡¬ä»¶è¿›è¡Œé…ç½®ï¼Œé»˜è®¤æ˜¯ falseï¼Œå¦‚æœè¯¥èŠ‚ç‚¹æœ‰å¾ˆå¤šå…¶ä»–åº”ç”¨ç¨‹åºï¼Œå»ºè®®æ‰‹åŠ¨é…ç½®ã€‚å¦‚æœè¯¥èŠ‚ç‚¹æ²¡æœ‰å…¶ä»–åº”ç”¨ç¨‹åºï¼Œå¯ä»¥é‡‡ç”¨è‡ªåŠ¨ -->
<property>
	<description>Enable auto-detection of node capabilities such as memory and CPU.
	</description>
	<name>yarn.nodemanager.resource.detect-hardware-capabilities</name>
	<value>false</value>
</property>
<!-- æ˜¯å¦å°†è™šæ‹Ÿæ ¸æ•°å½“ä½œ CPU æ ¸æ•°ï¼Œé»˜è®¤æ˜¯ falseï¼Œé‡‡ç”¨ç‰©ç† CPU æ ¸æ•° -->
<property>
	<description>Flag to determine if logical processors(such as hyperthreads) should be 	 counted as cores. Only applicable on Linux when yarn.nodemanager.resource.cpu-vcores 	  is set to -1 and yarn.nodemanager.resource.detect-hardware-capabilities is true.
	</description>
	<name>yarn.nodemanager.resource.count-logical-processors-ascores</name>
	<value>false</value>
</property>
<!-- è™šæ‹Ÿæ ¸æ•°å’Œç‰©ç†æ ¸æ•°ä¹˜æ•°ï¼Œé»˜è®¤æ˜¯ 1.0 -->
<property>
	<description>Multiplier to determine how to convert phyiscal cores to vcores. This value is used if yarn.nodemanager.resource.cpu-vcores is set to -1(which implies 		auto-calculate vcores) and yarn.nodemanager.resource.detect-hardware-capabilities is 	 set to true.The number of vcores will be calculated as number of CPUs * multiplier.
	</description>
	<name>yarn.nodemanager.resource.pcores-vcores-multiplier</name>
	<value>1.0</value>
</property>
<!-- NodeManager ä½¿ç”¨å†…å­˜æ•°ï¼Œé»˜è®¤ 8Gï¼Œä¿®æ”¹ä¸º 4G å†…å­˜ -->
<property>
	<description>Amount of physical memory, in MB, that can be allocated for containers. 	 If set to -1 and yarn.nodemanager.resource.detect-hardware-capabilities is true, it 	 is automatically calculated(in case of Windows and Linux).In other cases, the default 	   is 8192MB.</description>
	<name>yarn.nodemanager.resource.memory-mb</name>
	<value>4096</value>
</property>
<!-- nodemanager çš„ CPU æ ¸æ•°ï¼Œä¸æŒ‰ç…§ç¡¬ä»¶ç¯å¢ƒè‡ªåŠ¨è®¾å®šæ—¶é»˜è®¤æ˜¯ 8 ä¸ªï¼Œä¿®æ”¹ä¸º 4 ä¸ª -->
<property>
	<description>Number of vcores that can be allocated for containers. This is used by 	the RM scheduler when allocating resources for containers. This is not used to limit 	 the number of CPUs used by YARN containers. If it is set to -1 and
	yarn.nodemanager.resource.detect-hardware-capabilities is true, it is automatically 	determined from the hardware in case of Windows and Linux.In other cases, number of 	vcores is 8 by default.</description>
	<name>yarn.nodemanager.resource.cpu-vcores</name>
	<value>4</value>
</property>
<!-- å®¹å™¨æœ€å°å†…å­˜ï¼Œé»˜è®¤ 1G -->
<property>
	<description>The minimum allocation for every container request at theRM in MBs. 		Memory 	requests lower than this will be set to the value of this property. 		 
    Additionally, a node manager that is configured to have less memory than this value 	will be shut down by the resource manager.</description>
	<name>yarn.scheduler.minimum-allocation-mb</name>
	<value>1024</value>
</property>
<!-- å®¹å™¨æœ€å¤§å†…å­˜ï¼Œé»˜è®¤ 8Gï¼Œä¿®æ”¹ä¸º 2G -->
<property>
	<description>The maximum allocation for every container request at the RM in MBs. 		Memory requests higher than this will throw an InvalidResourceRequestException.
	</description>
	<name>yarn.scheduler.maximum-allocation-mb</name>
	<value>2048</value>
</property>
<!-- å®¹å™¨æœ€å° CPU æ ¸æ•°ï¼Œé»˜è®¤ 1 ä¸ª -->
<property>
	<description>The minimum allocation for every container request at the RM in terms of 	  virtual CPU cores. Requests lower than this will be set to the value of this 		
    property. Additionally, a node manager that is configured to have fewer virtual cores 	  than this value will be shut down by the resource manager.
	</description>
	<name>yarn.scheduler.minimum-allocation-vcores</name>
	<value>1</value>
</property>
<!-- å®¹å™¨æœ€å¤§ CPU æ ¸æ•°ï¼Œé»˜è®¤ 4 ä¸ªï¼Œä¿®æ”¹ä¸º 2 ä¸ª -->
<property>
	<description>The maximum allocation for every container request at the RM in terms of 	  virtual CPU cores. Requests higher than this will throw an
    InvalidResourceRequestException.</description>
	<name>yarn.scheduler.maximum-allocation-vcores</name>
	<value>2</value>
</property>
<!-- è™šæ‹Ÿå†…å­˜æ£€æŸ¥ï¼Œé»˜è®¤æ‰“å¼€ï¼Œä¿®æ”¹ä¸ºå…³é—­ -->
<property>
	<description>Whether virtual memory limits will be enforced for containers.			
    </description>
	<name>yarn.nodemanager.vmem-check-enabled</name>
	<value>false</value>
</property>
<!-- è™šæ‹Ÿå†…å­˜å’Œç‰©ç†å†…å­˜è®¾ç½®æ¯”ä¾‹,é»˜è®¤ 2.1 -->
<property>
	<description>Ratio between virtual memory to physical memory whensetting memory 		limits for containers. Container allocations are expressed in terms of physical 		memory, and virtual memory usage is allowed to exceed this allocation by this ratio.
	</description>
	<name>yarn.nodemanager.vmem-pmem-ratio</name>
	<value>2.1</value>
</property>
<!-- æŒ‡å®šå¤šé˜Ÿåˆ—ï¼Œå¢åŠ  hive é˜Ÿåˆ— -->
<property>
 	<name>yarn.scheduler.capacity.root.queues</name>
 	<value>default,hive</value>
 	<description>
 	The queues at the this level (root is the root queue).
	</description>
</property>
<!-- é™ä½ default é˜Ÿåˆ—èµ„æºé¢å®šå®¹é‡ä¸º 40%ï¼Œé»˜è®¤ 100% -->
<property>
 	<name>yarn.scheduler.capacity.root.default.capacity</name>
 	<value>40</value>
</property>
<!-- é™ä½ default é˜Ÿåˆ—èµ„æºæœ€å¤§å®¹é‡ä¸º 60%ï¼Œé»˜è®¤ 100% -->
<property>
 	<name>yarn.scheduler.capacity.root.default.maximum-capacity</name>
 	<value>60</value>
</property>
<!-- æŒ‡å®š hive é˜Ÿåˆ—çš„èµ„æºé¢å®šå®¹é‡ -->
<property>
 	<name>yarn.scheduler.capacity.root.hive.capacity</name>
 	<value>60</value>
</property>
<!-- ç”¨æˆ·æœ€å¤šå¯ä»¥ä½¿ç”¨é˜Ÿåˆ—å¤šå°‘èµ„æºï¼Œ1 è¡¨ç¤º -->
<property>
 	<name>yarn.scheduler.capacity.root.hive.user-limit-factor</name>
 	<value>1</value>
</property>
<!-- æŒ‡å®š hive é˜Ÿåˆ—çš„èµ„æºæœ€å¤§å®¹é‡ -->
<property>
 	<name>yarn.scheduler.capacity.root.hive.maximum-capacity</name>
 	<value>80</value>
</property>
<!-- å¯åŠ¨ hive é˜Ÿåˆ— -->
<property>
 	<name>yarn.scheduler.capacity.root.hive.state</name>
 	<value>RUNNING</value>
</property>
<!-- å“ªäº›ç”¨æˆ·æœ‰æƒå‘é˜Ÿåˆ—æäº¤ä½œä¸š -->
<property>
 	<name>yarn.scheduler.capacity.root.hive.acl_submit_applications</name>
 	<value>*</value>
</property>
<!-- å“ªäº›ç”¨æˆ·æœ‰æƒæ“ä½œé˜Ÿåˆ—ï¼Œç®¡ç†å‘˜æƒé™ï¼ˆæŸ¥çœ‹/æ€æ­»ï¼‰ -->
<property>
 	<name>yarn.scheduler.capacity.root.hive.acl_administer_queue</name>
 	<value>*</value>
</property>
<!-- å“ªäº›ç”¨æˆ·æœ‰æƒé…ç½®æäº¤ä»»åŠ¡ä¼˜å…ˆçº§ -->
<property>
	<name>yarn.scheduler.capacity.root.hive.acl_application_max_priority</name>
 	<value>*</value>
</property>
<!-- ä»»åŠ¡çš„è¶…æ—¶æ—¶é—´è®¾ç½®ï¼šyarn application -appId appId -updateLifetime Timeout å‚è€ƒèµ„æ–™ï¼š https://blog.cloudera.com/enforcing-application-lifetime-slasyarn/ -->
<!-- å¦‚æœ application æŒ‡å®šäº†è¶…æ—¶æ—¶é—´ï¼Œåˆ™æäº¤åˆ°è¯¥é˜Ÿåˆ—çš„ application èƒ½å¤ŸæŒ‡å®šçš„æœ€å¤§è¶…æ—¶æ—¶é—´ä¸èƒ½è¶…è¿‡è¯¥å€¼ã€‚-->
<property>
 	<name>yarn.scheduler.capacity.root.hive.maximum-applicationlifetime</name>
 	<value>-1</value>
</property>
<!-- å¦‚æœ application æ²¡æŒ‡å®šè¶…æ—¶æ—¶é—´ï¼Œåˆ™ç”¨ default-application-lifetime ä½œä¸ºé»˜è®¤
å€¼ -->
<property>
 	<name>yarn.scheduler.capacity.root.hive.default-applicationlifetime</name>
 	<value>-1</value>
</property>
</configuration>
```

##### 6.workers

```shell
vim workers
```

```shell
node01
node02
node03
```

##### 7.xsyncè„šæœ¬Hadoopåˆ†å‘

```shell
xsync hadoop-3.1.4
```



##### 8.é…ç½®hadoopç¯å¢ƒå˜é‡/etc/profile.d/my_env.sh

```shell
export HADOOP_HOME=/export/server/hadoop-3.1.4 export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
```

##### 9.åˆ†å‘hadoopç¯å¢ƒå˜é‡é…ç½®

```shell
xsync /etc/profile.d/my_env.sh
```

##### 10.source

```shell
source /etc/profile.d/my_env.sh
```

##### 11.æ ¼å¼åŒ–æ“ä½œ

é¦–æ¬¡å¯åŠ¨HDFSæ—¶ï¼Œå¿…é¡»å¯¹å…¶è¿›è¡Œæ ¼å¼åŒ–æ“ä½œã€‚ 

formatæœ¬è´¨ä¸Šæ˜¯åˆå§‹åŒ–å·¥ä½œï¼Œè¿›è¡ŒHDFSæ¸…ç†å’Œå‡†å¤‡å·¥ä½œ

```shell
hdfs namenode -format
```

##### 12.é›†ç¾¤å¯åŠ¨å‘½ä»¤

**Hadoopé›†ç¾¤å¯åŠ¨å…³é—­-æ‰‹åŠ¨é€ä¸ªè¿›ç¨‹å¯åœ**

HDFSé›†ç¾¤ï¼š

```shell
hdfs --daemon start namenode|datanode|secondarynamenode 
hdfs --daemon stop namenode|datanode|secondarynamenode
```

YARNé›†ç¾¤

```shell
yarn --daemon start resourcemanager|nodemanager 
yarn --daemon stop resourcemanager|nodemanager
```

**Hadoopé›†ç¾¤å¯åŠ¨å…³é—­-shellè„šæœ¬ä¸€é”®å¯åœ**

åœ¨node1ä¸Šï¼Œä½¿ç”¨è½¯ä»¶è‡ªå¸¦çš„shellè„šæœ¬ä¸€é”®å¯åŠ¨ 

å‰æï¼šé…ç½®å¥½æœºå™¨ä¹‹é—´çš„SSHå…å¯†ç™»å½•å’Œworkersæ–‡ä»¶ã€‚

HDFSé›†ç¾¤:

```shell
start-dfs.sh 
stop-dfs.sh
```

YARNé›†ç¾¤:

```shell
start-yarn.sh 
stop-yarn.sh 
```

Hadoopé›†ç¾¤:

```shell
start-all.sh 
stop-all.sh
```

##### 13.è¿›ç¨‹æŸ¥çœ‹è„šæœ¬

ï¼ˆ1ï¼‰åœ¨/home/dsjprs/binç›®å½•ä¸‹åˆ›å»ºè„šæœ¬jspcall.sh

```shell
[dsjprs@node01 bin]$ vim jspcall.sh
```

ï¼ˆ2ï¼‰åœ¨è„šæœ¬ä¸­ç¼–å†™å¦‚ä¸‹å†…å®¹

```shell
#! /bin/bash
for i in node01 node02 ndoe03
do
    echo --------- $i ----------
    ssh $i "$*"
done
```

ï¼ˆ3ï¼‰ä¿®æ”¹è„šæœ¬æ‰§è¡Œæƒé™

```shell
[dsjprs@node01 bin]$ chmod +x jspcall.sh
```

ï¼ˆ4ï¼‰å¯åŠ¨è„šæœ¬

```shell
[djsprs@node01 bin]$ jspcall.sh jps
```

##### 14.Hadoopé›†ç¾¤å¸¸ç”¨å¯åŠ¨åœæ­¢è„šæœ¬

1ï¼‰Hadoop é›†ç¾¤å¯åœè„šæœ¬ï¼ˆåŒ…å« HDFSï¼ŒYarnï¼ŒHistoryserverï¼‰ï¼šmyhadoop.sh 

```shell
cd /home/dsjprs/bin 
vim myhadoop.sh 
```



```shell
#!/bin/bash
if [ $# -lt 1 ]
then
 echo "No Args Input..."
 exit ;
fi
case $1 in
"start")
 echo " =================== å¯åŠ¨ hadoop é›†ç¾¤ ==================="
 echo " --------------- å¯åŠ¨ hdfs ---------------"
 ssh  node01 "/opt/module/hadoop-3.1.3/sbin/start-dfs.sh"
 echo " --------------- å¯åŠ¨ yarn ---------------"
 ssh  node02 "/opt/module/hadoop-3.1.3/sbin/start-yarn.sh"
 echo " --------------- å¯åŠ¨ historyserver ---------------"
 ssh  node01 "/opt/module/hadoop-3.1.3/bin/mapred --daemon start historyserver"
;;
"stop")
 echo " =================== å…³é—­ hadoop é›†ç¾¤ ==================="
 echo " --------------- å…³é—­ historyserver ---------------"
 ssh  node01 "/opt/module/hadoop-3.1.3/bin/mapred --daemon stop historyserver"
 echo " --------------- å…³é—­ yarn ---------------"
 ssh  node02 "/opt/module/hadoop-3.1.3/sbin/stop-yarn.sh"
 echo " --------------- å…³é—­ hdfs ---------------"
 ssh  node01 "/opt/module/hadoop-3.1.3/sbin/stop-dfs.sh"
;;
*)
 echo "Input Args Error..."
;;
esac
```

2ï¼‰èµ‹äºˆè„šæœ¬æ‰§è¡Œæƒé™ï¼š

```shell
chmod +x myhadoop.sh
```

##### **è¡¥å……**

é›†ç¾¤æ—¶é—´åŒæ­¥

å¦‚æœæœåŠ¡å™¨åœ¨å…¬ç½‘ç¯å¢ƒï¼ˆèƒ½è¿æ¥å¤–ç½‘ï¼‰ï¼Œå¯ä»¥ä¸é‡‡ç”¨é›†ç¾¤æ—¶é—´åŒæ­¥ï¼Œå› ä¸ºæœåŠ¡å™¨ä¼šå®šæœŸå’Œå…¬ç½‘æ—¶é—´è¿›è¡Œæ ¡å‡†ï¼›
å¦‚æœæœåŠ¡å™¨åœ¨å†…ç½‘ç¯å¢ƒï¼Œå¿…é¡»è¦é…ç½®é›†ç¾¤æ—¶é—´åŒæ­¥ï¼Œå¦åˆ™æ—¶é—´ä¹…äº†ï¼Œä¼šäº§ç”Ÿæ—¶é—´åå·®ï¼Œå¯¼è‡´é›†ç¾¤æ‰§è¡Œä»»åŠ¡æ—¶é—´ä¸åŒæ­¥ã€‚

1ï¼‰éœ€æ±‚ æ‰¾ä¸€ä¸ªæœºå™¨ï¼Œä½œä¸ºæ—¶é—´æœåŠ¡å™¨ï¼Œæ‰€æœ‰çš„æœºå™¨ä¸è¿™å°é›†ç¾¤æ—¶é—´è¿›è¡Œå®šæ—¶çš„åŒæ­¥ï¼Œç”Ÿäº§ç¯å¢ƒ æ ¹æ®ä»»åŠ¡å¯¹æ—¶é—´çš„å‡†ç¡®ç¨‹åº¦è¦æ±‚å‘¨æœŸåŒæ­¥ã€‚æµ‹è¯•ç¯å¢ƒä¸ºäº†å°½å¿«çœ‹åˆ°æ•ˆæœï¼Œé‡‡ç”¨ 1 åˆ†é’ŸåŒæ­¥ä¸€ æ¬¡ã€‚

  (1ï¼‰æŸ¥çœ‹æ‰€æœ‰èŠ‚ç‚¹ ntpd æœåŠ¡çŠ¶æ€å’Œå¼€æœºè‡ªå¯åŠ¨çŠ¶æ€

```shell
sudo systemctl status ntpd
sudo systemctl start ntpd
sudo systemctl is-enabled ntpd
```

ï¼ˆ2ï¼‰ä¿®æ”¹ hadoop102 çš„ ntp.conf é…ç½®æ–‡ä»¶

```shell
sudo vim /etc/ntp.conf
```

ä¿®æ”¹å†…å®¹å¦‚ä¸‹
ï¼ˆaï¼‰ä¿®æ”¹ 1ï¼ˆæˆæƒ 192.168.10.0-192.168.10.255 ç½‘æ®µä¸Šçš„æ‰€æœ‰æœºå™¨å¯ä»¥ä»è¿™å°æœºå™¨ä¸ŠæŸ¥ è¯¢å’ŒåŒæ­¥æ—¶é—´ï¼‰

```shell
#restrict 192.168.10.0 mask 255.255.255.0 nomodify notrap

ä¸º

restrict 192.168.10.0 mask 255.255.255.0 nomodify notrap
```

ï¼ˆbï¼‰ä¿®æ”¹ 2ï¼ˆé›†ç¾¤åœ¨å±€åŸŸç½‘ä¸­ï¼Œä¸ä½¿ç”¨å…¶ä»–äº’è”ç½‘ä¸Šçš„æ—¶é—´ï¼‰

```shell
server 0.centos.pool.ntp.org iburst
server 1.centos.pool.ntp.org iburst
server 2.centos.pool.ntp.org iburst
server 3.centos.pool.ntp.org iburst

ä¸º
#server 0.centos.pool.ntp.org iburst
#server 1.centos.pool.ntp.org iburst
#server 2.centos.pool.ntp.org iburst
#server 3.centos.pool.ntp.org iburst 
```

ï¼ˆcï¼‰æ·»åŠ  3ï¼ˆå½“è¯¥èŠ‚ç‚¹ä¸¢å¤±ç½‘ç»œè¿æ¥ï¼Œä¾ç„¶å¯ä»¥é‡‡ç”¨æœ¬åœ°æ—¶é—´ä½œä¸ºæ—¶é—´æœåŠ¡å™¨ä¸ºé›†ç¾¤ä¸­ çš„å…¶ä»–èŠ‚ç‚¹æä¾›æ—¶é—´åŒæ­¥ï¼‰

```shell
server 127.127.1.0 fudge 127.127.1.0 stratum 10
```

ï¼ˆ3ï¼‰ä¿®æ”¹ hadoop102 çš„/etc/sysconfig/ntpd æ–‡ä»¶

```shell
sudo vim /etc/sysconfig/ntpd
```

å¢åŠ å†…å®¹å¦‚ä¸‹ï¼ˆè®©ç¡¬ä»¶æ—¶é—´ä¸ç³»ç»Ÿæ—¶é—´ä¸€èµ·åŒæ­¥ï¼‰ 

```shell
SYNC_HWCLOCK=yes
```

ï¼ˆ4ï¼‰é‡æ–°å¯åŠ¨ ntpd æœåŠ¡

```shell
sudo systemctl start ntpd
sudo systemctl enable ntpd 
```

ï¼ˆ5ï¼‰è®¾ç½® ntpd æœåŠ¡å¼€æœºå¯åŠ¨

```shell
sudo systemctl enable ntpd
```

##### 15.Hadoopåˆä½“éªŒ

æ‰§è¡ŒHadoopå®˜æ–¹è‡ªå¸¦çš„MapReduceæ¡ˆä¾‹ 

è¯„ä¼°åœ†å‘¨ç‡Ï€çš„å€¼ 

```shell
cd /export/server/hadoop-3.1.4/share/hadoop/mapreduce/ 

hadoop jar hadoop-mapreduce-examples-3.1.4.jar pi 2 4
```

##### 16.Hadoop HDFSåŸºå‡†æµ‹è¯•



**æµ‹è¯•å†™å…¥é€Ÿåº¦**

ç¡®ä¿HDFSé›†ç¾¤å’ŒYARNé›†ç¾¤æˆåŠŸå¯åŠ¨ 

```shell
hadoop jar /export/server/hadoop-3.1.4/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.4- tests.jar TestDFSIO -write -nrFiles 10 -fileSize 10MB
```

è¯´æ˜ï¼šå‘HDFSæ–‡ä»¶ç³»ç»Ÿä¸­å†™å…¥æ•°æ®,10ä¸ªæ–‡ä»¶,æ¯ä¸ªæ–‡ä»¶10MB,æ–‡ä»¶å­˜æ”¾åˆ°/benchmarks/TestDFSIOä¸­ 

Throughputï¼šååé‡ã€

Average IO rateï¼šå¹³å‡IOç‡ã€

IO rate std deviationï¼šIOç‡æ ‡å‡†åå·®



**æµ‹è¯•è¯»å–é€Ÿåº¦**

ç¡®ä¿HDFSé›†ç¾¤å’ŒYARNé›†ç¾¤æˆåŠŸå¯åŠ¨ 

```sh
hadoop jar /export/server/hadoop-3.1.4/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.4- tests.jar TestDFSIO -read -nrFiles 10 -fileSize 10MB
```

è¯´æ˜ï¼šåœ¨HDFSæ–‡ä»¶ç³»ç»Ÿä¸­è¯»å…¥10ä¸ªæ–‡ä»¶,æ¯ä¸ªæ–‡ä»¶10M 

Throughputï¼šååé‡ã€

Average IO rateï¼šå¹³å‡IOç‡ã€

IO rate std deviationï¼šIOç‡æ ‡å‡†åå·®



**æ¸…é™¤æµ‹è¯•æ•°æ®**

ç¡®ä¿HDFSé›†ç¾¤æˆåŠŸå¯åŠ¨ 

```sh
hadoop jar /export/server/hadoop-3.1.4/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.4- tests.jar TestDFSIO -clean
```

è¯´æ˜ï¼šæµ‹è¯•æœŸé—´ï¼Œä¼šåœ¨HDFSé›†ç¾¤ä¸Šåˆ›å»º /benchmarksç›®å½•ï¼Œæµ‹è¯•å®Œæ¯•åï¼Œå¯ä»¥æ¸…ç†è¯¥ç›®å½•ã€‚



### å®‰è£…Zookeeper

#### åˆ†å¸ƒå¼å®‰è£…éƒ¨ç½²

##### 1ï¼‰é›†ç¾¤è§„åˆ’

â€‹	åœ¨node01ã€node02 å’Œ node03ä¸‰ä¸ªèŠ‚ç‚¹ä¸Šéƒ¨ç½²Zookeeperã€‚

##### 2ï¼‰è§£å‹å®‰è£…

 ï¼ˆ1ï¼‰è§£å‹Zookeeperå®‰è£…åŒ…åˆ°/opt/module/ç›®å½•ä¸‹

```shell
[dsjprs@node01 software]$ tar -zxvf zookeeper-3.5.7.tar.gz -C /opt/module/
```

ï¼ˆ2ï¼‰ä¿®æ”¹/opt/module/apache-zookeeper-3.5.7-binåç§°ä¸ºzookeeper-3.5.7

```shell
[dsjprs@node01 module]$ mv apache-zookeeper-3.5.7-bin/ zookeeper-3.5.7
```

ï¼ˆ3ï¼‰åŒæ­¥/opt/module/zookeeper-3.5.7ç›®å½•å†…å®¹åˆ°node02ã€node03

```shell
[dsjprs@node01 module]$ xsync zookeeper-3.5.7/
```

##### 3ï¼‰é…ç½®æœåŠ¡å™¨ç¼–å·

ï¼ˆ1ï¼‰åœ¨/opt/module/zookeeper-3.5.7/è¿™ä¸ªç›®å½•ä¸‹åˆ›å»ºzkData

```shell
[dsjprs@node01 zookeeper-3.5.7]$ mkdir zkData
```

ï¼ˆ2ï¼‰åœ¨/opt/module/zookeeper-3.5.7/zkDataç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ªmyidçš„æ–‡ä»¶

```shell
[dsjprs@node01 zkData]$ vi myid
```

 æ·»åŠ myidæ–‡ä»¶ï¼Œæ³¨æ„ä¸€å®šè¦åœ¨linuxé‡Œé¢åˆ›å»ºï¼Œåœ¨notepad++é‡Œé¢å¾ˆå¯èƒ½ä¹±ç 

 åœ¨æ–‡ä»¶ä¸­æ·»åŠ ä¸serverå¯¹åº”çš„ç¼–å·ï¼š1

ï¼ˆ3ï¼‰æ‹·è´é…ç½®å¥½çš„zookeeperåˆ°å…¶ä»–æœºå™¨ä¸Š

```shell
[dsjrsp@node01 zkData]$ xsync myid
```

 å¹¶åˆ†åˆ«åœ¨node02, node03ä¸Šä¿®æ”¹myidæ–‡ä»¶ä¸­å†…å®¹ä¸º2, 3

##### 4ï¼‰é…ç½®zoo.cfgæ–‡ä»¶

ï¼ˆ1ï¼‰é‡å‘½å/opt/module/zookeeper-3.5.7/confè¿™ä¸ªç›®å½•ä¸‹çš„zoo_sample.cfgä¸ºzoo.cfg

```shell
[dsjprs@node01 conf]$ mv zoo_sample.cfg zoo.cfg
```

ï¼ˆ2ï¼‰æ‰“å¼€zoo.cfgæ–‡ä»¶

```shell
[dsjprs@node01 conf]$ vim zoo.cfg
```

 ä¿®æ”¹æ•°æ®å­˜å‚¨è·¯å¾„é…ç½®

```shell
dataDir=/opt/module/zookeeper-3.5.7/zkData

å¢åŠ å¦‚ä¸‹é…ç½®
#######################cluster##########################

server.1=node01:2888:3888
server.2=node02:2888:3888
server.3=node03:2888:3888
```

ï¼ˆ3ï¼‰åŒæ­¥zoo.cfgé…ç½®æ–‡ä»¶

```shell
[dsjprs@node01 conf]$ xsync zoo.cfg
```

ï¼ˆ4ï¼‰é…ç½®å‚æ•°è§£è¯»

```shell
Aæ˜¯ä¸€ä¸ªæ•°å­—ï¼Œè¡¨ç¤ºè¿™ä¸ªæ˜¯ç¬¬å‡ å·æœåŠ¡å™¨ï¼›
é›†ç¾¤æ¨¡å¼ä¸‹é…ç½®ä¸€ä¸ªæ–‡ä»¶myidï¼Œè¿™ä¸ªæ–‡ä»¶åœ¨dataDirç›®å½•ä¸‹ï¼Œè¿™ä¸ªæ–‡ä»¶é‡Œé¢æœ‰ä¸€ä¸ªæ•°æ®å°±æ˜¯Açš„å€¼ï¼ŒZookeeperå¯åŠ¨æ—¶è¯»å–æ­¤æ–‡ä»¶ï¼Œæ‹¿åˆ°é‡Œé¢çš„æ•°æ®ä¸zoo.cfgé‡Œé¢çš„é…ç½®ä¿¡æ¯æ¯”è¾ƒä»è€Œåˆ¤æ–­åˆ°åº•æ˜¯å“ªä¸ªserverã€‚

B:æ˜¯è¿™ä¸ªæœåŠ¡å™¨çš„åœ°å€ï¼›

C:æ˜¯è¿™ä¸ªæœåŠ¡å™¨Followerä¸é›†ç¾¤ä¸­çš„LeaderæœåŠ¡å™¨äº¤æ¢ä¿¡æ¯çš„ç«¯å£ï¼›

D:æ˜¯ä¸‡ä¸€é›†ç¾¤ä¸­çš„LeaderæœåŠ¡å™¨æŒ‚äº†ï¼Œéœ€è¦ä¸€ä¸ªç«¯å£æ¥é‡æ–°è¿›è¡Œé€‰ä¸¾ï¼Œé€‰å‡ºä¸€ä¸ªæ–°çš„Leaderï¼Œè€Œè¿™ä¸ªç«¯å£å°±æ˜¯ç”¨æ¥æ‰§è¡Œé€‰ä¸¾æ—¶æœåŠ¡å™¨ç›¸äº’é€šä¿¡çš„ç«¯å£ã€‚
```

##### 5) é›†ç¾¤æ“ä½œ

ï¼ˆ1ï¼‰åˆ†åˆ«å¯åŠ¨Zookeeper

```shell
[dsjprs@node01 zookeeper-3.5.7]$ bin/zkServer.sh start
[dsjprs@node01 zookeeper-3.5.7]$ bin/zkServer.sh start
[dsjprs@node01 zookeeper-3.5.7]$ bin/zkServer.sh start
```

ï¼ˆ2ï¼‰æŸ¥çœ‹çŠ¶æ€

```shell
[dsjprs@node01 zookeeper-3.5.7]# bin/zkServer.sh status
JMX enabled by default
Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg
Mode: follower
[dsjprs@node02 zookeeper-3.5.7]# bin/zkServer.sh status
JMX enabled by default
Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg
Mode: leader
[dsjprs@node03 zookeeper-3.4.5]# bin/zkServer.sh status
JMX enabled by default
Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg
Mode: follower
```

##### 6ï¼‰å¯åŠ¨å®¢æˆ·ç«¯

```shell
[dsjprs@node01 zookeeper-3.5.7]$ bin/zkCli.sh
```

##### 7) ZKé›†ç¾¤å¯åŠ¨åœæ­¢è„šæœ¬

ï¼ˆ1ï¼‰åœ¨hadoop102çš„/home/atguigu/binç›®å½•ä¸‹åˆ›å»ºè„šæœ¬

```shell
[dsjprs@node01 bin]$ vim zk.sh
```

 åœ¨è„šæœ¬ä¸­ç¼–å†™å¦‚ä¸‹å†…å®¹

```shell
#!/bin/bash

case $1 in
"start"){
	for i in node01 node02 node03
	do
        echo ---------- zookeeper $i å¯åŠ¨ ------------
		ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh start"
	done
};;
"stop"){
	for i in node01 node02 node03
	do
        echo ---------- zookeeper $i åœæ­¢ ------------    
		ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh stop"
	done
};;
"status"){
	for i in node01 node02 node03
	do
        echo ---------- zookeeper $i çŠ¶æ€ ------------    
		ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh status"
	done
};;
esac
```

ï¼ˆ2ï¼‰å¢åŠ è„šæœ¬æ‰§è¡Œæƒé™

```shell
[dsjprs@node01 bin]$ chmod u+x zk.sh
```

ï¼ˆ3ï¼‰Zookeeperé›†ç¾¤å¯åŠ¨è„šæœ¬

```shell
[dsjprs@node01 module]$ zk.sh start
```

ï¼ˆ4ï¼‰Zookeeperé›†ç¾¤åœæ­¢è„šæœ¬

```shell
[dsjprs@node01 module]$ zk.sh stop
```

### å®‰è£…Kafkaé›†ç¾¤

#### **å®‰è£…éƒ¨ç½²**

#### é›†ç¾¤è§„åˆ’

![image-20211023123207493](Images/image-20211023123207493.png)

#### é›†ç¾¤éƒ¨ç½²

##### 1ï¼‰è§£å‹å®‰è£…åŒ…

```shell
[dsjprs@node01 software]$ tar -zxvf kafka_2.11-2.4.1.tgz -C /opt/module/
```

##### 2ï¼‰ä¿®æ”¹è§£å‹åçš„æ–‡ä»¶åç§°

```shell
[dsjprs@node01 module]$ mv kafka_2.11-2.4.1/ kafka
```

##### 3ï¼‰åœ¨/opt/module/kafkaç›®å½•ä¸‹åˆ›å»ºlogsæ–‡ä»¶å¤¹

```shell
[dsjprs@node01 kafka]$ mkdir logs
```

##### 4ï¼‰ä¿®æ”¹é…ç½®æ–‡ä»¶

```shell
[dsjprs@node01 kafka]$ cd config/
[dsjprs@node01 config]$ vi server.properties
ä¿®æ”¹æˆ–è€…å¢åŠ ä»¥ä¸‹å†…å®¹ï¼š
#brokerçš„å…¨å±€å”¯ä¸€ç¼–å·ï¼Œä¸èƒ½é‡å¤
broker.id=0
#åˆ é™¤topicåŠŸèƒ½ä½¿èƒ½
delete.topic.enable=true
#kafkaè¿è¡Œæ—¥å¿—å­˜æ”¾çš„è·¯å¾„
log.dirs=/opt/module/kafka/data
#é…ç½®è¿æ¥Zookeeperé›†ç¾¤åœ°å€
zookeeper.connect=node01:2181,node02:2181,node03:2181/kafka
```

##### 5ï¼‰é…ç½®ç¯å¢ƒå˜é‡

```shell
[dsjprs@node01 module]$ sudo vi /etc/profile.d/my_env.sh

#KAFKA_HOME
export KAFKA_HOME=/opt/module/kafka
export PATH=$PATH:$KAFKA_HOME/bin

[dsjprs@node01 module]$ source /etc/profile.d/my_env.sh
```

##### 6ï¼‰åˆ†å‘å®‰è£…åŒ…å’Œmy_env.shç¯å¢ƒé…ç½®æ–‡ä»¶

```shell
[dsjprs@node01 module]$ xsync kafka/
[dsjprs@node01 module]$ xsync /etc/profile.d/my_env.sh
```

##### 7ï¼‰åˆ†åˆ«åœ¨node02å’Œnode3ä¸Šä¿®æ”¹é…ç½®æ–‡ä»¶

```shell
/opt/module/kafka/configserver.propertiesä¸­

broker.id=1
broker.id=2

æ³¨ï¼šbroker.idä¸å¾—é‡å¤
```

##### 8ï¼‰å¯åŠ¨é›†ç¾¤

ä¾æ¬¡åœ¨node01ã€node02ã€node03èŠ‚ç‚¹ä¸Šå¯åŠ¨kafka

```shell
[dsjprs@node01 kafka]$ bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties
[dsjprs@node02 kafka]$ bin/kafka-server-start.sh -daemon  /opt/module/kafka/config/server.properties
[dsjprs@node03 kafka]$ bin/kafka-server-start.sh -daemon  /opt/module/kafka/config/server.properties
```

##### 9ï¼‰å…³é—­é›†ç¾¤

```shell
[dsjprs@node01 kafka]$ bin/kafka-server-stop.sh
[dsjprs@node02 kafka]$ bin/kafka-server-stop.sh
[dsjprs@node03 kafka]$ bin/kafka-server-stop.sh
```

##### 10ï¼‰Kafkaé›†ç¾¤å¯åŠ¨åœæ­¢è„šæœ¬

ï¼ˆ1ï¼‰åœ¨/home/atguigu/binç›®å½•ä¸‹åˆ›å»ºè„šæœ¬kf.sh

```shell
[dsjprs@node01 bin]$ vim kf.sh
```

 åœ¨è„šæœ¬ä¸­å¡«å†™å¦‚ä¸‹å†…å®¹

```shell
#! /bin/bash

case $1 in
"start"){
    for i in node01 node02 node3
    do
        echo " --------å¯åŠ¨ $i Kafka-------"
        ssh $i "/opt/module/kafka/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties"
    done
};;
"stop"){
    for i in node01 node02 node03
    do
        echo " --------åœæ­¢ $i Kafka-------"
        ssh $i "/opt/module/kafka/bin/kafka-server-stop.sh stop"
    done
};;
esac
```

ï¼ˆ2ï¼‰å¢åŠ è„šæœ¬æ‰§è¡Œæƒé™

```shell
[dsjprs@node01 bin]$ chmod u+x kf.sh
```

ï¼ˆ3ï¼‰kfé›†ç¾¤å¯åŠ¨è„šæœ¬

```shell
[dsjprs@node01 module]$ kf.sh start
```

ï¼ˆ4ï¼‰kfé›†ç¾¤åœæ­¢è„šæœ¬

```
[dsjprs@node01 module]$ kf.sh stop
```

##### 11ï¼‰Kafkaå¸¸ç”¨å‘½ä»¤

ï¼ˆ1ï¼‰æŸ¥çœ‹kafka topic åˆ—è¡¨

```shell
[dsjprs@node01 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181/kafka --list
```

ï¼ˆ2ï¼‰åˆ›å»ºkakfka topic

  è¿›å…¥åˆ°/opt/module/kafka/ç›®å½•ä¸‹åˆ›å»ºæ—¥å¿—ä¸»é¢˜

```shell
[dsjprs@node01 kafka]$ bin/kafka-topics.sh --zookeeper node01:2181,node02:2181,node03:2181/kafka  --create --replication-factor 1 --partitions 1 --topic topic_log
```

ï¼ˆ3ï¼‰åˆ é™¤kafka topic

```shell
[dsjprs@node01 kafka]$ bin/kafka-topics.sh --delete --zookeeper node01:2181,node02:2181,node03:2181/kafka --topic topic_log
```

ï¼ˆ4ï¼‰kafkaç”Ÿäº§æ¶ˆæ¯

```shell
[dsjprs@node01 kafka]$ bin/kafka-console-producer.sh \
--broker-list hadoop102:9092 --topic topic_log
>hello world
>dsjprs code
```

ï¼ˆ5ï¼‰kafkaæ¶ˆè´¹æ¶ˆæ¯

```shell
[dsjprs@node01 kafka]$ bin/kafka-console-consumer.sh \
--bootstrap-server node01:9092 --from-beginning --topic topic_log
```

--from-beginningï¼š

ä¼šæŠŠä¸»é¢˜ä¸­ä»¥å¾€æ‰€æœ‰çš„æ•°æ®éƒ½è¯»å–å‡ºæ¥ã€‚æ ¹æ®ä¸šåŠ¡åœºæ™¯é€‰æ‹©æ˜¯å¦å¢åŠ è¯¥é…ç½®ã€‚

ï¼ˆ6ï¼‰æŸ¥çœ‹kafka topic è¯¦æƒ…

```shell
[dsjprs@node kafka]$ bin/kafka-topics.sh --zookeeper node01:2181/kafka \
--describe --topic topic_log
```

##### kafkaå‹åŠ›æµ‹è¯•

1ï¼‰kafka å‹æµ‹

  ç”¨Kafkaå®˜æ–¹è‡ªå¸¦çš„è„šæœ¬ï¼Œå¯¹Kafkaè¿›è¡Œå‹æµ‹ã€‚

```sh
kafka-consumer-perf-test.sh

kafka-producer-perf-test.sh
```

  Kafkaå‹æµ‹æ—¶ï¼Œåœ¨ç¡¬ç›˜è¯»å†™é€Ÿåº¦ä¸€å®šçš„æƒ…å†µä¸‹ï¼Œå¯ä»¥æŸ¥çœ‹åˆ°å“ªäº›åœ°æ–¹å‡ºç°äº†ç“¶é¢ˆï¼ˆCPUï¼Œå†…å­˜ï¼Œç½‘ç»œIOï¼‰ã€‚ä¸€èˆ¬éƒ½æ˜¯ç½‘ç»œIOè¾¾åˆ°ç“¶é¢ˆã€‚

2ï¼‰kafka prodeucer å‹åŠ›æµ‹è¯•

  ç¯å¢ƒå‡†å¤‡

  â‘ node01ã€node02ã€node03çš„ç½‘ç»œå¸¦å®½éƒ½è®¾ç½®ä¸º100mbpsã€‚

  â‘¡å…³é—­node01ä¸»æœºï¼Œå¹¶æ ¹æ®node01å…‹éš†å‡ºnode04ï¼ˆä¿®æ”¹IPå’Œä¸»æœºåç§°ï¼‰

  â‘¢node04çš„å¸¦å®½ä¸è®¾é™

  â‘£åˆ›å»ºä¸€ä¸ªtest topicï¼Œè®¾ç½®ä¸º3ä¸ªåˆ†åŒº2ä¸ªå‰¯æœ¬

```shell
[dsjprs@node01 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka --create --replication-factor 2 --partitions 3 --topic test
```

 åœ¨/opt/module/kafka/binç›®å½•ä¸‹é¢æœ‰è¿™ä¸¤ä¸ªæ–‡ä»¶ã€‚æˆ‘ä»¬æ¥æµ‹è¯•ä¸€ä¸‹

```shell
[dsjprs@node01 kafka]$ bin/kafka-producer-perf-test.sh  --topic test --record-size 100 --num-records 10000000 --throughput -1 --producer-props bootstrap.servers=node01:9092,node02:9092,node03:9092
```

è¯´æ˜ï¼š

record-sizeï¼šæ˜¯ä¸€æ¡ä¿¡æ¯æœ‰å¤šå¤§ï¼Œå•ä½æ˜¯å­—èŠ‚ã€‚

num-recordsï¼šæ˜¯æ€»å…±å‘é€å¤šå°‘æ¡ä¿¡æ¯ã€‚

throughputï¼šæ˜¯æ¯ç§’å¤šå°‘æ¡ä¿¡æ¯ï¼Œè®¾æˆ-1ï¼Œè¡¨ç¤ºä¸é™æµï¼Œå°½å¯èƒ½å¿«çš„ç”Ÿäº§æ•°æ®ï¼Œå¯æµ‹å‡ºç”Ÿäº§è€…æœ€å¤§ååé‡ã€‚



Kafkaä¼šæ‰“å°ä¸‹é¢çš„ä¿¡æ¯

```shell
699884 records sent, 139976.8 records/sec (13.35 MB/sec), 1345.6 ms avg latency, 2210.0 ms max latency.
713247 records sent, 141545.3 records/sec (13.50 MB/sec), 1577.4 ms avg latency, 3596.0 ms max latency.
773619 records sent, 153862.2 records/sec (14.67 MB/sec), 2326.8 ms avg latency, 4051.0 ms max latency.
773961 records sent, 154206.2 records/sec (15.71 MB/sec), 1964.1 ms avg latency, 2917.0 ms max latency.
776970 records sent, 154559.4 records/sec (15.74 MB/sec), 1960.2 ms avg latency, 2922.0 ms max latency.
776421 records sent, 154727.2 records/sec (15.76 MB/sec), 1960.4 ms avg latency, 2954.0 ms max latency.
```

å‚æ•°è§£æï¼šKafkaçš„ååé‡15m/så·¦å³æ˜¯å¦ç¬¦åˆé¢„æœŸå‘¢ï¼Ÿ

node01ã€node02ã€node03ä¸‰å°é›†ç¾¤çš„ç½‘ç»œæ€»å¸¦å®½30m/så·¦å³ï¼Œç”±äºæ˜¯ä¸¤ä¸ªå‰¯æœ¬ï¼Œæ‰€ä»¥Kafkaçš„ååé‡30m/s â— 2ï¼ˆå‰¯æœ¬ï¼‰ = 15m/s

ç»“è®ºï¼šç½‘ç»œå¸¦å®½å’Œå‰¯æœ¬éƒ½ä¼šå½±å“ååé‡ã€‚



è°ƒæ•´batch.size

batch.sizeï¼šé»˜è®¤å€¼æ˜¯16kã€‚

batch.sizeï¼šè¾ƒå°ï¼Œä¼šé™ä½ååé‡ã€‚æ¯”å¦‚è¯´ï¼Œæ‰¹æ¬¡å¤§å°ä¸º0åˆ™å®Œå…¨ç¦ç”¨æ‰¹å¤„ç†ï¼Œä¼šä¸€æ¡ä¸€æ¡å‘é€æ¶ˆæ¯ï¼‰ï¼›

batch.sizeï¼šè¿‡å¤§ï¼Œä¼šå¢åŠ æ¶ˆæ¯å‘é€å»¶è¿Ÿã€‚æ¯”å¦‚è¯´ï¼ŒBatchè®¾ç½®ä¸º64kï¼Œä½†æ˜¯è¦ç­‰å¾…5ç§’é’ŸBatchæ‰å‡‘æ»¡äº†64kï¼Œæ‰èƒ½å‘é€å‡ºå»ã€‚é‚£è¿™æ¡æ¶ˆæ¯çš„å»¶è¿Ÿå°±æ˜¯5ç§’é’Ÿã€‚

```shell
[dsjprs@node01 kafka]$ bin/kafka-producer-perf-test.sh  --topic test --record-size 100 --num-records 10000000 --throughput -1 --producer-props bootstrap.servers=node01:9092,node02:9092,node03:9092 batch.size=500
```

è¾“å‡ºç»“æœ:

```shell
69169 records sent, 13833.8 records/sec (1.32 MB/sec), 2517.6 ms avg latency, 4299.0 ms max latency.
105372 records sent, 21074.4 records/sec (2.01 MB/sec), 6748.4 ms avg latency, 9016.0 ms max latency.
113188 records sent, 22637.6 records/sec (2.16 MB/sec), 11348.0 ms avg latency, 13196.0 ms max latency.
108896 records sent, 21779.2 records/sec (2.08 MB/sec), 12272.6 ms avg latency, 12870.0 ms max latency.
```



linger.ms

å¦‚æœè®¾ç½®batch sizeä¸º64kï¼Œä½†æ˜¯æ¯”å¦‚è¿‡äº†10åˆ†é’Ÿä¹Ÿæ²¡æœ‰å‡‘å¤Ÿ64kï¼Œæ€ä¹ˆåŠï¼Ÿ

å¯ä»¥è®¾ç½®ï¼Œlinger.msã€‚æ¯”å¦‚linger.ms=5msï¼Œé‚£ä¹ˆå°±æ˜¯è¦å‘é€çš„æ•°æ®æ²¡æœ‰åˆ°64kï¼Œ5msåï¼Œæ•°æ®ä¹Ÿä¼šå‘å‡ºå»ã€‚



æ€»ç»“

åŒæ—¶è®¾ç½®batch.sizeå’Œ linger.msï¼Œå°±æ˜¯å“ªä¸ªæ¡ä»¶å…ˆæ»¡è¶³å°±éƒ½ä¼šå°†æ¶ˆæ¯å‘é€å‡ºå»

Kafkaéœ€è¦è€ƒè™‘é«˜ååé‡ä¸å»¶æ—¶çš„å¹³è¡¡ã€‚



3ï¼‰ kafka consunmer å‹åŠ›æµ‹è¯•

ï¼ˆ1ï¼‰Consumerçš„æµ‹è¯•ï¼Œå¦‚æœè¿™å››ä¸ªæŒ‡æ ‡ï¼ˆIOï¼ŒCPUï¼Œå†…å­˜ï¼Œç½‘ç»œï¼‰éƒ½ä¸èƒ½æ”¹å˜ï¼Œè€ƒè™‘å¢åŠ åˆ†åŒºæ•°æ¥æå‡æ€§èƒ½ã€‚

```shell
[dsjprs@node01 kafka]$ bin/kafka-consumer-perf-test.sh --broker-list node01:9092,node02:9092,node03:9092 --topic test --fetch-size 10000 --messages 10000000 --threads 1
```

â‘ å‚æ•°è¯´æ˜ï¼š

--broker-listï¼šæŒ‡å®šKafkaé›†ç¾¤åœ°å€

--topicï¼šæŒ‡å®štopicçš„åç§°

--fetch-sizeï¼šæŒ‡å®šæ¯æ¬¡fetchçš„æ•°æ®çš„å¤§å°

--messagesï¼šæ€»å…±è¦æ¶ˆè´¹çš„æ¶ˆæ¯ä¸ªæ•°



â‘¡æµ‹è¯•ç»“æœè¯´æ˜ï¼š

```shell
start.time, end.time, data.consumed.in.MB, MB.sec,data.consumed.in.nMsg, nMsg.sec

2021-08-03 21:17:21:778, 2021-08-03 21:18:19:775, 514.7169, 8.8749, 5397198, 93059.9514
```

å¼€å§‹æµ‹è¯•æ—¶é—´ï¼Œæµ‹è¯•ç»“æŸæ•°æ®ï¼Œå…±æ¶ˆè´¹æ•°æ®**514.7169MBï¼Œååé‡8.8749**MB/s

ï¼ˆ2ï¼‰è°ƒæ•´fetch-size

â€‹	â‘ å¢åŠ fetch-sizeå€¼ï¼Œè§‚å¯Ÿæ¶ˆè´¹ååé‡ã€‚

```shell
dsjprs@node01 kafka]$ bin/kafka-consumer-perf-test.sh --broker-list node01:9092,node02:9092,node03:9092 --topic test --fetch-size 100000 --messages 10000000 --threads 1
```

â‘¡æµ‹è¯•ç»“æœè¯´æ˜ï¼š

```shell
start.time, end.time, data.consumed.in.MB, MB.sec,data.consumed.in.nMsg, nMsg.sec

2021-08-03 21:22:57:671, 2021-08-03 21:23:41:938, 514.7169, 11.6276, 5397198, 121923.7355
```

ï¼ˆ3ï¼‰æ€»ç»“

ååé‡å—ç½‘ç»œå¸¦å®½å’Œfetch-sizeçš„å½±å“



4ï¼‰Kafkaåˆ†åŒºæ•°è®¡ç®—

ï¼ˆ1ï¼‰åˆ›å»ºä¸€ä¸ªåªæœ‰1ä¸ªåˆ†åŒºçš„topic

ï¼ˆ2ï¼‰æµ‹è¯•è¿™ä¸ªtopicçš„producerååé‡å’Œconsumerååé‡ã€‚

ï¼ˆ3ï¼‰å‡è®¾ä»–ä»¬çš„å€¼åˆ†åˆ«æ˜¯Tpå’ŒTcï¼Œå•ä½å¯ä»¥æ˜¯MB/sã€‚

ï¼ˆ4ï¼‰ç„¶åå‡è®¾æ€»çš„ç›®æ ‡ååé‡æ˜¯Ttï¼Œé‚£ä¹ˆåˆ†åŒºæ•° = Tt / minï¼ˆTpï¼ŒTcï¼‰

  ä¾‹å¦‚ï¼šproducerååé‡ = 20m/sï¼›consumerååé‡ = 50m/sï¼ŒæœŸæœ›ååé‡   

  100m/sï¼›

  åˆ†åŒºæ•° = 100 / 20 = 5åˆ†åŒº

  https://blog.csdn.net/weixin_42641909/article/details/89294698

  åˆ†åŒºæ•°ä¸€èˆ¬è®¾ç½®ä¸ºï¼š3-10ä¸ª





### å®‰è£… Flume æ—¥å¿—é‡‡é›†

#### **å®‰è£…éƒ¨ç½²**

ï¼ˆ1ï¼‰å°†apache-flume-1.9.0-bin.tar.gzä¸Šä¼ åˆ°linuxçš„/opt/softwareç›®å½•ä¸‹

ï¼ˆ2ï¼‰è§£å‹apache-flume-1.9.0-bin.tar.gzåˆ°/opt/module/ç›®å½•ä¸‹

```shell
[dsjprs@node01 software]$ tar -zxf /opt/software/apache-flume-1.9.0-bin.tar.gz -C /opt/module/
```

ï¼ˆ3ï¼‰ä¿®æ”¹apache-flume-1.9.0-binçš„åç§°ä¸ºflume

```shell
[dsjprs@node01 module]$ mv /opt/module/apache-flume-1.9.0-bin /opt/module/flume
```

ï¼ˆ4ï¼‰å°†libæ–‡ä»¶å¤¹ä¸‹çš„guava-11.0.2.jaråˆ é™¤ä»¥å…¼å®¹Hadoop 3.1.3

```shell
[dsjprs@node01 module]$ rm /opt/module/flume/lib/guava-11.0.2.jar
```

æ³¨æ„ï¼šåˆ é™¤guava-11.0.2.jarçš„æœåŠ¡å™¨èŠ‚ç‚¹ï¼Œä¸€å®šè¦é…ç½®hadoopç¯å¢ƒå˜é‡ã€‚å¦åˆ™ä¼šæŠ¥å¦‚ä¸‹å¼‚å¸¸ã€‚

```shell
Caused by: java.lang.ClassNotFoundException: com.google.common.collect.Lists
 at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
 at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
 at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
 at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
 ... 1 more
```

ï¼ˆ5ï¼‰å°†flume/confä¸‹çš„flume-env.sh.templateæ–‡ä»¶ä¿®æ”¹ä¸ºflume-env.shï¼Œå¹¶é…ç½®flume-env.shæ–‡ä»¶

```shell
[dsjprs@node01 conf]$ mv flume-env.sh.template flume-env.sh

[dsjprs@node01 conf]$ vi flume-env.sh

export JAVA_HOME=/opt/module/jdk1.8.0_212
```

#### Flumeé›†ç¾¤åœæ­¢å¯åŠ¨è„šæœ¬

  åœ¨/home/atguigu/binç›®å½•ä¸‹åˆ›å»ºè„šæœ¬f2.sh

```shell
[dsjprs@node01 bin]$ vim f2.sh
```

â€‹	åœ¨è„šæœ¬ä¸­å¡«å†™å¦‚ä¸‹å†…å®¹

```shell
#! /bin/bash

case $1 in
"start"){
        for i in node03
        do
            echo " --------å¯åŠ¨ $i æ¶ˆè´¹flume-------"
            ssh $i "nohup /opt/module/flume/bin/flume-ng agent --conf-file /opt/module/flume/conf/kafka-flume-hdfs.conf --name a1 -Dflume.root.logger=INFO,LOGFILE >/opt/module/flume/log2.txt   2>&1 &"
        done
};;
"stop"){
        for i in node03
        do
            echo " --------åœæ­¢ $i æ¶ˆè´¹flume-------"
            ssh $i "ps -ef | grep kafka-flume-hdfs | grep -v grep |awk '{print \$2}' | xargs -n1 kill"
        done

};;
esac
```

### å®‰è£…MySQL

#### å®‰è£…åŒ…å‡†å¤‡

##### 1ï¼‰å°†å®‰è£…åŒ…å’ŒJDBCé©±åŠ¨ä¸Šä¼ åˆ°/opt/softwareï¼Œå…±è®¡6ä¸ª

```shell
01_mysql-community-common-5.7.16-1.el7.x86_64.rpm
02_mysql-community-libs-5.7.16-1.el7.x86_64.rpm
03_mysql-community-libs-compat-5.7.16-1.el7.x86_64.rpm
04_mysql-community-client-5.7.16-1.el7.x86_64.rpm
05_mysql-community-server-5.7.16-1.el7.x86_64.rpm
mysql-connector-java-5.1.27-bin.jar
```

##### 2ï¼‰å¦‚æœæ˜¯è™šæ‹ŸæœºæŒ‰ç…§å¦‚ä¸‹æ­¥éª¤æ‰§è¡Œ

ï¼ˆ1ï¼‰å¸è½½è‡ªå¸¦çš„Mysql-libsï¼ˆå¦‚æœä¹‹å‰å®‰è£…è¿‡MySQLï¼Œè¦å…¨éƒ½å¸è½½æ‰ï¼‰

```shell
[dsjprs@node01 software]$ rpm -qa | grep -i -E mysql\|mariadb | xargs -n1 sudo rpm -e --nodeps
```

##### 3ï¼‰å¦‚æœæ˜¯é˜¿é‡Œäº‘æœåŠ¡å™¨æŒ‰ç…§å¦‚ä¸‹æ­¥éª¤æ‰§è¡Œ

è¯´æ˜ï¼šç”±äºé˜¿é‡Œäº‘æœåŠ¡å™¨å®‰è£…çš„æ˜¯Linuxæœ€å°ç³»ç»Ÿç‰ˆï¼Œæ²¡æœ‰å¦‚ä¸‹å·¥å…·ï¼Œæ‰€ä»¥éœ€è¦å®‰è£…ã€‚

ï¼ˆ1ï¼‰å¸è½½MySQLä¾èµ–ï¼Œè™½ç„¶æœºå™¨ä¸Šæ²¡æœ‰è£…MySQLï¼Œä½†æ˜¯è¿™ä¸€æ­¥ä¸å¯å°‘

```shell
[dsjprs@node01 software]# sudo yum remove mysql-libs
```

ï¼ˆ2ï¼‰ä¸‹è½½ä¾èµ–å¹¶å®‰è£…

```shell
[dsjprs@node01 software]# sudo yum install libaio

[dsjprs@node01 software]# sudo yum -y install autoconf
```

#### **å®‰è£…MySQL**

##### 1ï¼‰å®‰è£…MySQLä¾èµ–

```shell
[dsjprs@node01 software]$ sudo rpm -ivh 01_mysql-community-common-5.7.16-1.el7.x86_64.rpm
[dsjprs@node01 software]$ sudo rpm -ivh 02_mysql-community-libs-5.7.16-1.el7.x86_64.rpm
[dsjprs@node01 software]$ sudo rpm -ivh 03_mysql-community-libs-compat-5.7.16-1.el7.x86_64.rpm
```

##### 2ï¼‰å®‰è£…mysql-client

```shell
[dsjprs@node01 software]$ sudo rpm -ivh 04_mysql-community-client-5.7.16-1.el7.x86_64.rpm
```

##### 3ï¼‰å®‰è£…mysql-server

```shell
[dsjprs@node01 software]$ sudo rpm -ivh 05_mysql-community-server-5.7.16-1.el7.x86_64.rpm
```

æ³¨æ„ï¼šå¦‚æœæŠ¥å¦‚ä¸‹é”™è¯¯ï¼Œè¿™æ˜¯ç”±äºyumå®‰è£…äº†æ—§ç‰ˆæœ¬çš„GPG keysæ‰€é€ æˆï¼Œä»rpmç‰ˆæœ¬4.1åï¼Œåœ¨å®‰è£…æˆ–å‡çº§è½¯ä»¶åŒ…æ—¶ä¼šè‡ªåŠ¨æ£€æŸ¥è½¯ä»¶åŒ…çš„ç­¾åã€‚

```shell
warning: 05_mysql-community-server-5.7.16-1.el7.x86_64.rpm: Header V3 DSA/SHA1 Signature, key ID 5072e1f5: NOKEY
error: Failed dependencies:
libaio.so.1()(64bit) is needed by mysql-community-server-5.7.16-1.el7.x86_64
```

è§£å†³åŠæ³•

```shell
[dsjprs@node01 software]$ sudo rpm -ivh 05_mysql-community-server-5.7.16-1.el7.x86_64.rpm --force --nodeps
```

##### 4ï¼‰å¯åŠ¨MySQL

```shell
[dsjprs@node01 software]$ sudo systemctl start mysqld
```

##### 5ï¼‰æŸ¥çœ‹MySQLå¯†ç 

```shell
[dsjprs@node01 software]$ sudo cat /var/log/mysqld.log | grep password
```

#### é…ç½®MySQL

é…ç½®åªè¦æ˜¯rootç”¨æˆ· + å¯†ç ï¼Œåœ¨ä»»ä½•ä¸»æœºä¸Šéƒ½èƒ½ç™»å½•MySQLæ•°æ®åº“ã€‚

##### 1ï¼‰ç”¨åˆšåˆšæŸ¥åˆ°çš„å¯†ç è¿›å…¥MySQLï¼ˆå¦‚æœæŠ¥é”™ï¼Œç»™å¯†ç åŠ å•å¼•å·ï¼‰

```shell
[dsjprs@node01 software]$ mysql -uroot -p'password'
```

##### 2ï¼‰è®¾ç½®å¤æ‚å¯†ç ï¼ˆç”±äºMySQLå¯†ç ç­–ç•¥ï¼Œæ­¤å¯†ç å¿…é¡»è¶³å¤Ÿå¤æ‚ï¼‰

```mysql
mysql> set password=password("Qs23=zs32");
```

##### 3ï¼‰æ›´æ”¹MySQLå¯†ç ç­–ç•¥

```mysql
mysql> set global validate_password_length=4;
mysql> set global validate_password_policy=0;
```

##### 4ï¼‰è®¾ç½®ç®€å•å¥½è®°çš„å¯†ç 

```mysql
mysql> set password=password("000000");
```

##### 5ï¼‰è¿›å…¥MySQLåº“

```mysql
mysql> use mysql
```

##### 6ï¼‰æŸ¥è¯¢userè¡¨

```mysql
mysql> select user, host from user;
```

##### 7ï¼‰ä¿®æ”¹userè¡¨ï¼ŒæŠŠHostè¡¨å†…å®¹ä¿®æ”¹ä¸º%

```mysql
mysql> update user set host="%" where user="root";
```

##### 8ï¼‰åˆ·æ–°

```mysql
mysql> flush privileges;
```

##### 9ï¼‰é€€å‡º

```mysql
mysql> quit;
```

##### 10ï¼‰linuxç¯å¢ƒä¸‹ MySQLå‡ºç°ä¹±ç çš„è§£å†³æ–¹æ¡ˆ

é¡¹ç›®è¿›è¡Œåˆ°å’ŒæœåŠ¡å™¨äº¤äº’ï¼Œé€šè¿‡postè®¿é—®æœåŠ¡å™¨ç«¯jspï¼Œjspè®¿é—®æœåŠ¡å™¨ç«¯mysqlæ•°æ®åº“ï¼Œæœ€ç»ˆè¿”å›åˆ°å®¢æˆ·ç«¯çš„ä¸­æ–‡å‡ºç°ä¹±ç é—®é¢˜ã€‚



åœ¨æ•´ä¸ªæµç¨‹ä¸­ï¼Œå‡ºç°é”™è¯¯çš„åŸå› å¯èƒ½æ˜¯ä¸‰ä¸ªï¼špostæœªè®¾ç½®ç¼–ç æˆ–è€…ç¼–ç ä¸ç›¸ç¬¦åˆï¼Œjdbcå‡ºç°é—®é¢˜ï¼Œlinuxä¸‹mysqlåˆå§‹ç åˆ¶é—®é¢˜ã€‚

åœ¨ç»è¿‡ç¹ççš„æ’æŸ¥åï¼Œæœ€ç»ˆç¡®å®šé—®é¢˜ä¸ºmysqlç¼–ç é—®é¢˜ã€‚



ä¸‹æ–‡ä»‹ç»å¦‚ä½•è§£å†³linuxä¸‹mysqlä¸­æ–‡ä¹±ç é—®é¢˜ã€‚

é¦–å…ˆè¿›å…¥mysqlå‘½ä»¤è¡Œæ¨¡å¼ï¼Œé”®å…¥mysql -uroot -p å³å¯è¿›å…¥ã€‚

éšåé”®å…¥ 

```mysql
SHOW VARIABLES LIKE 'character_set_%'; 
```

è‹¥æ˜¾ç¤ºå†…å®¹ç±»ä¼¼è¿™æ ·ï¼š

```mysql
+-----------------------+--------------------------------+
| Variable_name      | Value                    |
+-----------------------+--------------------------------+
| character_set_client   	| utf8                     |
| character_set_connection  | utf8                     |
| character_set_database  	| utf8                     |
| character_set_filesystem 	| binary                   |
| character_set_results  	| utf8                     |
| character_set_server   	| utf8                     |
| character_set_system  	| utf8                     |
| character_sets_dir    	| /alidata/server/mysql-5.1.73/share/charsets/ |
```

åˆ™å·²ä¿®æ”¹æ­£ç¡®ï¼Œè€Œmysqlé»˜è®¤åˆå§‹è®¾ç½®çš„éƒ½æ˜¯latin1è€Œéutf8ã€‚

ä¸€ç§è§£å†³æ–¹æ³•æ˜¯æ›´æ”¹è¡¨æ ¼çš„å±æ€§è®¾ç½®ä¸ºutf8æˆ–è€…åœ¨åˆ›å»ºè¡¨æ ¼çš„æ—¶å€™åœ¨æœ€ååŠ ä¸Š`DEFAULT CHARSET=utf8`ã€‚

è®¾ç½®è¡¨æ ¼ä¸ºutf8ç ã€‚è¿™æ ·çš„æ–¹æ³•æœ‰å¯èƒ½å¤±æ•ˆã€‚

æœ€æ ¹æœ¬çš„è§£å†³æ–¹æ³•æ˜¯æ‰“å¼€mysqlé…ç½®æ–‡ä»¶ä¿®æ”¹ã€‚linuxä¸‹mysqlé…ç½®æ–‡ä»¶åä¸ºmy.cnf,ç›®å½•ä¸º/etc/my.cnfï¼Œæ‰“å¼€åæŒ‰ç…§ä»¥ä¸‹æ“ä½œï¼š

```mysql
--åœ¨ [mysqld] æ ‡ç­¾ä¸‹åŠ ä¸Šä¸‰è¡Œ
default-character-set = utf8
character_set_server = utf8
lower_case_table_names = 1 //è¡¨åä¸åŒºåˆ†å¤§å°å†™ï¼ˆæ­¤ä¸ç¼–ç æ— å…³ï¼‰
--åœ¨ [mysql] æ ‡ç­¾ä¸‹åŠ ä¸Šä¸€è¡Œ
default-character-set = utf8
--åœ¨ [mysql.server]æ ‡ç­¾ä¸‹åŠ ä¸Šä¸€è¡Œ
default-character-set = utf8
--åœ¨ [mysqld_safe]æ ‡ç­¾ä¸‹åŠ ä¸Šä¸€è¡Œ
default-character-set = utf8
--åœ¨ [client]æ ‡ç­¾ä¸‹åŠ ä¸Šä¸€è¡Œ
default-character-set = utf8
```

ä¸Šè¿°æ ‡ç­¾è‹¥æœªèƒ½å…¨éƒ¨æ‰¾åˆ°ä¹Ÿæ²¡å…³ç³»ã€‚å†æ¬¡æ‰“å¼€mysqlå‘½ä»¤è¡Œï¼Œæ‰§è¡Œ`SHOW VARIABLES LIKE 'character_set_%';`åè‹¥ä»å­˜åœ¨latin1ï¼Œåˆ™åœ¨mysqlå‘½ä»¤è¡Œä¸‹æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```mysql
  set character_set_client = utf8;
  set character_set_server = utf8;
  set character_set_connection = utf8;
  set character_set_database = utf8;
  set character_set_results = utf8;
  set collation_connection = utf8_general_ci;
  set collation_database = utf8_general_ci;
  set collation_server = utf8_general_ci
```

æ‰§è¡Œåå†æ‰§è¡Œä¸Šè¿°show å‘½ä»¤å¯å¾—åˆ°ç›®æ ‡ç»“æœã€‚

è®¾ç½®å®Œæˆåéœ€è¦é‡æ–°å¯åŠ¨mysqlï¼Œ

é‡å¯å‘½ä»¤ /etc/init.d/mysqld restart  ã€‚

åŸæ•°æ®è¡¨éœ€è¦åˆ é™¤åé‡å»ºã€‚

æœ€ç»ˆå®Œæˆï¼Œå¤§åŠŸå‘Šæˆã€‚



æ€»ç»“ï¼š
1ã€ä¿®æ”¹/etc/my.cnfæ–‡ä»¶ï¼Œå¢åŠ ä»¥ä¸‹å‡ è¡Œï¼š

```mysql
[client]
# pipe=
# socket=MYSQL
port=3306
default-character-set=utf8
[mysql]
no-beep
# default-character-set=
default-character-set=utf8
# SERVER SECTION
# ----------------------------------------------------------------------
# The following options will be read by the MySQL Server. Make sure that
# you have installed the server correctly (see above) so it reads this 
# file.
# server_type=3
[mysqld]
character_set_server=utf8
```

2ã€é‡å¯mysqlæœåŠ¡ï¼š

```mysql
service mysql stopï¼›
service mysql statusï¼›
service mysql startï¼›
æˆ–è€… 
service mysql restartï¼›
```



### å®‰è£… Sqoop 

#### ä¸‹è½½å¹¶è§£å‹

1ï¼‰sqoopå®˜ç½‘åœ°å€ï¼š[http://sqoop.apache.org](http://sqoop.apache.org/docs/1.4.7/index.html)



2ï¼‰ä¸‹è½½åœ°å€ï¼šhttp://mirrors.hust.edu.cn/apache/sqoop/1.4.6/



3ï¼‰ä¸Šä¼ å®‰è£…åŒ…sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gzåˆ°node01çš„/opt/softwareè·¯å¾„ä¸­



4ï¼‰è§£å‹sqoopå®‰è£…åŒ…åˆ°æŒ‡å®šç›®å½•ï¼Œå¦‚ï¼š

```shell
[dsjprs@node01 software]$ tar -zxf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz -C /opt/module/
```

5ï¼‰è§£å‹sqoopå®‰è£…åŒ…åˆ°æŒ‡å®šç›®å½•ï¼Œå¦‚ï¼š

```shell
[dsjprs@node01 module]$ mv sqoop-1.4.6.bin__hadoop-2.0.4-alpha/ sqoop
```

#### **ä¿®æ”¹é…ç½®æ–‡ä»¶**

1ï¼‰è¿›å…¥åˆ°/opt/module/sqoop/confç›®å½•ï¼Œé‡å‘½åé…ç½®æ–‡ä»¶

```shell
[dsjprs@node01 conf]$ mv sqoop-env-template.sh sqoop-env.sh
```

2ï¼‰ä¿®æ”¹é…ç½®æ–‡ä»¶

```shell
[dsjprs@node01 conf]$ vim sqoop-env.sh 
```

å¢åŠ å¦‚ä¸‹å†…å®¹

```shell
export HADOOP_COMMON_HOME=/opt/module/hadoop-3.1.3
export HADOOP_MAPRED_HOME=/opt/module/hadoop-3.1.3
export HIVE_HOME=/opt/module/hive
export ZOOKEEPER_HOME=/opt/module/zookeeper-3.5.7
export ZOOCFGDIR=/opt/module/zookeeper-3.5.7/conf
```

#### **æ‹·è´JDBCé©±åŠ¨**

1ï¼‰å°†mysql-connector-java-5.1.48.jar ä¸Šä¼ åˆ°/opt/softwareè·¯å¾„

2ï¼‰è¿›å…¥åˆ°/opt/software/è·¯å¾„ï¼Œæ‹·è´jdbcé©±åŠ¨åˆ°sqoopçš„libç›®å½•ä¸‹ã€‚

```shell
[dsjprs@node01 software]$ cp mysql-connector-java-5.1.48.jar /opt/module/sqoop/lib/
```

#### **éªŒè¯Sqoop**

ï¼ˆ1ï¼‰æˆ‘ä»¬å¯ä»¥é€šè¿‡æŸä¸€ä¸ªcommandæ¥éªŒè¯sqoopé…ç½®æ˜¯å¦æ­£ç¡®ï¼š

```shell
[dsjprs@node01 sqoop]$ bin/sqoop help
```

ï¼ˆ2ï¼‰å‡ºç°ä¸€äº›Warningè­¦å‘Šï¼ˆè­¦å‘Šä¿¡æ¯å·²çœç•¥ï¼‰ï¼Œå¹¶ä¼´éšç€å¸®åŠ©å‘½ä»¤çš„è¾“å‡ºï¼š

```shell
Available commands:
codegen            Generate code to interact with database records
create-hive-table     Import a table definition into Hive
eval               Evaluate a SQL statement and display the results
export             Export an HDFS directory to a database table
help               List available commands
import             Import a table from a database to HDFS
import-all-tables     Import tables from a database to HDFS
import-mainframe    Import datasets from a mainframe server to HDFS
  job                Work with saved jobs
  list-databases        List available databases on a server
  list-tables           List available tables in a database
  merge              Merge results of incremental imports
  metastore           Run a standalone Sqoop metastore
  version            Display version information
```

#### æµ‹è¯•Sqoopæ˜¯å¦èƒ½å¤ŸæˆåŠŸè¿æ¥æ•°æ®åº“**

```shell
[dsjprs@node01 sqoop]$ bin/sqoop list-databases --connect jdbc:mysql://node01:3306/ --username root --password 000000
```

å‡ºç°å¦‚ä¸‹è¾“å‡ºï¼š

```mysql
information_schema
metastore
mysql
oozie
performance_schema
```

#### SqoopåŸºæœ¬ä½¿ç”¨

å°†mysqlä¸­user_infoè¡¨æ•°æ®å¯¼å…¥åˆ°HDFSçš„/testè·¯å¾„

```shell
bin/sqoop import \
--connect jdbc:mysql://node01:3306/gmall \
--username root \
--password 000000 \
--table user_info \
--columns id,login_name \
--where "id>=10 and id<=30" \
--target-dir /test \
--delete-target-dir \
--fields-terminated-by '\t' \
--num-mappers 2 \
--split-by id
```

#### **åŒæ­¥ç­–ç•¥**

æ•°æ®åŒæ­¥ç­–ç•¥çš„ç±»å‹åŒ…æ‹¬ï¼šå…¨é‡åŒæ­¥ã€å¢é‡åŒæ­¥ã€æ–°å¢åŠå˜åŒ–åŒæ­¥ã€ç‰¹æ®Šæƒ…å†µ

Ã˜ å…¨é‡è¡¨ï¼šå­˜å‚¨å®Œæ•´çš„æ•°æ®ã€‚

Ã˜ å¢é‡è¡¨ï¼šå­˜å‚¨æ–°å¢åŠ çš„æ•°æ®ã€‚

Ã˜ æ–°å¢åŠå˜åŒ–è¡¨ï¼šå­˜å‚¨æ–°å¢åŠ çš„æ•°æ®å’Œå˜åŒ–çš„æ•°æ®ã€‚

Ã˜ ç‰¹æ®Šè¡¨ï¼šåªéœ€è¦å­˜å‚¨ä¸€æ¬¡ã€‚

##### ä¸šåŠ¡æ•°æ®é¦–æ—¥åŒæ­¥è„šæœ¬

1ï¼‰è„šæœ¬ç¼–å†™

ï¼ˆ1ï¼‰åœ¨/home/dsjprs/binç›®å½•ä¸‹åˆ›å»º

```shell
[atguigu@hadoop102 bin]$ vim mysql_to_hdfs_init.sh
```

æ·»åŠ å¦‚ä¸‹å†…å®¹ï¼š

```shell
#! /bin/bash

APP=gmall
sqoop=/opt/module/sqoop/bin/sqoop

if [ -n "$2" ] ;then
   do_date=$2
else 
   echo "è¯·ä¼ å…¥æ—¥æœŸå‚æ•°"
   exit
fi 
import_data(){
$sqoop import \
--connect jdbc:mysql://node01:3306/$APP \
--username root \
--password 000000 \
--target-dir /origin_data/$APP/db/$1/$do_date \
--delete-target-dir \
--query "$2 where \$CONDITIONS" \
--num-mappers 1 \
--fields-terminated-by '\t' \
--compress \
--compression-codec lzop \
--null-string '\\N' \
--null-non-string '\\N'

hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /origin_data/$APP/db/$1/$do_date
}
import_order_info(){
  import_data order_info "select
                            id, 
                            total_amount, 
                            order_status, 
                            user_id, 
                            payment_way,
                            delivery_address,
                            out_trade_no, 
                            create_time, 
                            operate_time,
                            expire_time,
                            tracking_no,
                            province_id,
                            activity_reduce_amount,
                            coupon_reduce_amount,                            
                            original_total_amount,
                            feight_fee,
                            feight_fee_reduce      
                        from order_info"
}

import_coupon_use(){
  import_data coupon_use "select
                          id,
                          coupon_id,
                          user_id,
                          order_id,
                          coupon_status,
                          get_time,
                          using_time,
                          used_time,
                          expire_time
                        from coupon_use"
}
...............................
case $1 in
  "order_info")
     import_order_info
;;
  "base_category1")
     import_base_category1
;;
  "base_category2")
     import_base_category2
;;
  "base_category3")
     import_base_category3
;;
  "order_detail")
     import_order_detail
;;
  "sku_info")
     import_sku_info
;;
  "user_info")
     import_user_info
;;
  "payment_info")
     import_payment_info
;;
  "base_province")
     import_base_province
;;
  "base_region")
     import_base_region
;;
  "base_trademark")
     import_base_trademark
;;
  "activity_info")
      import_activity_info
;;
  "cart_info")
      import_cart_info
;;
  "comment_info")
      import_comment_info
;;
  "coupon_info")
      import_coupon_info
;;
  "coupon_use")
      import_coupon_use
;;
  "favor_info")
      import_favor_info
;;
  "order_refund_info")
      import_order_refund_info
;;
  "order_status_log")
      import_order_status_log
;;
  "spu_info")
      import_spu_info
;;
  "activity_rule")
      import_activity_rule
;;
  "base_dic")
      import_base_dic
;;
  "order_detail_activity")
      import_order_detail_activity
;;
  "order_detail_coupon")
      import_order_detail_coupon
;;
  "refund_payment")
      import_refund_payment
;;
  "sku_attr_value")
      import_sku_attr_value
;;
  "sku_sale_attr_value")
      import_sku_sale_attr_value
;;
  "all")
;;
esac
```

è¯´æ˜1ï¼š

[ -n å˜é‡å€¼ ] åˆ¤æ–­å˜é‡çš„å€¼ï¼Œæ˜¯å¦ä¸ºç©º

-- å˜é‡çš„å€¼ï¼Œéç©ºï¼Œè¿”å›true

-- å˜é‡çš„å€¼ï¼Œä¸ºç©ºï¼Œè¿”å›false

è¯´æ˜2ï¼š

æŸ¥çœ‹dateå‘½ä»¤çš„ä½¿ç”¨ï¼Œ

```shell
[dsjprs@node01 ~]$ date --help
```

ï¼ˆ2ï¼‰å¢åŠ è„šæœ¬æ‰§è¡Œæƒé™

```shell
[dsjrs@node01 bin]$ chmod +x mysql_to_hdfs_init.sh
```

2ï¼‰è„šæœ¬ä½¿ç”¨

```shell
[dsjprs@node01 bin]$ mysql_to_hdfs_init.sh all 2020-06-14
```

##### ä¸šåŠ¡æ•°æ®æ¯æ—¥åŒæ­¥è„šæœ¬

```shell
#! /bin/bash

APP=gmall
sqoop=/opt/module/sqoop/bin/sqoop

if [ -n "$2" ] ;then
    do_date=$2
else
    do_date=`date -d '-1 day' +%F`
fi

import_data(){
$sqoop import \
--connect jdbc:mysql://hadoop102:3306/$APP \
--username root \
--password 000000 \
--target-dir /origin_data/$APP/db/$1/$do_date \
--delete-target-dir \
--query "$2 and  \$CONDITIONS" \
--num-mappers 1 \
--fields-terminated-by '\t' \
--compress \
--compression-codec lzop \
--null-string '\\N' \
--null-non-string '\\N'

hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /origin_data/$APP/db/$1/$do_date
}

import_order_info(){
  import_data order_info "select
                            id, 
                            total_amount, 
                            order_status, 
                            user_id, 
                            payment_way,
                            delivery_address,
                            out_trade_no, 
                            create_time, 
                            operate_time,
                            expire_time,
                            tracking_no,
                            province_id,
                            activity_reduce_amount,
                            coupon_reduce_amount,                            
                            original_total_amount,
                            feight_fee,
                            feight_fee_reduce      
                        from order_info
                        where (date_format(create_time,'%Y-%m-%d')='$do_date' 
                        or date_format(operate_time,'%Y-%m-%d')='$do_date')"
}

import_coupon_use(){
  import_data coupon_use "select
                          id,
                          coupon_id,
                          user_id,
                          order_id,
                          coupon_status,
                          get_time,
                          using_time,
                          used_time,
                          expire_time
                        from coupon_use
                        where (date_format(get_time,'%Y-%m-%d')='$do_date'
                        or date_format(using_time,'%Y-%m-%d')='$do_date'
                        or date_format(used_time,'%Y-%m-%d')='$do_date'
                        or date_format(expire_time,'%Y-%m-%d')='$do_date')"
}

import_order_status_log(){
  import_data order_status_log "select
                                  id,
                                  order_id,
                                  order_status,
                                  operate_time
                                from order_status_log
                                where date_format(operate_time,'%Y-%m-%d')='$do_date'"
}

import_user_info(){
  import_data "user_info" "select 
                            id,
                            login_name,
                            nick_name,
                            name,
                            phone_num,
                            email,
                            user_level, 
                            birthday,
                            gender,
                            create_time,
                            operate_time
                          from user_info 
                          where (DATE_FORMAT(create_time,'%Y-%m-%d')='$do_date' 
                          or DATE_FORMAT(operate_time,'%Y-%m-%d')='$do_date')"
}

import_order_detail(){
  import_data order_detail "select 
                              id,
                              order_id, 
                              sku_id,
                              sku_name,
                              order_price,
                              sku_num, 
                              create_time,
                              source_type,
                              source_id,
                              split_total_amount,
                              split_activity_amount,
                              split_coupon_amount
                            from order_detail 
                            where DATE_FORMAT(create_time,'%Y-%m-%d')='$do_date'"
}

import_payment_info(){
  import_data "payment_info"  "select 
                                id,  
                                out_trade_no, 
                                order_id, 
                                user_id, 
                                payment_type, 
                                trade_no, 
                                total_amount,  
                                subject, 
                                payment_status,
                                create_time,
                                callback_time 
                              from payment_info 
                              where (DATE_FORMAT(create_time,'%Y-%m-%d')='$do_date' 
                              or DATE_FORMAT(callback_time,'%Y-%m-%d')='$do_date')"
}

import_comment_info(){
  import_data comment_info "select
                              id,
                              user_id,
                              sku_id,
                              spu_id,
                              order_id,
                              appraise,
                              create_time
                            from comment_info
                            where date_format(create_time,'%Y-%m-%d')='$do_date'"
}

import_order_refund_info(){
  import_data order_refund_info "select
                                id,
                                user_id,
                                order_id,
                                sku_id,
                                refund_type,
                                refund_num,
                                refund_amount,
                                refund_reason_type,
                                refund_status,
                                create_time
                              from order_refund_info
                              where date_format(create_time,'%Y-%m-%d')='$do_date'"
}

import_sku_info(){
  import_data sku_info "select 
                          id,
                          spu_id,
                          price,
                          sku_name,
                          sku_desc,
                          weight,
                          tm_id,
                          category3_id,
                          is_sale,
                          create_time
                        from sku_info where 1=1"
}

import_base_category1(){
  import_data "base_category1" "select 
                                  id,
                                  name 
                                from base_category1 where 1=1"
}

import_base_category2(){
  import_data "base_category2" "select
                                  id,
                                  name,
                                  category1_id 
                                from base_category2 where 1=1"
}

import_base_category3(){
  import_data "base_category3" "select
                                  id,
                                  name,
                                  category2_id
                                from base_category3 where 1=1"
}

import_base_province(){
  import_data base_province "select
                              id,
                              name,
                              region_id,
                              area_code,
                              iso_code,
                              iso_3166_2
                            from base_province
                            where 1=1"
}

import_base_region(){
  import_data base_region "select
                              id,
                              region_name
                            from base_region
                            where 1=1"
}

import_base_trademark(){
  import_data base_trademark "select
                                id,
                                tm_name
                              from base_trademark
                              where 1=1"
}

import_spu_info(){
  import_data spu_info "select
                            id,
                            spu_name,
                            category3_id,
                            tm_id
                          from spu_info
                          where 1=1"
}

import_favor_info(){
  import_data favor_info "select
                          id,
                          user_id,
                          sku_id,
                          spu_id,
                          is_cancel,
                          create_time,
                          cancel_time
                        from favor_info
                        where 1=1"
}

import_cart_info(){
  import_data cart_info "select
                        id,
                        user_id,
                        sku_id,
                        cart_price,
                        sku_num,
                        sku_name,
                        create_time,
                        operate_time,
                        is_ordered,
                        order_time,
                        source_type,
                        source_id
                      from cart_info
                      where 1=1"
}

import_coupon_info(){
  import_data coupon_info "select
                          id,
                          coupon_name,
                          coupon_type,
                          condition_amount,
                          condition_num,
                          activity_id,
                          benefit_amount,
                          benefit_discount,
                          create_time,
                          range_type,
                          limit_num,
                          taken_count,
                          start_time,
                          end_time,
                          operate_time,
                          expire_time
                        from coupon_info
                        where 1=1"
}

import_activity_info(){
  import_data activity_info "select
                              id,
                              activity_name,
                              activity_type,
                              start_time,
                              end_time,
                              create_time
                            from activity_info
                            where 1=1"
}

import_activity_rule(){
    import_data activity_rule "select
                                    id,
                                    activity_id,
                                    activity_type,
                                    condition_amount,
                                    condition_num,
                                    benefit_amount,
                                    benefit_discount,
                                    benefit_level
                                from activity_rule
                                where 1=1"
}

import_base_dic(){
    import_data base_dic "select
                            dic_code,
                            dic_name,
                            parent_code,
                            create_time,
                            operate_time
                          from base_dic
                          where 1=1"
}


import_order_detail_activity(){
    import_data order_detail_activity "select
                                                                id,
                                                                order_id,
                                                                order_detail_id,
                                                                activity_id,
                                                                activity_rule_id,
                                                                sku_id,
                                                                create_time
                                                            from order_detail_activity
                                                            where date_format(create_time,'%Y-%m-%d')='$do_date'"
}


import_order_detail_coupon(){
    import_data order_detail_coupon "select
                                                                id,
								                                                order_id,
                                                                order_detail_id,
                                                                coupon_id,
                                                                coupon_use_id,
                                                                sku_id,
                                                                create_time
                                                            from order_detail_coupon
                                                            where date_format(create_time,'%Y-%m-%d')='$do_date'"
}


import_refund_payment(){
    import_data refund_payment "select
                                                        id,
                                                        out_trade_no,
                                                        order_id,
                                                        sku_id,
                                                        payment_type,
                                                        trade_no,
                                                        total_amount,
                                                        subject,
                                                        refund_status,
                                                        create_time,
                                                        callback_time
                                                    from refund_payment
                                                    where (DATE_FORMAT(create_time,'%Y-%m-%d')='$do_date' 
                                                    or DATE_FORMAT(callback_time,'%Y-%m-%d')='$do_date')"                                                    

}

import_sku_attr_value(){
    import_data sku_attr_value "select
                                                    id,
                                                    attr_id,
                                                    value_id,
                                                    sku_id,
                                                    attr_name,
                                                    value_name
                                                from sku_attr_value
                                                where 1=1"
}


import_sku_sale_attr_value(){
    import_data sku_sale_attr_value "select
                                                            id,
                                                            sku_id,
                                                            spu_id,
                                                            sale_attr_value_id,
                                                            sale_attr_id,
                                                            sale_attr_name,
                                                            sale_attr_value_name
                                                        from sku_sale_attr_value
                                                        where 1=1"
}
case $1 in
  "order_info")
     import_order_info
;;
  "base_category1")
     import_base_category1
;;
  "base_category2")
     import_base_category2
;;
  "base_category3")
     import_base_category3
;;
  "order_detail")
     import_order_detail
;;
  "sku_info")
     import_sku_info
;;
  "user_info")
     import_user_info
;;
  "payment_info")
     import_payment_info
;;
  "base_province")
     import_base_province
;;
  "activity_info")
      import_activity_info
;;
  "cart_info")
      import_cart_info
;;
  "comment_info")
      import_comment_info
;;
  "coupon_info")
      import_coupon_info
;;
  "coupon_use")
      import_coupon_use
;;
  "favor_info")
      import_favor_info
;;
  "order_refund_info")
      import_order_refund_info
;;
  "order_status_log")
      import_order_status_log
;;
  "spu_info")
      import_spu_info
;;
  "activity_rule")
      import_activity_rule
;;
  "base_dic")
      import_base_dic
;;
  "order_detail_activity")
      import_order_detail_activity
;;
  "order_detail_coupon")
      import_order_detail_coupon
;;
  "refund_payment")
      import_refund_payment
;;
  "sku_attr_value")
      import_sku_attr_value
;;
  "sku_sale_attr_value")
      import_sku_sale_attr_value
;;
"all")
   import_base_category1
   import_base_category2
   import_base_category3
   import_order_info
   import_order_detail
   import_sku_info
   import_user_info
   import_payment_info
   import_base_trademark
   import_activity_info
   import_cart_info
   import_comment_info
   import_coupon_use
   import_coupon_info
   import_favor_info
   import_order_refund_info
   import_order_status_log
   import_spu_info
   import_activity_rule
   import_base_dic
   import_order_detail_activity
   import_order_detail_coupon
   import_refund_payment
   import_sku_attr_value
   import_sku_sale_attr_value
;;
esac
```

### å®‰è£…Hive

#### Hiveå®‰è£…éƒ¨ç½²

ï¼ˆ1ï¼‰æŠŠapache-hive-3.1.2-bin.tar.gzä¸Šä¼ åˆ°Linuxçš„/opt/softwareç›®å½•ä¸‹

ï¼ˆ2ï¼‰è§£å‹apache-hive-3.1.2-bin.tar.gzåˆ°/opt/module/ç›®å½•ä¸‹é¢

```shell
[dsjprs@node01 software]$ tar -zxvf /opt/software/apache-hive-3.1.2-bin.tar.gz -C /opt/module/
```

ï¼ˆ3ï¼‰ä¿®æ”¹apache-hive-3.1.2-bin.tar.gzçš„åç§°ä¸ºhive

```shell
[dsjprs@node01 software]$ mv /opt/module/apache-hive-3.1.2-bin/ /opt/module/hive
```

ï¼ˆ4ï¼‰ä¿®æ”¹/etc/profile.d/my_env.shï¼Œæ·»åŠ ç¯å¢ƒå˜é‡

```shell
[dsjprs@node01 software]$ sudo vim /etc/profile.d/my_env.sh
```

```shell
# æ·»åŠ å†…å®¹
# HIVE_HOME
export HIVE_HOME=/opt/module/hive
export PATH=$PATH:$HIVE_HOME/bin
```

```shell
# sourceä¸€ä¸‹ /etc/profile.d/my_env.shæ–‡ä»¶ï¼Œä½¿ç¯å¢ƒå˜é‡ç”Ÿæ•ˆ
[djsprs@node01 software]$ source /etc/profile.d/my_env.sh
```

ï¼ˆ5ï¼‰è§£å†³æ—¥å¿—JaråŒ…å†²çªï¼Œè¿›å…¥/opt/module/hive/libç›®å½•

```shell
[dsjprs@node01 lib]$ mv log4j-slf4j-impl-2.10.0.jar log4j-slf4j-impl-2.10.0.jar.bak
```

#### Hiveå…ƒæ•°æ®é…ç½®åˆ°MySQL

ï¼ˆ1ï¼‰æ‹·è´é©±åŠ¨

å°†MySQLçš„JDBCé©±åŠ¨æ‹·è´åˆ°Hiveçš„libç›®å½•ä¸‹

```shell
[dsjprs@node01 lib]$ cp /opt/software/mysql-connector-java-5.1.27.jar /opt/module/hive/lib/
```

ï¼ˆ2ï¼‰é…ç½®MetaStoreåˆ°MySQL

ï¼ˆ1ï¼‰åœ¨$HIVE_HOME/confç›®å½•ä¸‹æ–°å»ºhive-site.xmlæ–‡ä»¶

```shell
[dsjprs@node01 conf]$ vim hive-site.xml
```

æ·»åŠ å¦‚ä¸‹å†…å®¹

```xml
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://ndoe01:3306/metastore?		
        useSSL=false</value>
    </property>

    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
    </property>

    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>root</value>
    </property>

    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>000000</value>
    </property>

    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>/user/hive/warehouse</value>
    </property>

    <property>
        <name>hive.metastore.schema.verification</name>
        <value>false</value>
    </property>

    <property>
    <name>hive.server2.thrift.port</name>
    <value>10000</value>
    </property>

    <property>
        <name>hive.server2.thrift.bind.host</name>
        <value>hadoop102</value>
    </property>

    <property>
        <name>hive.metastore.event.db.notification.api.auth</name>
        <value>false</value>
    </property>
    
    <property>
        <name>hive.cli.print.header</name>
        <value>true</value>
    </property>

    <property>
        <name>hive.cli.print.current.db</name>
        <value>true</value>
    </property>
</configuration>
```

ï¼ˆ2ï¼‰ä¿®æ”¹hive-env.sh

```shell
cd conf/
mv hive-env.sh.template hive-env.sh
vi hive-env.sh
```

æ·»åŠ ï¼š

```shell
HADOOP_HOME=/opt/hadoop
# Hive Configuration Directory can be controlled by:
# export HIVE_CONF_DIR=
export HIVE_CONF_DIR=/opt/hive/conf
# Folder containing extra ibraries required for hive compilation/execution can be controlled by:
# export HIVE_AUX_JARS_PATH=
export HIVE_AUX_JARS_PATH=/opt/hive/lib
```

ï¼ˆ3ï¼‰åœ¨2ä¸ªworker å‰¯èŠ‚ç‚¹hive-site.xmlæ–‡ä»¶æ·»åŠ ï¼š

```xml
<property>
    <name>hive.metastore.uris</name>
    <value>thrift://hadoop001:9083</value>
  </property>
```



#### å¯åŠ¨Hive

1ï¼‰åˆå§‹åŒ–å…ƒæ•°æ®åº“

ï¼ˆ1ï¼‰ç™»é™†MySQL

```shell
[dsjprs@node01 conf]$ mysql -uroot -p000000
```

ï¼ˆ2ï¼‰æ–°å»ºHiveå…ƒæ•°æ®åº“

```mysql
mysql> create database metastore;

mysql> quit;
```

ï¼ˆ3ï¼‰åˆå§‹åŒ–Hiveå…ƒæ•°æ®åº“

```shell
[dsjprs@node01 conf]$ schematool -initSchema -dbType mysql -verbose
```

#### å¯åŠ¨Hiveå®¢æˆ·ç«¯

ï¼ˆ1ï¼‰å¯åŠ¨Hiveå®¢æˆ·ç«¯

```shell
[dsjprs@node01 hive]$ bin/hive
```

ï¼ˆ2ï¼‰æŸ¥çœ‹ä¸€ä¸‹æ•°æ®åº“

```mysql
hive (default)> show databases;
OK
database_name
default
```

### Hive on Spark

Hiveå¼•æ“ç®€ä»‹

Hiveå¼•æ“åŒ…æ‹¬ï¼šé»˜è®¤MRã€tezã€spark

Hive on Sparkï¼šHiveæ—¢ä½œä¸ºå­˜å‚¨å…ƒæ•°æ®åˆè´Ÿè´£SQLçš„è§£æä¼˜åŒ–ï¼Œè¯­æ³•æ˜¯HQLè¯­æ³•ï¼Œæ‰§è¡Œå¼•æ“å˜æˆäº†Sparkï¼ŒSparkè´Ÿè´£é‡‡ç”¨RDDæ‰§è¡Œã€‚

Spark on Hive : Hiveåªä½œä¸ºå­˜å‚¨å…ƒæ•°æ®ï¼ŒSparkè´Ÿè´£SQLè§£æä¼˜åŒ–ï¼Œè¯­æ³•æ˜¯Spark SQLè¯­æ³•ï¼ŒSparkè´Ÿè´£é‡‡ç”¨RDDæ‰§è¡Œã€‚



**Hive** **on** **Sparké…ç½®**

#### 1ï¼‰å…¼å®¹æ€§è¯´æ˜

æ³¨æ„ï¼šå®˜ç½‘ä¸‹è½½çš„Hive3.1.2å’ŒSpark3.0.0é»˜è®¤æ˜¯ä¸å…¼å®¹çš„ã€‚å› ä¸ºHive3.1.2æ”¯æŒçš„Sparkç‰ˆæœ¬æ˜¯2.4.5ï¼Œæ‰€ä»¥éœ€è¦æˆ‘ä»¬é‡æ–°ç¼–è¯‘Hive3.1.2ç‰ˆæœ¬ã€‚

ç¼–è¯‘æ­¥éª¤ï¼šå®˜ç½‘ä¸‹è½½Hive3.1.2æºç ï¼Œä¿®æ”¹pomæ–‡ä»¶ä¸­å¼•ç”¨çš„Sparkç‰ˆæœ¬ä¸º3.0.0ï¼Œå¦‚æœç¼–è¯‘é€šè¿‡ï¼Œç›´æ¥æ‰“åŒ…è·å–jaråŒ…ã€‚å¦‚æœæŠ¥é”™ï¼Œå°±æ ¹æ®æç¤ºï¼Œä¿®æ”¹ç›¸å…³æ–¹æ³•ï¼Œç›´åˆ°ä¸æŠ¥é”™ï¼Œæ‰“åŒ…è·å–jaråŒ…ã€‚



#### 2ï¼‰åœ¨Hiveæ‰€åœ¨èŠ‚ç‚¹éƒ¨ç½²Spark

å¦‚æœä¹‹å‰å·²ç»éƒ¨ç½²äº†Sparkï¼Œåˆ™è¯¥æ­¥éª¤å¯ä»¥è·³è¿‡ï¼Œä½†è¦æ£€æŸ¥SPARK_HOMEçš„ç¯å¢ƒå˜é‡é…ç½®æ˜¯å¦æ­£ç¡®ã€‚

##### ï¼ˆ1ï¼‰Sparkå®˜ç½‘ä¸‹è½½jaråŒ…åœ°å€ï¼š

http://spark.apache.org/downloads.html

##### ï¼ˆ2ï¼‰ä¸Šä¼ å¹¶è§£å‹è§£å‹spark-3.0.0-bin-hadoop3.2.tgz

```shell
[dsjprs@node01 software]$ tar -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module/

[dsjprs@node01 software]$ mv /opt/module/spark-3.0.0-bin-hadoop3.2 /opt/module/spark
```

##### ï¼ˆ3ï¼‰é…ç½®SPARK_HOMEç¯å¢ƒå˜é‡

```shell
[dsjprs@node01 software]$ sudo vim /etc/profile.d/my_env.sh
```

æ·»åŠ å¦‚ä¸‹å†…å®¹

```shell
# SPARK_HOME
export SPARK_HOME=/opt/module/spark
export PATH=$PATH:$SPARK_HOME/bin
```

source ä½¿å…¶ç”Ÿæ•ˆ

```shell
[dsjprs@node01 software]$ source /etc/profile.d/my_env.sh
```

##### ï¼“ï¼‰åœ¨hiveä¸­åˆ›å»ºsparké…ç½®æ–‡ä»¶

```shell
[dsjprs@node01 software]$ vim /opt/module/hive/conf/spark-defaults.conf
```

æ·»åŠ å¦‚ä¸‹å†…å®¹ï¼ˆåœ¨æ‰§è¡Œä»»åŠ¡æ—¶ï¼Œä¼šæ ¹æ®å¦‚ä¸‹å‚æ•°æ‰§è¡Œï¼‰

```shell
spark.master                yarn
spark.eventLog.enabled          true
spark.eventLog.dir             hdfs://hadoop102:8020/spark-history
spark.executor.memory           1g
spark.driver.memory					  1g
```

åœ¨HDFSåˆ›å»ºå¦‚ä¸‹è·¯å¾„ï¼Œç”¨äºå­˜å‚¨å†å²æ—¥å¿—

```shell
[dsjprs@node01 software]$ hadoop fs -mkdir /spark-history
```

##### ï¼”ï¼‰å‘HDFSä¸Šä¼ Sparkçº¯å‡€ç‰ˆjaråŒ…

è¯´æ˜1ï¼šç”±äºSpark3.0.0éçº¯å‡€ç‰ˆé»˜è®¤æ”¯æŒçš„æ˜¯hive2.3.7ç‰ˆæœ¬ï¼Œç›´æ¥ä½¿ç”¨ä¼šå’Œå®‰è£…çš„Hive3.1.2å‡ºç°å…¼å®¹æ€§é—®é¢˜ã€‚æ‰€ä»¥é‡‡ç”¨Sparkçº¯å‡€ç‰ˆjaråŒ…ï¼Œä¸åŒ…å«hadoopå’Œhiveç›¸å…³ä¾èµ–ï¼Œé¿å…å†²çªã€‚



è¯´æ˜2ï¼šHiveä»»åŠ¡æœ€ç»ˆç”±Sparkæ¥æ‰§è¡Œï¼ŒSparkä»»åŠ¡èµ„æºåˆ†é…ç”±Yarnæ¥è°ƒåº¦ï¼Œè¯¥ä»»åŠ¡æœ‰å¯èƒ½è¢«åˆ†é…åˆ°é›†ç¾¤çš„ä»»ä½•ä¸€ä¸ªèŠ‚ç‚¹ã€‚æ‰€ä»¥éœ€è¦å°†Sparkçš„ä¾èµ–ä¸Šä¼ åˆ°HDFSé›†ç¾¤è·¯å¾„ï¼Œè¿™æ ·é›†ç¾¤ä¸­ä»»ä½•ä¸€ä¸ªèŠ‚ç‚¹éƒ½èƒ½è·å–åˆ°ã€‚

ï¼ˆ1ï¼‰ä¸Šä¼ å¹¶è§£å‹spark-3.0.0-bin-without-hadoop.tgz

```shell
[dsjprs@node01 software]$ tar -zxvf /opt/software/spark-3.0.0-bin-without-hadoop.tgz
```

ï¼ˆ2ï¼‰ä¸Šä¼ Sparkçº¯å‡€ç‰ˆjaråŒ…åˆ°HDFS

```shell
[dsjprs@node01 software]$ hadoop fs -mkdir /spark-jars
[dsjprs@node01 software]$ hadoop fs -put spark-3.0.0-bin-without-hadoop/jars/* /spark-jars
```

##### ï¼•ï¼‰ä¿®æ”¹hive-site.xmlæ–‡ä»¶

```shell
[dsjprs@node01 ~]$ vim /opt/module/hive/conf/hive-site.xml
```

æ·»åŠ å¦‚ä¸‹å†…å®¹

```xml
<!--Sparkä¾èµ–ä½ç½®ï¼ˆæ³¨æ„ï¼šç«¯å£å·8020å¿…é¡»å’Œnamenodeçš„ç«¯å£å·ä¸€è‡´ï¼‰-->

<property>
  <name>spark.yarn.jars</name>
  <value>hdfs://node01:8020/spark-jars/*</value>
</property>
<!--Hiveæ‰§è¡Œå¼•æ“-->
<property>
  <name>hive.execution.engine</name>
  <value>spark</value>
</property>
```

#### Hive on Sparkæµ‹è¯•

ï¼ˆ1ï¼‰å¯åŠ¨hiveå®¢æˆ·ç«¯

```shell
[dsjprs@node01 hive]$ bin/hive
```

ï¼ˆ2ï¼‰åˆ›å»ºä¸€å¼ æµ‹è¯•è¡¨

```mysql
hive (default)> create table student(id int, name string);
```

ï¼ˆ3ï¼‰é€šè¿‡insertæµ‹è¯•æ•ˆæœ

```mysql
hive (default)> insert into table student values(1,'abc');
```

è‹¥ç»“æœå¦‚ä¸‹ï¼Œåˆ™è¯´æ˜é…ç½®æˆåŠŸ

![image-20211023142312850](Images/image-20211023142312850.png)



#### Yarné…ç½®

#### å¢åŠ ApplicationMasterèµ„æºæ¯”ä¾‹

å®¹é‡è°ƒåº¦å™¨å¯¹æ¯ä¸ªèµ„æºé˜Ÿåˆ—ä¸­åŒæ—¶è¿è¡Œçš„Application Masterå ç”¨çš„èµ„æºè¿›è¡Œäº†é™åˆ¶ï¼Œè¯¥é™åˆ¶é€šè¿‡yarn.scheduler.capacity.maximum-am-resource-percentå‚æ•°å®ç°ï¼Œå…¶é»˜è®¤å€¼æ˜¯0.1ï¼Œè¡¨ç¤ºæ¯ä¸ªèµ„æºé˜Ÿåˆ—ä¸ŠApplication Masteræœ€å¤šå¯ä½¿ç”¨çš„èµ„æºä¸ºè¯¥é˜Ÿåˆ—æ€»èµ„æºçš„10%ï¼Œç›®çš„æ˜¯é˜²æ­¢å¤§éƒ¨åˆ†èµ„æºéƒ½è¢«Application Masterå ç”¨ï¼Œè€Œå¯¼è‡´Map/Reduce Taskæ— æ³•æ‰§è¡Œã€‚



ç”Ÿäº§ç¯å¢ƒè¯¥å‚æ•°å¯ä½¿ç”¨é»˜è®¤å€¼ã€‚ä½†å­¦ä¹ ç¯å¢ƒï¼Œé›†ç¾¤èµ„æºæ€»æ•°å¾ˆå°‘ï¼Œå¦‚æœåªåˆ†é…10%çš„èµ„æºç»™Application Masterï¼Œåˆ™å¯èƒ½å‡ºç°ï¼ŒåŒä¸€æ—¶åˆ»åªèƒ½è¿è¡Œä¸€ä¸ªJobçš„æƒ…å†µï¼Œå› ä¸ºä¸€ä¸ªApplication Masterä½¿ç”¨çš„èµ„æºå°±å¯èƒ½å·²ç»è¾¾åˆ°10%çš„ä¸Šé™äº†ã€‚æ•…æ­¤å¤„å¯å°†è¯¥å€¼é€‚å½“è°ƒå¤§ã€‚

ï¼ˆ1ï¼‰åœ¨hadoop102çš„/opt/module/hadoop-3.1.3/etc/hadoop/capacity-scheduler.xmlæ–‡ä»¶ä¸­ä¿®æ”¹å¦‚ä¸‹å‚æ•°å€¼

```shell
[atguigu@hadoop102 hadoop]$ vim capacity-scheduler.xml
```

```xml
<property>
  <name>yarn.scheduler.capacity.maximum-am-resource-percent</name>
  <value>0.8</value>
</property
```

ï¼ˆ2ï¼‰åˆ†å‘capacity-scheduler.xmlé…ç½®æ–‡ä»¶

```shell
[dsjprs@node01 hadoop]$ xsync capacity-scheduler.xml
```

ï¼ˆ3ï¼‰å…³é—­æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡ï¼Œé‡æ–°å¯åŠ¨yarné›†ç¾¤

```shell
[dsjprs@node02 hadoop-3.1.3]$ sbin/stop-yarn.sh

[dsjprs@node02 hadoop-3.1.3]$ sbin/start-yarn.sh
```

### å®‰è£… Spark

#### Sparkå®‰è£…ï¼ˆä¸‰å°æœºå™¨å¯åŒæ­¥è¿›è¡Œï¼‰

1. ä¸‹è½½spark-2.1.0-bin-hadoop2.7.tgzï¼Œæ”¾åˆ°optä¸‹è§£å‹ã€‚
2. å°†sparkç¯å¢ƒå˜é‡é…ç½®åˆ°/etc/profile.d/my_env.shä¸­

```shell
export SPARK_HOME=/opt/spark-2.1.0-bin-hadoop2.7
export PATH=$JAVA_HOME/bin:$SPARK_HOME/bin:$PATH
```

3. è¿›å…¥spark-2.1.0-bin-hadoop2.7/confå¤åˆ¶spark-env.sh.templateå¹¶é‡å‘½åä¸ºspark-env.sh

```shell
cp spark-env.sh.template spark-env.sh
```

ç¼–è¾‘spark-env.shæ–‡ä»¶ï¼Œæ·»åŠ ä»¥ä¸‹å†…å®¹

```shell
export JAVA_HOME=/opt/jdk
export SCALA_HOME=/home/sword/DC/scala
export SPARK_MASTER_IP=192.168.241.132
export SPARK_WORKER_MEMORY=8g
export SPARK_WORKER_CORES=4
export SPARK_EXECUTOR_MEMORY=4g
export HADOOP_HOME=/opt/hadoop-2.7.7/
export HADOOP_CONF_DIR=/opt/hadoop-2.7.7/etc/hadoop
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/jdk/jre/lib/amd64
export SPARK_WORKER_INSTANCES=2
export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=node01:2181,node02:2181,node03:2181 -Dspark.deploy.zookeeper.dir=/home/sword/DC/SDOZK"
```

4. æŠŠslaves.templateæ‹·è´ä¸ºslaves,å¹¶ç¼–è¾‘ slavesæ–‡ä»¶

```shell
cp slaves.template slaves
```

5. é…ç½®spark-defaults.conf ï¼ˆæŸ¥çœ‹å†å²è®°å½•ï¼Œéœ€è¦å…ˆæ‰§è¡Œstart-history-server.shï¼‰
   å°†spark-defaults.conf.template æ”¹ä¸º spark-defaults.conf
   mvspark-defaults.conf.template spark-defaults.conf
   ä¿®æ”¹ä»¥ä¸‹ä¸¤é¡¹ï¼š

   ```shell
   spark.eventLog.enabled           true
   spark.eventLog.dir               hdfs://master:9000/directory
   ```

6. hadoop fs -mkdir /directory åœ¨hdfsä¸Šåˆ›å»ºdirectory æ–‡ä»¶å¤¹ï¼š

   ```shell
   export SPARK_HISTORY_OPTS="-Dspark.history.ui.port=4000 -Dspark.history.retainedApplications=3 -Dspark.history.fs.logDirectory=hdfs://master:9000/directory"
   ```

7. é…ç½®log4j.propertiesï¼ˆå‡å°‘è¾“å‡ºçš„æ—¥å¿—ä¿¡æ¯ï¼Œå¯ä¸æ”¹ï¼‰
   å°†log4j.properties.template æ”¹ä¸º log4j.properties
   mv log4j.properties.template log4j.properties

   ```shell
   vi log4j.properties æŠŠINFOæ”¹ä¸ºWARN
   ```

   

ç¼–è¾‘slavesæ–‡ä»¶ï¼Œæ·»åŠ ä»¥ä¸‹å†…å®¹ï¼ˆå¤šæœºæ·»åŠ å¤šä¸ªï¼‰

```shell
node01
node02
node03
```

### Hadoop Spark on Yarn Zookeeper HA å®Œå…¨åˆ†å¸ƒå¼

#### **è§’è‰²åˆ†é…**

Zookeeperé›†ç¾¤åˆ†é…ä¸‰å°ã€‚

Hadoopåˆ†é…éœ€è¦åˆ†å¼€è¯´ï¼š

é¦–å…ˆæ—¶HDFSï¼šä¸¤ä¸ªä¸»èŠ‚ç‚¹ï¼Œä¸‰ä¸ªä»èŠ‚ç‚¹ï¼Œ5å°ã€‚

JNé›†ç¾¤ï¼šä¸‰å°

Yarné›†ç¾¤ï¼šä¸¤ä¸ªä¸»èŠ‚ç‚¹ï¼Œä¸‰ä¸ªä»èŠ‚ç‚¹ï¼Œ5å°ã€‚

Sparké›†ç¾¤åˆ†é…ä¸‰å°ã€‚

 å°†ä»¥ä¸Šå„ä¸ªé›†ç¾¤çš„èŠ‚ç‚¹åˆå¹¶ï¼Œå…·ä½“åˆ†é…å¦‚ä¸‹ï¼š

  spark01ï¼šZookeeperã€ResourceManager(active)ã€NameNode(active)ã€‚

  spark02ï¼šZookeeperã€NameNode(standby)ã€‚

  spark03ï¼šZookeeperã€ResourceManager(standby)ã€‚

  spark04ï¼šJournalNodeã€DataNodeã€NodeManagerã€Sparkã€‚

  spark05ï¼šJournalNodeã€DataNodeã€NodeManagerã€Sparkã€‚

  spark06ï¼šJournalNodeã€DataNodeã€NodeManagerã€Sparkã€‚



#### åˆ›å»ºç¯å¢ƒ /etc/profile.d/my_env.sh é…ç½®JAVA_HOME

```shell
export JAVA_HOME=/usr/share/jdk1.6.0_14
export PATH=$JAVA_HOME/bin:$PATH
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
```

#### SCALA_HOME

```SHELL
export SCALA_HOME=/usr/share/jdk1.6.0_14
export PATH=$SCALA_HOME/bin:$PATH
```

#### Hadoop HA

##### é…ç½®æ–‡ä»¶

ï¼ˆ1ï¼‰hadoop-env.sh

```shell
export JAVA_HOME=/usr/share/jdk1.6.0_14
export HADOOP_CONF_DIR=/home/software/hadoop-2.7.1/etc/hadoop
```

ï¼ˆ2ï¼‰core-site.xml

```xml
<configuration>
<!--ç”¨æ¥æŒ‡å®šhdfsçš„è€å¤§ï¼Œnsä¸ºå›ºå®šå±æ€§åï¼Œæ­¤å€¼å¯ä»¥è‡ªå·±è®¾ç½®ï¼Œä½†æ˜¯åé¢çš„å€¼è¦å’Œæ­¤å€¼å¯¹åº”ï¼Œè¡¨ç¤ºä¸¤ä¸ªnamenode-->
<property>
ã€€ã€€<name>fs.defaultFS</name>
ã€€ã€€<value>hdfs://ns</value>
</property>
<!--ç”¨æ¥æŒ‡å®šhadoopè¿è¡Œæ—¶äº§ç”Ÿæ–‡ä»¶çš„å­˜æ”¾ç›®å½•-->
<property>
ã€€ã€€<name>hadoop.tmp.dir</name>
ã€€ã€€<value>/home/software/hadoop-2.7.1/tmp</value>
</property>
<!--æ‰§è¡Œzookeeperåœ°å€-->
<property>
ã€€ã€€<name>ha.zookeeper.quorum</name>
ã€€ã€€<value>spark01:2181,spark02:2181,spark03:2181</value>
</property>
</configuration>
```

ï¼ˆ3ï¼‰hdfs-site.xml

```xml
<configuration>
<!--æ‰§è¡Œhdfsçš„nameserviceä¸ºns,å’Œcore-site.xmlä¿æŒä¸€è‡´-->
<property>
ã€€ã€€<name>dfs.nameservices</name>
ã€€ã€€<value>ns</value>
</property>
<!--nsä¸‹æœ‰ä¸¤ä¸ªnamenode,åˆ†åˆ«æ˜¯nn1,nn2-->
<property>
ã€€ã€€<name>dfs.ha.namenodes.ns</name>
ã€€ã€€<value>nn1,nn2</value>
</property>
<!--nn1çš„RPCé€šä¿¡åœ°å€-->
<property>
ã€€ã€€<name>dfs.namenode.rpc-address.ns.nn1</name>
ã€€ã€€<value>spnode01:9000</value>
</property>

<!--nn1çš„httpé€šä¿¡åœ°å€-->
<property>
ã€€ã€€<name>dfs.namenode.http-address.ns.nn1</name>
ã€€ã€€<value>spnode01:50070</value>
</property>
<!--nn2çš„RPCé€šä¿¡åœ°å€-->
<property>
ã€€ã€€<name>dfs.namenode.rpc-address.ns.nn2</name>
ã€€ã€€<value>spnode02:9000</value>
</property>
<!--nn2çš„httpé€šä¿¡åœ°å€-->
<property>
ã€€ã€€<name>dfs.namenode.http-address.ns.nn2</name>
ã€€ã€€<value>spnode02:50070</value>
</property>
<!--æŒ‡å®šnamenodeçš„å…ƒæ•°æ®åœ¨JournalNodeä¸Šçš„å­˜æ”¾ä½ç½®,è¿™æ ·ï¼Œnamenode2å¯ä»¥ä»jné›†ç¾¤é‡Œè·å–æœ€æ–°çš„namenodeçš„ä¿¡æ¯ï¼Œè¾¾åˆ°çƒ­å¤‡çš„æ•ˆæœ-->
<property>
ã€€ã€€<name>dfs.namenode.shared.edits.dir</name>
<value>qjournal://spark04:8485;spark05:8485;spark06:8485/ns</value>
</property>
<!--æŒ‡å®šJournalNodeå­˜æ”¾æ•°æ®çš„ä½ç½®-->
<property>
ã€€ã€€<name>dfs.journalnode.edits.dir</name>
ã€€ã€€<value>/home/software/hadoop-2.7.1/journal</value>
</property>
<!--å¼€å¯namenodeæ•…éšœæ—¶è‡ªåŠ¨åˆ‡æ¢-->
<property>
ã€€ã€€<name>dfs.ha.automatic-failover.enabled</name>
ã€€ã€€<value>true</value>
</property>
<!--é…ç½®åˆ‡æ¢çš„å®ç°æ–¹å¼-->
<property>
ã€€ã€€<name>dfs.client.failover.proxy.provider.ns</name>
<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
</property>
<!--é…ç½®éš”ç¦»æœºåˆ¶-->
<property>
ã€€ã€€<name>dfs.ha.fencing.methods</name>
ã€€ã€€<value>sshfence</value>
</property>
<!--é…ç½®éš”ç¦»æœºåˆ¶çš„sshç™»å½•ç§˜é’¥æ‰€åœ¨çš„ä½ç½®-->
<property>
ã€€ã€€<name>dfs.ha.fencing.ssh.private-key-files</name>
ã€€ã€€<value>/root/.ssh/id_rsa</value>
</property>
 
<!--é…ç½®namenodeæ•°æ®å­˜æ”¾çš„ä½ç½®,å¯ä»¥ä¸é…ç½®ï¼Œå¦‚æœä¸é…ç½®ï¼Œé»˜è®¤ç”¨çš„æ˜¯core-site.xmlé‡Œé…ç½®çš„hadoop.tmp.dirçš„è·¯å¾„-->
<property>
ã€€ã€€<name>dfs.namenode.name.dir</name>
ã€€ã€€<value>file:///home/software/hadoop2.7.1/tmp/namenode</value>
</property>

<!--é…ç½®datanodeæ•°æ®å­˜æ”¾çš„ä½ç½®,å¯ä»¥ä¸é…ç½®ï¼Œå¦‚æœä¸é…ç½®ï¼Œé»˜è®¤ç”¨çš„æ˜¯core-site.xmlé‡Œé…ç½®çš„hadoop.tmp.dirçš„è·¯å¾„-->
<property>
ã€€ã€€<name>dfs.datanode.data.dir</name>
ã€€ã€€<value>file:///home/software/hadoop2.7.1/tmp/datanode</value>
</property>
 
<!--é…ç½®blockå‰¯æœ¬æ•°é‡-->
<property>
ã€€ã€€<name>dfs.replication</name>
ã€€ã€€<value>3</value>
</property>

<!--è®¾ç½®hdfsçš„æ“ä½œæƒé™ï¼Œfalseè¡¨ç¤ºä»»ä½•ç”¨æˆ·éƒ½å¯ä»¥åœ¨hdfsä¸Šæ“ä½œæ–‡ä»¶ï¼Œç”Ÿäº§ç¯å¢ƒä¸é…ç½®æ­¤é¡¹ï¼Œé»˜è®¤ä¸ºtrue-->
<property>
ã€€ã€€<name>dfs.permissions</name>
ã€€ã€€<value>false</value>
</property>
</configuration>
```

ï¼ˆ4ï¼‰mapred-site.xml

```xml
<configuration>
<property>
ã€€ã€€<!--æŒ‡å®šmapreduceè¿è¡Œåœ¨yarnä¸Š-->
ã€€ã€€<name>mapreduce.framework.name</name>
ã€€ã€€<value>yarn</value>
</property>
</configuration>
```

ï¼ˆ5ï¼‰yarn-site.xml

```xml
<configuration>
<!-- å¼€å¯YARN HA --> 
<property>
ã€€ã€€<name>yarn.resourcemanager.ha.enabled</name>
ã€€ã€€<value>true</value>
</property>
<!-- æŒ‡å®šä¸¤ä¸ªresourcemanagerçš„åç§° --> 
<property>
ã€€ã€€<name>yarn.resourcemanager.ha.rm-ids</name>
ã€€ã€€<value>rm1,rm2</value>
</property>
<!-- é…ç½®rm1ï¼Œrm2çš„ä¸»æœº --> 
<property>
ã€€ã€€<name>yarn.resourcemanager.hostname.rm1</name>
ã€€ã€€<value>spnode01</value>
</property>

<property>
ã€€ã€€<name>yarn.resourcemanager.hostname.rm2</name>
ã€€ã€€<value>spnode02</value>
</property>

<!--å¼€å¯yarnæ¢å¤æœºåˆ¶-->
<property>
ã€€ã€€<name>yarn.resourcemanager.recovery.enabled</name>
ã€€ã€€<value>true</value>
</property>

<!--æ‰§è¡Œrmæ¢å¤æœºåˆ¶å®ç°ç±»-->
<property>
ã€€ã€€<name>yarn.resourcemanager.store.class</name>
<value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
</property>

<!-- é…ç½®zookeeperçš„åœ°å€ -->  
<property>
ã€€ã€€<name>yarn.resourcemanager.zk-address</name>
ã€€ã€€<value>spnode01:2181,spnode02:2181,spnode03:2181</value>
ã€€ã€€<description>For multiple zk services, separate them with comma</description>
</property>

<!-- æŒ‡å®šYARN HAçš„åç§° -->
<property>
ã€€ã€€<name>yarn.resourcemanager.cluster-id</name>
ã€€ã€€<value>yarn-ha</value>
</property>
<!--æŒ‡å®šyarnçš„è€å¤§ resoucemanagerçš„åœ°å€-->
<property>
ã€€ã€€<name>yarn.resourcemanager.hostname</name>
ã€€ã€€<value>spnode02</value>
</property>
<!--NodeManagerè·å–æ•°æ®çš„æ–¹å¼-->
<property>
ã€€ã€€<name>yarn.nodemanager.aux-services</name>
ã€€ã€€<value>mapreduce_shuffle</value>
</property>
ã€€<!--å¼€å¯æ—¥å¿—èšåˆ-->
  <property>
  ã€€ã€€<name>yarn.log-aggregation-enable</name>
  ã€€ã€€<value>true</value>
  </property>

ã€€<!--æ—¥å¿—åœ¨HDFSä¸Šæœ€å¤šä¿å­˜å¤šé•¿æ—¶é—´-->
  <property>
  ã€€ã€€<name>yarn.log-aggregation.retain-seconds</name>
  ã€€ã€€<value>106800</value>
  </property>
    
  <property>
ã€€ã€€<name>yarn.nodemanager.vmem-check-enabled</name>
ã€€ã€€<value>false</value>
</property>

<property>
ã€€ã€€<name>yarn.nodemanager.pmem-check-enabled</name>
ã€€ã€€<value>false</value>
</property>

</configuration>
```

ï¼ˆ6ï¼‰slaves

```xml
spnode01
spnode02
spnode03
```

ï¼ˆ7ï¼‰é…ç½®hadoopç¯å¢ƒå˜é‡

```shell
# HADOOP_HOME
HADOOP_HOME=/home/software/hadoop-2.7.3
PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH
export PATH HADOOP_HOME
```

ï¼ˆ8ï¼‰sourceä½¿ç¯å¢ƒå˜é‡ç”Ÿæ•ˆ

```shell
source /etc/profile.d/my_env.sh
```

ï¼ˆ9ï¼‰åˆ›å»ºæ–‡ä»¶å¤¹

```shell
#å½“å‰æ‰€åœ¨Hadoopæ ¹ç›®å½•
mkdir journal
mkdir tmp
cd tmp/
mkdir namenode
mkdir datanode
```

ï¼ˆ10ï¼‰åˆ†å‘è„šæœ¬

```shell
xsync hadoop
```

ï¼ˆ11ï¼‰å¯åŠ¨zookeeperé›†ç¾¤

```shell
zkServer.sh start
```

ï¼ˆ12ï¼‰æ ¼å¼åŒ–zkfc

  zkfcç”¨æ¥åšä¸¤ä¸ªnamenodeçš„çŠ¶æ€åˆ‡æ¢ç®¡ç†æˆ–è€…å¤±è´¥åˆ‡æ¢ç®¡ç†ã€‚

  åœ¨zkçš„leaderèŠ‚ç‚¹æœåŠ¡å™¨ä¸Šï¼ŒHadoopçš„binç›®å½•ä¸­æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š

```shell
sh hdfs zkfc -formatZK
```

å¦‚æœé…ç½®äº†Hadoopçš„ç¯å¢ƒå˜é‡ï¼Œé‚£ä¹ˆåœ¨æ­¤èŠ‚ç‚¹çš„ä»»ä½•ç›®å½•ä¸‹éƒ½å¯ä»¥æ‰§è¡Œï¼Œè¿™ä¸ªæŒ‡ä»¤çš„ä½œç”¨æ˜¯åœ¨zookeeperé›†ç¾¤ä¸Šç”ŸæˆhaèŠ‚ç‚¹ï¼ˆnsèŠ‚ç‚¹ï¼‰ã€‚

ï¼ˆ13ï¼‰å¯åŠ¨journalnodeé›†ç¾¤

ä¸ºhadoopæä¾›å…ƒæ•°æ®ç®¡ç†ï¼ˆeditsï¼‰ã€‚åœ¨04ã€05ã€06ä»»æ„èŠ‚ç‚¹æœåŠ¡å™¨ä¸Šï¼Œå³åˆ†é…äº†journalnodeè§’è‰²çš„èŠ‚ç‚¹æœåŠ¡å™¨ä¸Šï¼Œåˆ‡æ¢åˆ°hadoopå®‰è£…ç›®å½•çš„sbinç›®å½•ä¸‹ï¼Œæ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š

```shell
hadoop-daemons.sh start journalnode
```

æ³¨æ„ï¼š

1ã€ç¬¬ä¸€æ¬¡éœ€è¦æ‰‹åŠ¨èµ·ï¼Œä»¥åå°±ä¸éœ€è¦æ‰‹åŠ¨å¯åŠ¨äº†ï¼Œå°±åŒ…å«åœ¨äº†start-dfs.shè„šæœ¬é‡Œé¢äº†ï¼›

2ã€æ­¤å‘½ä»¤æ‰§è¡Œä¸€æ¬¡å°±å¯ä»¥å¯åŠ¨æ‰€æœ‰journalnodeèŠ‚ç‚¹ã€‚å¦‚ä¸‹å›¾ï¼Œå‘½ä»¤ä½¿ç”¨çš„æ˜¯hadoop-daemons.shï¼Œæ˜¯æœ‰sçš„ï¼Œå¯åŠ¨çš„æ—¶å€™ä¸€å®šè¦æ³¨æ„ï¼Œä¸è¦ç”¨é”™äº†å‘½ä»¤ã€‚



ï¼ˆ14ï¼‰æ ¼å¼åŒ–NameNode

åœ¨spnode01æœåŠ¡å™¨ä¸Šæ ¼å¼åŒ–namenodeï¼Œæ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š

```shell
hadoop namenode -format
```

ï¼ˆ15ï¼‰å¯åŠ¨NameNode

â€‹	1.spnode01æœåŠ¡å™¨

â€‹	åœ¨spnode01èŠ‚ç‚¹ä¸Šæ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ï¼Œå¯åŠ¨NameNodeèŠ‚ç‚¹ï¼š

```shell
hadoop-daemon.sh start namenode
```

â€‹	2.spnode02æœåŠ¡å™¨

â€‹    é¦–å…ˆæŠŠspark02æœåŠ¡å™¨çš„ namenodeèŠ‚ç‚¹å˜ä¸ºstandby namenodeèŠ‚ç‚¹ï¼Œæ‰§è¡Œå‘½ä»¤ 

â€‹    å¦‚ä¸‹:

```shell
hdfs namenode -bootstrapStandby
```

â€‹	3.å¯åŠ¨spark02æœåŠ¡å™¨çš„namenodeèŠ‚ç‚¹ï¼Œæ‰§è¡Œå‘½ä»¤å¦‚ä¸‹ï¼š

```shell
hadoop-daemon.sh start namenode
```

â€‹	4.å¯åŠ¨DataNode

â€‹	åœ¨spnode01ã€spnode02ã€spnode03æœåŠ¡å™¨ä¸Šåˆ†åˆ«å¯åŠ¨datanodeèŠ‚ç‚¹ï¼Œåœ¨è¿™ä¸‰å°	æœåŠ¡å™¨ä¸Šåˆ†åˆ«æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š

```shell
hadoop-daemon.sh start datanode
```

â€‹	5.å¯åŠ¨zkfc

â€‹	FalioverControllerActiveæ˜¯å¤±è´¥æ¢å¤çº¿ç¨‹ã€‚è¿™ä¸ªçº¿ç¨‹éœ€è¦åœ¨NameNodeèŠ‚ç‚¹æ‰€åœ¨	çš„æœåŠ¡å™¨ä¸Šå¯åŠ¨ï¼Œåœ¨spark01ã€spark02æœåŠ¡å™¨ä¸Šæ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š

```shell
hadoop-daemon.sh start zkfc
```

â€‹	6.å¯åŠ¨Resourcemanager

â€‹	spnode01æœåŠ¡å™¨

â€‹	åœ¨spnode01æœåŠ¡å™¨ä¸Šå¯åŠ¨ä¸»ResourcemanagerèŠ‚ç‚¹ï¼Œæ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š

```shell
start-yarn.sh
```

â€‹	å¯åŠ¨æˆåŠŸåï¼Œspnode01ã€spnode02ã€spnode03æœåŠ¡å™¨ä¸Šçš„nodemanager ä¹Ÿä¼š	è·Ÿéšå¯åŠ¨ã€‚

â€‹	spnode02æœåŠ¡å™¨

â€‹	åœ¨spnode02æœåŠ¡å™¨ä¸Šå¯åŠ¨å‰¯ ResoucemanagerèŠ‚ç‚¹ï¼Œæ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š

```shell
yarn-daemon.sh start resourcemanager
```

##### Spark on Yarn æ­å»º

ï¼ˆ1ï¼‰spark-env.sh

```shell
export JAVA_HOME=/home/software/jdk1.8
export SCALA_HOME=/home/software/scala2.11
export HADOOP_HOME=/home/software/hadoop-2.7.1
export HADOOP_CONF_DIR=/home/software/hadoop-2.7.1/etc/hadoop
```

ï¼ˆ2ï¼‰spark-defaults.conf

```shell
spark.yarn.jars=hdfs://spark01:9000/spark_jars/*
```

ï¼ˆ3ï¼‰slaves

```shell
spnode01
spnode02
spnode03
```

ï¼ˆ4ï¼‰ä¸Šä¼ jaråŒ…

â€‹	åœ¨HDFSä¸Šï¼Œåˆ›å»ºä¸€ä¸ªç›®å½•ï¼Œç”¨æ¥å­˜æ”¾sparkçš„ä¾èµ–jaråŒ…ã€‚æ­¤ç›®å½•æ˜¯spark-		

â€‹	defaults.confç›®å½•ä¸­é…ç½®çš„ç›®å½•åç§°ï¼Œå‘½ä»¤å¦‚ä¸‹ï¼š

```shell
hadoop fs -mkdir /spark_jars
```

â€‹	è¿›å…¥sparkå®‰è£…ç›®å½•çš„jarsç›®å½•ï¼Œæ‰§è¡Œï¼š

```shell
hadoop fs -put ./* /spark_jars
```

ï¼ˆ5ï¼‰å¯åŠ¨

â€‹	å¯åŠ¨spark shellï¼Œè¿›å…¥Sparkå®‰è£…ç›®å½•çš„binç›®å½•ï¼Œæ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š

```shell
spark-shell --master yarn-client
```

### å®‰è£… HBase

#### ï¼ˆ1ï¼‰è§£å‹åˆ°/opt/moduleä¸‹

```shell
tar -zxvf hbase-2.5.0-bin.tar.gz -C /opt/module
```

#### ï¼ˆ2ï¼‰é…ç½®ï¼Œè¿›å…¥/opt/module/hbase/conf

```shell
cd /opt/hbase-2.0.5/conf
```

â‘  ä¿®æ”¹ hbase-env.sh æ–‡ä»¶ï¼Œä¿®æ”¹jdké…ç½®å’ŒZooKeeperé…ç½®ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```shell
# The java implementation to use.  Java 1.8+ required.
export JAVA_HOME=/opt/jdk1.8.0_192

# Tell HBase whether it should manage it's own instance of ZooKeeper or not.
export HBASE_MANAGES_ZK=false
```

â‘¡ ä¿®æ”¹ hbase-site.xml æ–‡ä»¶ï¼Œå†…å®¹å¦‚ä¸‹ï¼š

```
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
    <property>
        <name>hbase.rootdir</name>
        <value>hdfs://node01:9000/hbase</value>
    </property>
    <property>
        <name>hbase.cluster.distributed</name>
        <value>true</value>
    </property>
    <property>
        <name>hbase.zookeeper.quorum</name>
        <value>node01, node02,node03</value>
    </property>
    <property>
        <name>hbase.zookeeper.property.clientPort</name>
        <value>2181</value>
    </property>
</configuration>
```

â‘¢ ä¿®æ”¹ regionservers æ–‡ä»¶ ï¼Œå†…å®¹å¦‚ä¸‹ï¼š

```shell
node01
node02
node03
```

â‘£ åˆ›å»º backup-masters æ–‡ä»¶

```shell
echo 'node03' > back-master
```

â‘¤ å¤åˆ¶Hadoop é…ç½®æ–‡ä»¶ hdfs-site.xml å’Œ core-site.xml åˆ°Hbaseé…ç½®ç›®å½•ä¸‹

```shell
cp /opt/hadoop-2.7.7/etc/hadoop/hdfs-site.xml /opt/hbase-2.0.5/conf/
cp /opt/hadoop-2.7.7/etc/hadoop/core-site.xml /opt/hbase-2.0.5/conf/
```

#### ï¼ˆ3ï¼‰è¿œç¨‹å¤åˆ¶ hbase-2.0.5 å®‰è£…ç›®å½•åˆ°å…¶å®ƒä¸‰å°ä¸»æœº node01, node02, node03

```shell
scp -r /opt/hbase-2.0.5 root@s1:/opt/
scp -r /opt/hbase-2.0.5 root@s2:/opt/
scp -r /opt/hbase-2.0.5 root@s3:/opt/
```

#### ï¼ˆ4ï¼‰é…ç½®HBaseç¯å¢ƒå˜é‡

```shell
# HABSE_HOME
export HBASE_HOME=/opt/hbase-2.0.5
export PATH=$PATH:$HBASE_HOME/bin
```

#### ï¼ˆ5ï¼‰sourceä½¿ç¯å¢ƒå˜é‡ç”Ÿæ•ˆ

```shell
source /etc/profile.d/my_env.sh
```

#### ï¼ˆ6ï¼‰è®¿é—®é¡µé¢

â€‹	æµè§ˆå™¨è¾“å…¥åœ°å€ï¼š

â€‹	**[http://s0:16010](http://s0:16010/)**



### å®‰è£…Flink

localæœ¬åœ°æ¨¡å¼-äº†è§£

åŸç†

![image-20211023153809963](Images/image-20211023153809963.png)

1.ä¸‹è½½å®‰è£…åŒ…

https://archive.apache.org/dist/flink/

 

2.ä¸Šä¼ flink-1.12.0-bin-scala_2.12.tgzåˆ°node1çš„æŒ‡å®šç›®å½•

 

3.è§£å‹

tar -zxvf flink-1.12.0-bin-scala_2.12.tgz 

 

4.å¦‚æœå‡ºç°æƒé™é—®é¢˜ï¼Œéœ€è¦ä¿®æ”¹æƒé™

chown -R root:root /export/server/flink-1.12.0

5.æ”¹åæˆ–åˆ›å»ºè½¯é“¾æ¥

mv flink-1.12.0 flink

ln -s /export/server/flink-1.12.0 /export/server/flink

#### æµ‹è¯•

1.å‡†å¤‡æ–‡ä»¶/root/words.txt

vim /root/words.txt

 ```
hello me you her
hello me you
hello me
hello
 ```

2.å¯åŠ¨Flinkæœ¬åœ°â€œé›†ç¾¤â€

  /export/server/flink/bin/start-cluster.sh

  

3.ä½¿ç”¨jpså¯ä»¥æŸ¥çœ‹åˆ°ä¸‹é¢ä¸¤ä¸ªè¿›ç¨‹

  \- TaskManagerRunner

  \- StandaloneSessionClusterEntrypoint

 

4.è®¿é—®Flinkçš„Web UI

  http://node1:8081/#/overview



  slotåœ¨Flinké‡Œé¢å¯ä»¥è®¤ä¸ºæ˜¯èµ„æºç»„ï¼ŒFlinkæ˜¯é€šè¿‡å°†ä»»åŠ¡åˆ†æˆå­ä»»åŠ¡å¹¶ä¸”å°†è¿™äº›å­ä»»åŠ¡åˆ†é…åˆ°slotæ¥å¹¶è¡Œæ‰§è¡Œç¨‹åºã€‚

 

5.æ‰§è¡Œå®˜æ–¹ç¤ºä¾‹

```
/export/server/flink/bin/flink run /export/server/flink/examples/batch/WordCount.jar --input /root/words.txt --output /root/out
```



6.åœæ­¢Flink

/export/server/flink/bin/stop-cluster.sh

 

 

å¯åŠ¨shelläº¤äº’å¼çª—å£(ç›®å‰æ‰€æœ‰Scala 2.12ç‰ˆæœ¬çš„å®‰è£…åŒ…æš‚æ—¶éƒ½ä¸æ”¯æŒ Scala Shell)

/export/server/flink/bin/start-scala-shell.sh local

 

æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤

 ```
benv.readTextFile("/root/words.txt").flatMap(_.split(" ")).map((_,1)).groupBy(0).sum(1).print()
 ```



é€€å‡ºshell

:quit



Standaloneç‹¬ç«‹é›†ç¾¤æ¨¡å¼-äº†è§£

åŸç†

![image-20211023153946972](Images/image-20211023153946972.png)

#### æ“ä½œ

1.é›†ç¾¤è§„åˆ’:

\- æœåŠ¡å™¨: node1(Master + Slave): JobManager + TaskManager

\- æœåŠ¡å™¨: node2(Slave): TaskManager

\- æœåŠ¡å™¨: node3(Slave): TaskManager

 

2.ä¿®æ”¹flink-conf.yaml

vim /export/server/flink/conf/flink-conf.yaml

```
jobmanager.rpc.address: node1
taskmanager.numberOfTaskSlots: 2
web.submit.enable: true

#å†å²æœåŠ¡å™¨
jobmanager.archive.fs.dir: hdfs://node1:8020/flink/completed-jobs/
historyserver.web.address: node1
historyserver.web.port: 8082
historyserver.archive.fs.dir: hdfs://node1:8020/flink/completed-jobs/
```

2.ä¿®æ”¹masters

vim /export/server/flink/conf/masters

```
node1:8081
```



3.ä¿®æ”¹slaves

vim /export/server/flink/conf/workers

```
node1
node2
node3
```



4.æ·»åŠ HADOOP_CONF_DIRç¯å¢ƒå˜é‡

vim /etc/profile

```
export HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop
```



5.åˆ†å‘

  scp -r /export/server/flink node2:/export/server/flink

  scp -r /export/server/flink node3:/export/server/flink

  scp  /etc/profile node2:/etc/profile

  scp  /etc/profile node3:/etc/profile

  æˆ–

```
 for i in {2..3}; do scp -r flink node$i:$PWD; done
```



  

6.source 

source /etc/profile

#### æµ‹è¯•

1.å¯åŠ¨é›†ç¾¤ï¼Œåœ¨node1ä¸Šæ‰§è¡Œå¦‚ä¸‹å‘½ä»¤

  /export/server/flink/bin/start-cluster.sh

  æˆ–è€…å•ç‹¬å¯åŠ¨

/export/server/flink/bin/jobmanager.sh ((start|start-foreground) cluster)|stop|stop-all

/export/server/flink/bin/taskmanager.sh start|start-foreground|stop|stop-all

 

2.å¯åŠ¨å†å²æœåŠ¡å™¨

	/export/server/flink/bin/historyserver.sh start

 




3.è®¿é—®Flink UIç•Œé¢æˆ–ä½¿ç”¨jpsæŸ¥çœ‹

http://node1:8081/#/overview

http://node1:8082/#/overview



4.æ‰§è¡Œå®˜æ–¹æµ‹è¯•æ¡ˆä¾‹

```
/export/server/flink/bin/flink run /export/server/flink/examples/batch/WordCount.jar
```



6.åœæ­¢Flinké›†ç¾¤

/export/server/flink/bin/stop-cluster.sh



Standalone-HAé«˜å¯ç”¨é›†ç¾¤æ¨¡å¼

åŸç†

![image-20211023154105699](Images/image-20211023154105699.png)

#### æ“ä½œ

1.é›†ç¾¤è§„åˆ’

\- æœåŠ¡å™¨: node1(Master + Slave): JobManager + TaskManager

\- æœåŠ¡å™¨: node2(Master + Slave): JobManager + TaskManager

\- æœåŠ¡å™¨: node3(Slave): TaskManager

 

2.å¯åŠ¨ZooKeeper

zkServer.sh status

zkServer.sh stop

zkServer.sh start

 

3.å¯åŠ¨HDFS

/export/serves/hadoop/sbin/start-dfs.sh

 

4.åœæ­¢Flinké›†ç¾¤

/export/server/flink/bin/stop-cluster.sh

 

5.ä¿®æ”¹flink-conf.yaml

vim /export/server/flink/conf/flink-conf.yaml

å¢åŠ å¦‚ä¸‹å†…å®¹G

```
state.backend: filesystem
state.backend.fs.checkpointdir: hdfs://node1:8020/flink-checkpoints
high-availability: zookeeper
high-availability.storageDir: hdfs://node1:8020/flink/ha/
high-availability.zookeeper.quorum: node1:2181,node2:2181,node3:2181
```

6.ä¿®æ”¹masters

vim /export/server/flink/conf/masters

7.åŒæ­¥

```
scp -r /export/server/flink/conf/flink-conf.yaml node2:/export/server/flink/conf/
scp -r /export/server/flink/conf/flink-conf.yaml node3:/export/server/flink/conf/
scp -r /export/server/flink/conf/masters node2:/export/server/flink/conf/
scp -r /export/server/flink/conf/masters node3:/export/server/flink/conf/
```



8.ä¿®æ”¹node2ä¸Šçš„flink-conf.yaml

vim /export/server/flink/conf/flink-conf.yaml

```
jobmanager.rpc.address: node2
```



9.é‡æ–°å¯åŠ¨Flinké›†ç¾¤,node1ä¸Šæ‰§è¡Œ

/export/server/flink/bin/stop-cluster.sh

/export/server/flink/bin/start-cluster.sh

 

10.ä½¿ç”¨jpså‘½ä»¤æŸ¥çœ‹

å‘ç°æ²¡æœ‰Flinkç›¸å…³è¿›ç¨‹è¢«å¯åŠ¨

 

11.æŸ¥çœ‹æ—¥å¿—

cat /export/server/flink/log/flink-root-standalonesession-0-node1.log

å‘ç°å¦‚ä¸‹é”™è¯¯



å› ä¸ºåœ¨Flink1.8ç‰ˆæœ¬å,Flinkå®˜æ–¹æä¾›çš„å®‰è£…åŒ…é‡Œæ²¡æœ‰æ•´åˆHDFSçš„jar

 

12.ä¸‹è½½jaråŒ…å¹¶åœ¨Flinkçš„libç›®å½•ä¸‹æ”¾å…¥è¯¥jaråŒ…å¹¶åˆ†å‘ä½¿Flinkèƒ½å¤Ÿæ”¯æŒå¯¹Hadoopçš„æ“ä½œ

ä¸‹è½½åœ°å€

https://flink.apache.org/downloads.html

13.æ”¾å…¥libç›®å½•

cd /export/server/flink/lib



14.åˆ†å‘

for i in {2..3}; do scp -r flink-shaded-hadoop-2-uber-2.7.5-10.0.jar node$i:$PWD; done

 

15.é‡æ–°å¯åŠ¨Flinké›†ç¾¤,node1ä¸Šæ‰§è¡Œ

/export/server/flink/bin/stop-cluster.sh

/export/server/flink/bin/start-cluster.sh

 

16.ä½¿ç”¨jpså‘½ä»¤æŸ¥çœ‹,å‘ç°ä¸‰å°æœºå™¨å·²ç»ok

#### æµ‹è¯•

1.è®¿é—®WebUI

http://node1:8081/#/job-manager/config

http://node2:8081/#/job-manager/config



2.æ‰§è¡Œwc

/export/server/flink/bin/flink run  /export/server/flink/examples/batch/WordCount.jar

 

3.killæ‰å…¶ä¸­ä¸€ä¸ªmaster

 

4.é‡æ–°æ‰§è¡Œwc,è¿˜æ˜¯å¯ä»¥æ­£å¸¸æ‰§è¡Œ

/export/server/flink/bin/flink run  /export/server/flink/examples/batch/WordCount.jar

 

3.åœæ­¢é›†ç¾¤

/export/server/flink/bin/stop-cluster.sh



### Flink-On-Yarn-å¼€å‘ä½¿ç”¨



#### åŸç†

![image-202110231542193611](Images/image-202110231542193611.png)

![image-20211023154228605](Images/image-20211023154228605.png)

ä¸¤ç§æ¨¡å¼

Sessionä¼šè¯æ¨¡å¼

![image-20211023154241599](Images/image-20211023154241599.png)





![image-20211023154247436](Images/image-20211023154247436-16352243782541.png)

#### æ“ä½œ

1.å…³é—­yarnçš„å†…å­˜æ£€æŸ¥

vim /export/server/hadoop/etc/hadoop/yarn-site.xml

```
 <!-- å…³é—­yarnå†…å­˜æ£€æŸ¥ -->
    <property>
        <name>yarn.nodemanager.pmem-check-enabled</name>
        <value>false</value>
    </property>
    <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
    </property>
```

2.åˆ†å‘

```
scp -r /export/server/hadoop/etc/hadoop/yarn-site.xml node2:/export/server/hadoop/etc/hadoop/yarn-site.xml
scp -r /export/server/hadoop/etc/hadoop/yarn-site.xml node3:/export/server/hadoop/etc/hadoop/yarn-site.xml
```



3.é‡å¯yarn

/export/server/hadoop/sbin/stop-yarn.sh

/export/server/hadoop/sbin/start-yarn.sh

#### æµ‹è¯•

#### Sessionä¼šè¯æ¨¡å¼

åœ¨Yarnä¸Šå¯åŠ¨ä¸€ä¸ªFlinké›†ç¾¤,å¹¶é‡å¤ä½¿ç”¨è¯¥é›†ç¾¤,åç»­æäº¤çš„ä»»åŠ¡éƒ½æ˜¯ç»™è¯¥é›†ç¾¤,èµ„æºä¼šè¢«ä¸€ç›´å ç”¨,é™¤éæ‰‹åŠ¨å…³é—­è¯¥é›†ç¾¤----é€‚ç”¨äºå¤§é‡çš„å°ä»»åŠ¡



1.åœ¨yarnä¸Šå¯åŠ¨ä¸€ä¸ªFlinké›†ç¾¤/ä¼šè¯ï¼Œnode1ä¸Šæ‰§è¡Œä»¥ä¸‹å‘½ä»¤

/export/server/flink/bin/yarn-session.sh -n 2 -tm 800 -s 1 -d

 

è¯´æ˜:

ç”³è¯·2ä¸ªCPUã€1600Må†…å­˜

\# -n è¡¨ç¤ºç”³è¯·2ä¸ªå®¹å™¨ï¼Œè¿™é‡ŒæŒ‡çš„å°±æ˜¯å¤šå°‘ä¸ªtaskmanager

\# -tm è¡¨ç¤ºæ¯ä¸ªTaskManagerçš„å†…å­˜å¤§å°

\# -s è¡¨ç¤ºæ¯ä¸ªTaskManagerçš„slotsæ•°é‡

\# -d è¡¨ç¤ºä»¥åå°ç¨‹åºæ–¹å¼è¿è¡Œ

 

æ³¨æ„:

è¯¥è­¦å‘Šä¸ç”¨ç®¡

WARN  org.apache.hadoop.hdfs.DFSClient  - Caught exception 

java.lang.InterruptedException

 

2.æŸ¥çœ‹UIç•Œé¢

http://node1:8088/cluster

 

3.ä½¿ç”¨flink runæäº¤ä»»åŠ¡ï¼š

  /export/server/flink/bin/flink run  /export/server/flink/examples/batch/WordCount.jar

  è¿è¡Œå®Œä¹‹åå¯ä»¥ç»§ç»­è¿è¡Œå…¶ä»–çš„å°ä»»åŠ¡

  /export/server/flink/bin/flink run  /export/server/flink/examples/batch/WordCount.jar

 

4.é€šè¿‡ä¸Šæ–¹çš„ApplicationMasterå¯ä»¥è¿›å…¥Flinkçš„ç®¡ç†ç•Œé¢



  

==5.å…³é—­yarn-sessionï¼š==

yarn application -kill application_1609508087977_0005





#### Jobåˆ†ç¦»æ¨¡å¼--ç”¨çš„æ›´å¤š

é’ˆå¯¹æ¯ä¸ªFlinkä»»åŠ¡åœ¨Yarnä¸Šå¯åŠ¨ä¸€ä¸ªç‹¬ç«‹çš„Flinké›†ç¾¤å¹¶è¿è¡Œ,ç»“æŸåè‡ªåŠ¨å…³é—­å¹¶é‡Šæ”¾èµ„æº,----é€‚ç”¨äºå¤§ä»»åŠ¡



1.ç›´æ¥æäº¤job

/export/server/flink/bin/flink run -m yarn-cluster -yjm 1024 -ytm 1024 /export/server/flink/examples/batch/WordCount.jar

-m  jobmanagerçš„åœ°å€

-yjm 1024 æŒ‡å®šjobmanagerçš„å†…å­˜ä¿¡æ¯

-ytm 1024 æŒ‡å®štaskmanagerçš„å†…å­˜ä¿¡æ¯

 

 

2.æŸ¥çœ‹UIç•Œé¢

http://node1:8088/cluster





### ç«¯å£

#### hadoop 2.x VS 3.x ç«¯å£å¯¹æ¯”

æœ€è¿‘é…ç½®Hadoop3.xé›†ç¾¤çš„æ—¶å€™å‘ç°äº†ä¸€äº›ç«¯å£å˜åŠ¨ï¼Œå¯¼è‡´webè®¿é—®UIç•Œé¢å¤±è´¥ï¼ŒæŸ¥é˜…èµ„æ–™å†™ä¸ªå¸–å­è®°å½•åˆ†æä¸€ä¸‹ã€‚

Namenode ç«¯å£:
2.xç«¯å£ 	3.xç«¯å£ name desc
50470 	9871 	dfs.namenode.https-address The namenode secure http server address and port.
50070 	9870 	dfs.namenode.http-address The address and the base port where the dfs namenode web ui will listen on.
8020 9820 fs.defaultFS æŒ‡å®šHDFSè¿è¡Œæ—¶nameNodeåœ°å€
Secondary NN ç«¯å£:
2.xç«¯å£ 3.xç«¯å£ name desc
50091 9869 dfs.namenode.secondary.https-address The secondary namenode HTTPS server address and port
50090 9868 dfs.namenode.secondary.http-address The secondary namenode HTTPS server address and port
Datanode ç«¯å£:
2.xç«¯å£ 3.xç«¯å£ name desc
50020 9867 dfs.datanode.ipc.address The datanode ipc server address and port.
50010 9866 dfs.datanode.address The datanode server address and port for data transfer.
50475 9865 dfs.datanode.https.address The datanode secure http server address and port
50075 9864 dfs.datanode.http.address The datanode http server address and por
Yarn ç«¯å£
2.xç«¯å£ 3.xç«¯å£ name desc
8088 yarn.resourcemanager.webapp.address httpæœåŠ¡ç«¯å£
ä»¥ä¸Šæ˜¯å¸¸ç”¨åˆ°çš„ä¸€äº›é…ç½®ä¸­çš„ç«¯å£å˜åŠ¨





#### Sparkå¸¸ç”¨ç«¯å£

1ï¼‰Sparkå†å²æœåŠ¡å™¨ç«¯å£å·ï¼š18080 ï¼ˆç±»æ¯”äºHadoopå†å²æœåŠ¡å™¨ç«¯å£å·ï¼š19888ï¼‰

2ï¼‰Spark Master Webç«¯å£å·ï¼š8080ï¼ˆç±»æ¯”äºHadoopçš„NameNode Webç«¯å£å·ï¼š9870(50070)ï¼‰

3ï¼‰Spark Masterå†…éƒ¨é€šä¿¡æœåŠ¡ç«¯å£å·ï¼š7077 ï¼ˆç±»æ¯”äºHadoopï¼ˆé«˜ç‰ˆæœ¬ï¼‰çš„8020(9000)ç«¯å£ï¼‰

4ï¼‰SparkæŸ¥çœ‹å½“å‰Spark-shellè¿è¡Œä»»åŠ¡æƒ…å†µç«¯å£å·ï¼š4040

5ï¼‰Hadoop YARNä»»åŠ¡è¿è¡Œæƒ…å†µæŸ¥çœ‹ç«¯å£å·ï¼š8088

